{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWNgJtQ3FrN8"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "<font size='10' color = 'E3A440'>**Megadata and Advanced Techniques Demystified**</font>\n",
        "=======\n",
        "<font color = 'E3A440'>*New Analysis Methods and their Implications for Megadata Management in SSH (part 2)*</font>\n",
        "=============\n",
        "\n",
        "\n",
        "This workshop is part of the training [Megadata and Advanced Techniques Demystified](https://www.4point0.ca/en/2022/08/22/formation-megadonnees-demystifiees//) (session 6).\n",
        "\n",
        "Humanities and social sciences are often confronted with the analysis of unstructured data, such as text. After preparing the data, several analysis techniques from machine learning can be used. During this workshop, participants will be introduced to the preprocessing of textual data and to supervised and unsupervised methods for analysis purposes with Python.\n",
        "\n",
        "\n",
        "Structure of the workshop :\n",
        "1. Part 1 : Examples of supervised and unsupervised methods for text mining\n",
        "2. Part 2 : Exercices\n",
        "\n",
        "### Autors: \n",
        "- Bruno Agard <bruno.agard@polymtl.ca>\n",
        "- Davide Pulizzotto <davide.pulizzotto@polymtl.ca>\n",
        "\n",
        "Département de Mathématiques et de génie industriel\n",
        "\n",
        "École Polytechnique de Montréal"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = 'E3A440'>0. Preparation environnement </font>"
      ],
      "metadata": {
        "id": "HpAX32-4Kj8U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jddhTL7EFrN_"
      },
      "outputs": [],
      "source": [
        "# Downloading of data from the GitHub project\n",
        "!rm -rf Data_techniques_demystified_webinars/\n",
        "!git clone https://github.com/4point0-ChairInnovation-Polymtl/Data_techniques_demystified_webinars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJLRFWa6FrOA"
      },
      "outputs": [],
      "source": [
        "# Import modules\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.preprocessing import Normalizer\n",
        "import matplotlib.pyplot as plt\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwFKNpPQFrN-"
      },
      "source": [
        "<a name='Section_1'></a>\n",
        "# <font color = 'E3A440'>1. *Preparation of textual data* (Reminders)</font>\n",
        "\n",
        "The pre-processing of a corpus of texts may require the implementation of several steps including: the splitting of sentences, words, cleaning, filtering, etc.\n",
        "\n",
        "In the next blocks of code, a text will be segmented into sentences and preprocessed using the `CleaningText()` function prepared in the previous session (October 27th, 2022).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QOzWtcugFrOK"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"At eight o'clock, on Thursday morning, the great Arthur didn't feel VERY good.\n",
        "This morning, Arthur is feeling better.\n",
        "A dog runs in the street.\n",
        "A little boy in running in the street.\n",
        "Arthur is my dog, he sleeps every morning.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLrvRTIGdypS"
      },
      "outputs": [],
      "source": [
        "# extraction of sentences\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "print(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6rtngk2rFrOL"
      },
      "outputs": [],
      "source": [
        "# Cleaning fonction to preprocess text\n",
        "def CleaningText(text_as_string, language = 'english', reduce = '', list_pos_to_keep = [], Stopwords_to_add = []):\n",
        "    from nltk.corpus import stopwords\n",
        "\n",
        "    words = nltk.word_tokenize(text_as_string)\n",
        "    words_pos = nltk.pos_tag(words, tagset='universal')\n",
        "    words_pos = [(w, pos) for w, pos in words_pos if w.isalnum()]\n",
        "    words_pos = [(w.lower(), pos) for w, pos in words_pos]\n",
        "    \n",
        "    if reduce == 'stem': \n",
        "        from nltk.stem.porter import PorterStemmer\n",
        "        reduced_words_pos = [(PorterStemmer().stem(w), pos) for w, pos in words_pos]\n",
        "        \n",
        "    elif reduce == 'lemma':\n",
        "        from nltk.stem.wordnet import WordNetLemmatizer\n",
        "        reduced_words_pos = [(WordNetLemmatizer().lemmatize(w), pos) for w, pos in words_pos]\n",
        "    else:\n",
        "        import warnings\n",
        "        reduced_words_pos = words_pos\n",
        "        warnings.warn(\"Warning : any reduction was made on words! Please, use \\\"reduce\\\" argument to chosse between 'stem' or  'lemma'\")\n",
        "    if list_pos_to_keep:\n",
        "        reduced_words_pos = [(w, pos) for w, pos in reduced_words_pos if pos in list_pos_to_keep]\n",
        "    else:\n",
        "        import warnings\n",
        "        warnings.warn(\"Warning : any POS filtering was made. Please, use \\\"list_pos_to_keep\\\" to create a list of POS tag to keep.\")\n",
        "    \n",
        "    list_stopwords = stopwords.words(language) + Stopwords_to_add\n",
        "    reduced_words_pos = [(w, pos) for w, pos in reduced_words_pos if w not in list_stopwords and len(w) > 1 ]\n",
        "    return reduced_words_pos   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkpxyQfl2Wna"
      },
      "outputs": [],
      "source": [
        "# Cleaning of sentences and selection of POS tags\n",
        "cleaned_sentences = [CleaningText(sent, reduce = 'stem', list_pos_to_keep = ['NOUN','ADJ','VERB']) for sent in sentences]\n",
        "print(cleaned_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following, the list of POS tags"
      ],
      "metadata": {
        "id": "Vs4Y_iuW1YyL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **POS** | **DESCRIPTION**           | **EXAMPLES**                                      |\n",
        "| ------- | ------------------------- | ------------------------------------------------- |\n",
        "| ADJ     | adjective                 | big, old, green, incomprehensible, first      |\n",
        "| ADP     | adposition                | in, to, during                                |\n",
        "| ADV     | adverb                    | very, tomorrow, down, where, there            |\n",
        "| AUX     | auxiliary                 | is, has (done), will (do), should (do)        |\n",
        "| CONJ    | conjunction               | and, or, but                                  |\n",
        "| CCONJ   | coordinating conjunction  | and, or, but                                  |\n",
        "| DET     | determiner                | a, an, the                                    |\n",
        "| INTJ    | interjection              | psst, ouch, bravo, hello                      |\n",
        "| NOUN    | noun                      | girl, cat, tree, air, beauty                  |\n",
        "| NUM     | numeral                   | 1, 2017, one, seventy-seven, IV, MMXIV        |\n",
        "| PART    | particle                  | ’s, not                                      |\n",
        "| PRON    | pronoun                   | I, you, he, she, myself, themselves, somebody |\n",
        "| PROPN   | proper noun               | Mary, John, London, NATO, HBO                 |\n",
        "| PUNCT   | punctuation               | ., (, ), ?                                    |\n",
        "| SCONJ   | subordinating conjunction | if, while, that                               |\n",
        "| SYM     | symbol                    | $, %, §, ©, +, −, ×, ÷, =, :)               |\n",
        "| VERB    | verb                      | run, runs, running, eat, ate, eating          |\n",
        "| X       | other                     | sfpksdpsxmsa                                  |\n",
        "| SPACE   | space                     |                                                   |"
      ],
      "metadata": {
        "id": "X0CQ8_vP1W-W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = 'E3A440'>*2. Vectorization* (Reminders)</font>\n",
        "\n",
        "Typically, to use text in a data analysis or machine learning context, text must be transformed into an appropriate mathematical object.\n",
        "The simplest and most widespread model is the \"bags-of-words\", in which each text (or each text fragment) is defined in a vector, by a certain number of lexical units which characterize it. This model belongs to the family of vector semantics models and it has the following form:\n",
        "\n",
        "\n",
        "$$X = \\begin{bmatrix} \n",
        "x_{1,1} & x_{1,2} & \\ldots & x_{1,w} \\\\\n",
        "\\vdots & \\vdots       &  \\ddots      & \\vdots \\\\ \n",
        "x_{n,1} & x_{1,2} & \\ldots & x_{n,w} \\\\\n",
        "\\end{bmatrix}\n",
        "$$ \n",
        "\n",
        "In this matrix, the value $x_{i,j}$ represents the \"weigth\" of the word $j$ in the text fragment $i$. This weigth can be computed in several way. Thus :\n",
        "\n",
        "- $x_{i,j}$ can represents the presence of the word \"j\" in text fragment $i$,\n",
        "- $x_{i,j}$ can measures the quantoty of occurrences of a word $j$ in text fragment $i$,\n",
        "- $x_{i,j}$ can represent the **value** of the word $j$ in text fragment $i$, and this, using metric such as tf-idf :\n",
        " $$\\text{tf-idf}_{i,j}=\\text{tf}_{i,j}.log\\left(\\frac{n}{n_i}\\right)$$\n",
        " - $\\text{tf}_{i,j}$ is the frequency of word $i$ in text fragment $j$,\n",
        " - $n$ total count of text fragments,\n",
        " - $n_i$ total counts of text fragments containing the word $i$.\n"
      ],
      "metadata": {
        "id": "m0YdoOA1stB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pay attention to the following arguments of the `CountVectorizer()` function:\n",
        "\n",
        " 1.   `min_df`: the minimum documentary frequency that a word must respect to be retained in the matrix\n",
        " 2.   `max_df`: the maximum documentary frequency that a word must respect to be retained in the matrix\n",
        " 3,   `ngram_range`: allows to add the n-grams (bigrams, trigrams, etc.) to the vectorization"
      ],
      "metadata": {
        "id": "8AwAN6fDHR_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Object initialization\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def identity_tokenizer(text):\n",
        "    return text\n",
        "\n",
        "# Transforming the word in frequencies\n",
        "vectorized = CountVectorizer(lowercase = False, # Convert all characters to lowercase before tokenizing\n",
        "                             min_df = 1, # Ignore terms that have a document frequency strictly lower than the given threshold \n",
        "                             max_df = 10, # Ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words)\n",
        "                             stop_words = stopwords.words('english'), # Remove the list of words provided\n",
        "                             ngram_range = (1, 1), # Get the lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted\n",
        "                             tokenizer=identity_tokenizer) # Override the string tokenization step while preserving the preprocessing and n-grams generation steps"
      ],
      "metadata": {
        "id": "LpFPJrGI2Nsi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use of the \"vectorizer\" with a list of word lists (and not a list of word-pos tuples) and creation of the frequency weighting matrix."
      ],
      "metadata": {
        "id": "G9uC3usB5dtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Application du vectorizer\n",
        "freq_term_DTM = vectorized.fit_transform([[w for w, pos in sent] for sent in cleaned_sentences])\n",
        "\n",
        "freq_Matrix=pd.DataFrame(freq_term_DTM.toarray(), columns =  [k for k, v in sorted(vectorized.vocabulary_.items(), key=lambda item: item[1])] )\n",
        "print(freq_Matrix)"
      ],
      "metadata": {
        "id": "v8ydVdmC5c76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thus, we assign the result of the Tf-IDF weighting to the variable named `tfidf_DTM`. "
      ],
      "metadata": {
        "id": "pRPjGQVokg7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the tfidf matrix\n",
        "tfidf = TfidfTransformer(norm='l1')\n",
        "tfidf_DTM = tfidf.fit_transform(freq_term_DTM)\n",
        "\n",
        "tfidf_Matrix=pd.DataFrame(tfidf_DTM.toarray(), columns =  [k for k, v in sorted(vectorized.vocabulary_.items(), key=lambda item: item[1])] )\n",
        "print(tfidf_Matrix)"
      ],
      "metadata": {
        "id": "4GsktR7_Znke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAEHXJXZrCGi"
      },
      "source": [
        "\n",
        "# <font color = 'E3A440'>3.0. Segmentation </font>\n",
        "\n",
        "The purpose of segmentation is to divide a data set into smaller subsets that share certain characteristics.\n",
        "\n",
        "We want the elements within the same subgroup to be as similar as possible.\n",
        "\n",
        "A large majority of segmentation methods are then based on distance metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aALJbs9YrHHx"
      },
      "source": [
        "\n",
        "### <font color = 'E3A440'>3.1. Distance metrics </font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SAuddwtrLBS"
      },
      "source": [
        "\n",
        "\n",
        "#### <font color = 'E3A440'>3.1.1. Distance between points </font>\n",
        "\n",
        "There are many metrics to evaluate the similarity/distance between two points:\n",
        "\n",
        "\n",
        "### Euclidean Distance\n",
        "\n",
        "$$d(x_1,x_2) = \\sqrt{\\sum_i^n\\left(x_{1,i},x_{2,i}\\right)^2}$$\n",
        "\n",
        "\n",
        "### Hamming Distance\n",
        "\n",
        "In number of different coordinates\n",
        "\n",
        "$$d_1(x_1,x_2) = \\sum_i^n\\delta_i$$\n",
        "\n",
        "\n",
        "\\begin{split}\n",
        "    Tel\\ que\\ :&\\\\ \n",
        "    &\\delta_i=\\begin{cases}  \n",
        "    0,\\ if\\ x_{1,i} = x_{2,i}\\\\\n",
        "    Sinon\\ 1\\\\\n",
        "    \\end{cases}\n",
        "\\end{split}\n",
        "\n",
        "\n",
        "In number of different \"positive\" coordinates\n",
        "\n",
        "$$d_2(x_1,x_2) = \\sum_i^n\\delta_i$$\n",
        "\n",
        "\n",
        "\\begin{split}\n",
        "    Tel\\ que\\ :&\\\\ \n",
        "    &\\delta_i=\\begin{cases}  \n",
        "    0,\\ if\\ x_{1,i} = x_{2,i} = 1\\\\\n",
        "    Sinon\\ 1\\\\\n",
        "    \\end{cases}\n",
        "\\end{split}\n",
        "\n",
        "As a percentage of different \"positive\" coordinates\n",
        "\n",
        "$$d_3(x_1,x_2) = \\frac{d_1(x_1,x_2)\\ ou\\ d_2(x_1,x_2)}{n}$$\n",
        "\n",
        "With different possible weightings\n",
        "\n",
        "$$d_4(x_1,x_2) = \\sum_i^n\\delta_i$$\n",
        "\n",
        "\n",
        "\\begin{split}\n",
        "    Tel\\ que\\ :&\\\\ \n",
        "    &\\delta_i=\\begin{cases}  \n",
        "    p,\\ if\\ x_{1,i} = x_{2,i} = 1\\\\\n",
        "    1,\\ if\\ x_{1,i} = x_{2,i} = 0\\\\ \n",
        "    Sinon\\ 0\\\\\n",
        "    \\end{cases}\n",
        "\\end{split}\n",
        "\n",
        "There are other 'adapted' versions of Hamming...\n",
        "\n",
        "\n",
        "### Many more ...\n",
        "\n",
        "But none really does what you'll need, because your need is unique.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC8c_A6XrVqa"
      },
      "source": [
        "\n",
        "#### <font color = 'E3A440'>3.1.2. Hypermetrics, distances between groups </font>\n",
        "\n",
        " - Nearest neighbor (single linkage)\n",
        " - Farthest neighbor (complete linkage)\n",
        " - Average distance (average linkage)\n",
        " - Distance to the center of gravity\n",
        " - ...\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXR4EIkvrdhk"
      },
      "source": [
        "### <font color = 'E3A440'>3.2. Data normalisation </font>\n",
        "\n",
        "For distance-based algorithms, it is important to normalize the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pirDtYQqdypY"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial import distance\n",
        "\n",
        "X = np.array([[100000, 0, 0, 0, 0, 0, 0],\n",
        "              [100000, 1, 1, 1, 1, 1, 1],\n",
        "              [1, 1, 1, 1, 1, 1, 1],\n",
        "             ])\n",
        "\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Luj-VLTwdypZ"
      },
      "outputs": [],
      "source": [
        "print( distance.euclidean(X[0], X[1]) )\n",
        "print( distance.euclidean(X[1], X[2]) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GL1Djp_WdypZ"
      },
      "source": [
        "#### Decimal normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hZmISnqdypZ"
      },
      "outputs": [],
      "source": [
        "XD=X.copy()\n",
        "XD[:, 0] = XD[:, 0]/100000\n",
        "\n",
        "print(XD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83224ioddypZ"
      },
      "outputs": [],
      "source": [
        "print( distance.euclidean(XD[0], XD[1]) )\n",
        "print( distance.euclidean(XD[1], XD[2]) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "977e0F1_dypa"
      },
      "source": [
        "#### Min/max Normalisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLSLUvWgdypa"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "XmM=X.copy()\n",
        "scaler = MinMaxScaler().fit(XmM)\n",
        "XmM=scaler.transform(XmM)\n",
        "#X=scaler.inverse_transform(X)\n",
        "\n",
        "print(XmM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKLkwqYIdypb"
      },
      "outputs": [],
      "source": [
        "print( distance.euclidean(XmM[0], XmM[1]) )\n",
        "print( distance.euclidean(XmM[1], XmM[2]) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iy5Y2qp1dypb"
      },
      "source": [
        "#### Normalization on variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JTMscHQdypb"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "XV=X.copy()\n",
        "scaler = StandardScaler().fit(XV)\n",
        "XV=scaler.transform(XV)\n",
        "#X=scaler.inverse_transform(X)\n",
        "\n",
        "print(XV)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtYUwMkfdypb"
      },
      "outputs": [],
      "source": [
        "print( distance.euclidean(XV[0], XV[1]) )\n",
        "print( distance.euclidean(XV[1], XV[2]) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvmxXbl8XXST"
      },
      "source": [
        "### <font color = 'E3A440'>3.3. Segmentation algorithms </font>\n",
        "\n",
        "There are many algorithms to achieve the segmentation of a data set. We choose a particular algorithm according to the type of data and the type of result we are looking for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "vWpU4HMmwNTX"
      },
      "outputs": [],
      "source": [
        "# Draw fonction to plot results\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from numpy import unique\n",
        "from numpy import where\n",
        "\n",
        "def draw(X,yhat):\n",
        "    # retrieve unique clusters\n",
        "    clusters = unique(yhat)\n",
        "\n",
        "    #plt.figure(figsize=(10, 10))\n",
        "    # create scatter plot for samples from each cluster\n",
        "    for cluster in clusters:\n",
        "        # get row indexes for samples with this cluster\n",
        "        row_ix = where(yhat == cluster)\n",
        "        # create scatter of these samples\n",
        "        plt.scatter(X[row_ix, 0], X[row_ix, 1])\n",
        "\n",
        "    plt.title(\"Clusters\")\n",
        "    # show the plot\n",
        "    plt.show() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKm8hyy4dypc"
      },
      "outputs": [],
      "source": [
        "#dataset generator\n",
        "k=4\n",
        "n_samples=1000\n",
        "features=3\n",
        "\n",
        "# define dataset\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X, _ = make_blobs(n_samples, centers=k, n_features=features, cluster_std=0.6, random_state=0)\n",
        "\n",
        "scaler = StandardScaler().fit(X)\n",
        "X=scaler.transform(X)\n",
        "#X=scaler.inverse_transform(X)\n",
        "\n",
        "yhat=np.zeros(n_samples)\n",
        "    \n",
        "draw(X,yhat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SU00eYOjXXbW"
      },
      "source": [
        "#### <font color = 'E3A440'>3.3.1. k-means</font>\n",
        "Applying a <font color='E3A440'>**segmentation**</font> algorithm to a database makes it possible to segment observations into homogeneous groups of data.\n",
        "\n",
        "To do this, the objective of the k-means algorithm is to minimize the intra-class inertia around a centroid of each group. For k-means, the centroid of each group is the mean of that group. Centroids are updated at each iteration during training.\n",
        "\n",
        "The k-means method generates a vector $Y$ of size $n$, which contains the <font color='E3A440'>**group labels**</font> assigned to each observation. Labels can range from $0$ to $k$. $k$ is the parameter that allows the user to determine the number of groups to generate.\n",
        "\n",
        "$$\n",
        "Y = \\begin{bmatrix} \n",
        "c_1 \\\\\n",
        "c_2 \\\\\n",
        "\\vdots \\\\ \n",
        "c_n\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "$Y_i$ corresponds to the group assigned to $X_i$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-NUIXIqdypd"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "model = KMeans(n_clusters=4).fit(X)\n",
        "\n",
        "# assign a cluster to each example\n",
        "yhatkm = model.predict(X)\n",
        "\n",
        "draw(X,yhatkm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdXBgvQldypd"
      },
      "source": [
        "##### What happens at each iteration?\n",
        "\n",
        "The algorithm performs the following steps:\n",
        " 1. **Initialization of centroid**: k points (*k* is chosen by the user) are randomly selected from the data set. Each of these points will be used as a centroid of a group for the first iteration. Each centroid will wear the label of the group he represents.\n",
        " \n",
        " 2. **Start of iterations**:\n",
        "\n",
        "   2.1. **Classification**: Assign each point in the data set the label of the centroid closest to it.\n",
        "\n",
        "   2.2. **centroid update**: For each generated group, calculate the \"center\" of the data associated with the group. This center will be the new centroid for the group.\n",
        "\n",
        "   2.3. **Repetition**: Repeat operations 2.1. and 2.2. up to a stopping criterion (stabilization of the centroids, number of iterations, calculation time, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jH2XFE2dype"
      },
      "outputs": [],
      "source": [
        "k=4\n",
        "\n",
        "for i in range (1,4) :\n",
        "    print(\"Iteration :\", i)\n",
        "    model = KMeans(n_clusters=k, max_iter=i, n_init=1, init=\"random\", random_state=1).fit(X)\n",
        "    # assign a cluster to each example\n",
        "    yhatkm = model.predict(X)\n",
        "    draw(X,yhatkm)\n",
        "\n",
        "print(\"...\\n...\\n...\\n\")\n",
        "\n",
        "i=10\n",
        "print(\"Iteration :\", i)\n",
        "model = KMeans(n_clusters=k, max_iter=i, n_init=1, init=\"random\", random_state=1).fit(X)\n",
        "# assign a cluster to each example\n",
        "yhatkm = model.predict(X)\n",
        "draw(X,yhatkm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc44aMJ0dype"
      },
      "source": [
        "##### Initialization Sensitivity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASr1FkoMdype"
      },
      "outputs": [],
      "source": [
        "for rs in range (1,6) :\n",
        "    print(\"Random state :\", rs)\n",
        "    model = KMeans(n_clusters=k, n_init=1, init=\"random\", random_state=rs).fit(X)\n",
        "    # assign a cluster to each example\n",
        "    yhatkm = model.predict(X)\n",
        "    draw(X,yhatkm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twqpWLJGdype"
      },
      "source": [
        "Findings:\n",
        "\n",
        " - K-means is sensitive to the initial choice of centroids, this algorithm gives a LOCAL optimum.\n",
        " \n",
        " - The initialization has an impact on the results of the k-means, therefore on the quality of the partitioning.\n",
        "\n",
        " - To compensate, we can:\n",
        " \n",
        "     - carefully choose the initialization points according to our knowledge of the problem,\n",
        "\n",
        "     - apply k-means with different starting points and keep the best result,\n",
        "         \n",
        "     - apply specific initialization methods, among which there is *k-means++*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxPJOsCadype"
      },
      "source": [
        "##### Sensitivity to the number of groups\n",
        "\n",
        "The *k* parameter determines the number of groups desired. The method is very sensitive to this parameter, and it must be chosen wisely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGpSOlwydypf"
      },
      "outputs": [],
      "source": [
        "for m in range (1,6) :\n",
        "    print(\"k = \", m)\n",
        "    model = KMeans(n_clusters=m, max_iter=20, n_init=1, init=\"random\", random_state=1).fit(X)\n",
        "    # assign a cluster to each example\n",
        "    yhatkm = model.predict(X)\n",
        "    draw(X,yhatkm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGgjhYdEdypf"
      },
      "source": [
        "Conclusion :\n",
        "\n",
        "  - the wrong number of groups can result in groups that do not make sense for the purpose of the analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgBJhckyqa5w"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "max=30\n",
        "Sil=np.zeros(max)\n",
        "x_axis=np.zeros(max)\n",
        "\n",
        "for m in range (2,max) :\n",
        "\n",
        "    model = KMeans(n_clusters=m, max_iter=100, n_init=1, init=\"random\", random_state=3).fit(X)\n",
        "\n",
        "    # assign a cluster to each example\n",
        "    labels = model.predict(X)\n",
        "\n",
        "    Sil[m]=silhouette_score(X, labels, metric=\"euclidean\")\n",
        "    x_axis[m]=m\n",
        "\n",
        "fig = plt.figure(figsize=(6, 6))\n",
        "a1 = fig.add_axes([0,0,1,1])\n",
        "l1 = a1.plot(x_axis,Sil,'rs-') # solid line with yellow colour and square marker\n",
        "a1.set_xlabel('k')\n",
        "a1.set_ylabel('Silhouette')\n",
        "a1.set_title('Evolution of indexes')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRfH1HBTdypf"
      },
      "source": [
        "##### From our set of texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P57i7O3dwNTc"
      },
      "outputs": [],
      "source": [
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXlv6J8xZEj-"
      },
      "outputs": [],
      "source": [
        "tfidf_Matrix=tfidf_DTM.toarray()\n",
        "\n",
        "model = KMeans(n_clusters=2).fit(tfidf_Matrix)\n",
        "\n",
        "# assign a cluster to each example\n",
        "yhatkm_tfidf = model.predict(tfidf_Matrix)\n",
        "\n",
        "print(yhatkm_tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8qelreldypf"
      },
      "source": [
        "#### <font color = 'E3A440'>3.3.2. Hiéarchical Clustering</font>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvcnyQxvdypg"
      },
      "outputs": [],
      "source": [
        "import scipy.cluster.hierarchy as shc\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.title(\"Customer Dendograms\")\n",
        "dend = shc.dendrogram(shc.linkage(X, metric='euclidean', method='complete'))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAeuuEApdypg"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "model=AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='complete').fit(X)\n",
        "#{“ward”, “complete”, “average”, “single”},\n",
        "\n",
        "# assign a cluster to each example\n",
        "yhatac=model.fit_predict(X)\n",
        "\n",
        "draw(X,yhatac)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhIlVYF3dypj"
      },
      "source": [
        "##### From our set of texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Gpc2eGkqa5y"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 7))\n",
        "plt.title(\"Customer Dendograms\")\n",
        "dend = shc.dendrogram(shc.linkage(tfidf_Matrix, metric='euclidean', method='complete'))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxoIO7ELqa5y"
      },
      "source": [
        "#### <font color = 'E3A440'>3.3.3. DBSCAN</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmjudHHhqa5z"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "model = DBSCAN(eps=0.2, min_samples=4, metric=\"euclidean\").fit(X)\n",
        "\n",
        "# assign a cluster to each example\n",
        "yhatdbs=model.fit_predict(X)\n",
        "\n",
        "draw(X,yhatdbs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MCWDICTdypk"
      },
      "source": [
        "#### <font color = 'E3A440'>3.3.4. Analysis of group content</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0PKzHp3dypk"
      },
      "outputs": [],
      "source": [
        "model = KMeans(n_clusters=4, max_iter=20, n_init=1, init=\"random\", random_state=1).fit(X)\n",
        "\n",
        "# assign a cluster to each example\n",
        "labels = model.predict(X)\n",
        "\n",
        "draw(X,labels)\n",
        "\n",
        "print(\"Silhouette=\", silhouette_score(X, labels, metric=\"euclidean\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjigF2JMdypl"
      },
      "outputs": [],
      "source": [
        "clusters = unique(yhatkm)\n",
        "\n",
        "for cluster in clusters:\n",
        "    C = X[yhatkm[:] == cluster]\n",
        "    labels=yhatkm[yhatkm[:] == cluster]\n",
        "    print('Cluster %1.0f : size = %.0f' % (cluster, len(C)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHJQ7Iglqa51"
      },
      "outputs": [],
      "source": [
        "for cluster in clusters:\n",
        "    C = X[yhatkm[:] == cluster]\n",
        "    fig1, ax1 = plt.subplots()\n",
        "    ax1.set_title('Cluster %1.0f' % (cluster))\n",
        "    ax1.boxplot(C);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4wTUIPWZEX-"
      },
      "source": [
        "# <font color = 'E3A440'>4. Supervised methods</font>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4HD1S5gbtXz"
      },
      "source": [
        "## <font color = 'E3A440'>4.1. Multilayer perceptron</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IqRsMPFcgn4"
      },
      "source": [
        "Before going on, it is necessary to prepare the corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "vNzsFIL5qa51"
      },
      "outputs": [],
      "source": [
        "text2=\"\"\"At eight o'clock, on Thursday morning, the great Arthur didn't feel VERY good.\n",
        "This morning, at nine o'clock, Arthur is feeling better.\n",
        "A dog runs in the street.\n",
        "In my city, cats run in the street.\n",
        "A little boy is running in the street.\n",
        "Arthur is my dog, we love to walk in the street together.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5NFJMMpqa51"
      },
      "outputs": [],
      "source": [
        "# Clean\n",
        "sentences2 = nltk.sent_tokenize(text2)\n",
        "cleaned_sentences2 = [CleaningText(sent, reduce = 'stem', list_pos_to_keep = ['NOUN','ADJ','VERB']) for sent in sentences2]\n",
        "\n",
        "# Calculate the tfidf matrix\n",
        "freq_term_DTM2 = vectorized.fit_transform([[w for w, pos in sent] for sent in cleaned_sentences2])\n",
        "tfidf = TfidfTransformer(norm='l1')\n",
        "tfidf_DTM2 = tfidf.fit_transform(freq_term_DTM2)\n",
        "\n",
        "# convert in Dataframe for pedagogical reason\n",
        "tfidf_Matrix2 = pd.DataFrame(tfidf_DTM2.toarray(), columns =  [k for k, v in sorted(vectorized.vocabulary_.items(), key=lambda item: item[1])] )\n",
        "tfidf_Array2 = tfidf_DTM2.toarray()\n",
        "print(tfidf_Matrix2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UVTTVcVcBCA"
      },
      "source": [
        "### <font color = 'E3A440'>4.1.1. Preparation of the training and test corpus</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To execute a supervised method, it is necessary to obtain a training corpus and a test corpus."
      ],
      "metadata": {
        "id": "lEBydIeyPtf9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "NWY4EBh7qa51"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "#split the data\n",
        "X_train, X_test = tfidf_Array2[:5,:], tfidf_Array2[5:,:]\n",
        "y_train, y_test = [1,1,2,2,2],[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7mlhG2wcRQw"
      },
      "source": [
        "### <font color = 'E3A440'>4.1.2. Training</font>\n",
        "\n",
        "In the next chunck of code, the algorithm is first initialized.\n",
        "\n",
        "Then the `.ft()` method is used to run the training. `.fit()` requires `X_train` (training corpus) and `y_train` (the list of tags for the training corpus)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uhb02H1jqa52"
      },
      "outputs": [],
      "source": [
        "clf = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes=5, learning_rate=\"constant\", learning_rate_init=0.01)\n",
        "clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkywIT8rct9w"
      },
      "source": [
        "### <font color = 'E3A440'>4.1.3. Model evaluation</font>\n",
        "\n",
        "The variables `y_valid`, which allows to evaluate the learing error rate, and `y_pred`, which allows to evaluate the prediction error rate, are generated in the following chunck of code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "A5IJVA3Fyr_C"
      },
      "outputs": [],
      "source": [
        "y_valid = clf.predict(X_train)\n",
        "y_pred = clf.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkquI6lbe8l7"
      },
      "source": [
        "The `accuracy_score()` function calculates the evaluation metric, which is the **accuracy**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZKTB0Lwqa52"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Learning errors\")\n",
        "print(\"    Accuracy = \", accuracy_score(y_train, y_valid)*100)\n",
        "\n",
        "print(\"\\nPrediction errors\")\n",
        "print(\"    Accuracy = \", accuracy_score(y_test, y_pred)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZjRjr3zqa53"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "print(\"\\n** Learning performances: **\") \n",
        "print(\"Confusion Matrix: \") \n",
        "print(confusion_matrix(y_train, y_valid))\n",
        "print (\"Accuracy : \", accuracy_score(y_train, y_valid)*100) \n",
        "print(\"** Prediction performances: **\") \n",
        "print(\"Confusion Matrix: \") \n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print (\"Accuracy : \", accuracy_score(y_test,y_pred)*100) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQvxM4dEdypm"
      },
      "source": [
        "# <font color = 'E3A440'>5. Exercise: Analysis of the database 20 Newsgroups</font>\n",
        "\n",
        "The `20 Newsgroups` database is a collection of approximately 20,000 documents segmented into more or less 20 different subject areas.\n",
        "\n",
        "The data are downloaded from the `sklearn.datasets` module and reorganized in tabular format. For pedagogical reasons, this exercise foresees the use of a sample of approximately 3,000 documents segmented into 3 different groups:\n",
        " 1. `rec.autos`, identified with the value `7` in the `target` column\n",
        " 2. `rec.sport.hockey`, identified with the value `10` in the `target` column\n",
        " 3. `sci.med`, identified with the value `13` in the `target` column\n",
        "\n",
        "During the exercise, the participant will be prompted to fill in the missing parts of the code which are indicated with `...` (three dots)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YALuaU53SY6"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "archive20newsgroup = fetch_20newsgroups(subset = 'all')\n",
        "df = pd.DataFrame({'Text': archive20newsgroup.data, 'target': archive20newsgroup.target})\n",
        "target_names = archive20newsgroup.target_names\n",
        "print(target_names)\n",
        "\n",
        "# subset selection\n",
        "target_selected = [7,10,13]\n",
        "df = df[df.target.isin(target_selected)]\n",
        "target_names = [x for idx, x in enumerate(target_names) if idx in target_selected]\n",
        "print(target_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PX2raKXvdOJB"
      },
      "source": [
        "The sample of the database contains 2,979 documents, segmented into 3 themes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1c-fDi_Ck8_"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBaFWoDGdypn"
      },
      "source": [
        "Here are the available variable names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1udmVkRdypn"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONaA5iThdypo"
      },
      "source": [
        "Here is an observation (a row of the data table):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVhmJgQQdypo"
      },
      "outputs": [],
      "source": [
        "df.iloc[284].Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iicfl21wdypm"
      },
      "source": [
        "## <font color = 'E3A440'>5.1 Exercise presentation </font>\n",
        "\n",
        "The exercise consists of two parts:\n",
        "\n",
        " 1. Fill in the missing parts of code (indicated with the `...`)\n",
        " 2. Change preprocessing parameters to experiment the impacts they may have on unsupervised and supervised methods. In particular, you are invited to experiment the following operations:\n",
        "\n",
        "    2.1. Choose a different selection of POS tags (`CleaningText()` function)\n",
        "\n",
        "    2.2. Eliminate words of your choice by adding them to the stopword list (`CleaningText()` function)\n",
        "\n",
        "    2.3. Change the minimum frequency threshold to retain a word in the matrix (`CountVectorizer()` function)\n",
        "\n",
        "    2.4 Add bigrams and trigrams during vectorization (`CountVectorizer()` function)\n",
        "\n",
        "\n",
        "\n",
        "If you want to go further, you could also change the parameters of the unsupervised and supervised algorithms to analyze the impacts on the results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb_ydSo01iFn"
      },
      "source": [
        "Rappel: voici la liste de POS tag existant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVldkntL1iFo"
      },
      "source": [
        "| **POS** | **DESCRIPTION**           | **EXAMPLES**                                      |\n",
        "| ------- | ------------------------- | ------------------------------------------------- |\n",
        "| ADJ     | adjective                 | big, old, green, incomprehensible, first      |\n",
        "| ADP     | adposition                | in, to, during                                |\n",
        "| ADV     | adverb                    | very, tomorrow, down, where, there            |\n",
        "| AUX     | auxiliary                 | is, has (done), will (do), should (do)        |\n",
        "| CONJ    | conjunction               | and, or, but                                  |\n",
        "| CCONJ   | coordinating conjunction  | and, or, but                                  |\n",
        "| DET     | determiner                | a, an, the                                    |\n",
        "| INTJ    | interjection              | psst, ouch, bravo, hello                      |\n",
        "| NOUN    | noun                      | girl, cat, tree, air, beauty                  |\n",
        "| NUM     | numeral                   | 1, 2017, one, seventy-seven, IV, MMXIV        |\n",
        "| PART    | particle                  | ’s, not                                      |\n",
        "| PRON    | pronoun                   | I, you, he, she, myself, themselves, somebody |\n",
        "| PROPN   | proper noun               | Mary, John, London, NATO, HBO                 |\n",
        "| PUNCT   | punctuation               | ., (, ), ?                                    |\n",
        "| SCONJ   | subordinating conjunction | if, while, that                               |\n",
        "| SYM     | symbol                    | $, %, §, ©, +, −, ×, ÷, =, :)               |\n",
        "| VERB    | verb                      | run, runs, running, eat, ate, eating          |\n",
        "| X       | other                     | sfpksdpsxmsa                                  |\n",
        "| SPACE   | space                     |                                                   |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXJr5qVs3x3y"
      },
      "source": [
        "### <font color = 'E3A440'> a. Construction of some functions that will be used later</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "viU9xTMeOdZj"
      },
      "outputs": [],
      "source": [
        "def plot_data_by_cluster(DTM, cls_kmeans, figsize = (16,10) ):\n",
        "    ## Reduction of dimension to 2 for visualisation reasons\n",
        "    from sklearn.manifold import TSNE\n",
        "    import matplotlib.pyplot as plt\n",
        "    import time\n",
        "    time_start = time.time()\n",
        "    tsne = TSNE(n_components=2, verbose=1, perplexity=30, n_iter=1000,metric='cosine', learning_rate=10, random_state = 794)\n",
        "    reduc_dim_results = tsne.fit_transform(DTM)\n",
        "    print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
        "\n",
        "    ## Create data structure for plotting\n",
        "    df_reduction = pd.DataFrame()\n",
        "    df_reduction['y'] =  cls_kmeans.labels_\n",
        "    df_reduction['1-dim'] = reduc_dim_results[:,0]\n",
        "    df_reduction['2-dim'] = reduc_dim_results[:,1]\n",
        "\n",
        "    ## Generate the plot\n",
        "    import seaborn as sns\n",
        "    import colorcet as cc\n",
        "    plt.figure(figsize = figsize)\n",
        "    sns.scatterplot(data = df_reduction,\n",
        "                    x=\"1-dim\",\n",
        "                    y=\"2-dim\",\n",
        "                    hue=\"y\",\n",
        "                    palette = sns.color_palette(cc.glasbey, n_colors = cls_kmeans.n_clusters),)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def prepare_data_for_WC(DTM, vocabulary_dtm):\n",
        "    import scipy\n",
        "    # compute total frequency for each word\n",
        "    values_words = sum(DTM)\n",
        "    # values_words = sum(tfidf_matrix)\n",
        "    # verify type result and prepare data for wordcloud\n",
        "    if type(values_words) is np.ndarray:\n",
        "        values_words = [float(value) for value in np.nditer(values_words)]\n",
        "    elif type(values_words) is scipy.sparse.csr.csr_matrix:\n",
        "        values_words = [float(value) for value in np.nditer(values_words.todense())]\n",
        "    else:\n",
        "        print(\"Matrix in argument DTM has to be one of these two data classes:  'scipy.sparse.csr.csr_matrix' or 'numpy.ndarray'\")\n",
        "    ##Retrieve the word fromthe vocaboulary and sorting them based on the frequency\n",
        "    list_mots = sorted(vocabulary_dtm.items(), key= lambda x:x[1])\n",
        "    list_mots = [word for (word,idx) in  list_mots]\n",
        "    words = zip(list_mots, values_words)\n",
        "    words = sorted(words, key= lambda x:x[1], reverse=True)\n",
        "    ## prepare data structure for wordcloud\n",
        "    result_for_WC = {}\n",
        "    #iterating over the tuples lists\n",
        "    for (key, value) in words:\n",
        "        result_for_WC[key] = value\n",
        "    #\n",
        "    return result_for_WC\n",
        "\n",
        "\n",
        "def wordcloud_par_cluster(wordcloud, DTM, cls_kmeans, vocab, first_n_words=10, figsize=(18, 16), fontsize=32, plot_wordcloud = True, lst_clust = [], title_in_plot = \"Clust_\"):\n",
        "\n",
        "        \"\"\"\n",
        "        wordcloud; A WordCloud function.\n",
        "        DTM; A Docuemnt-Term Matrix\n",
        "        vocab; It is a vocabulary from skllarn vectorizer\n",
        "        first_n_words = 10; How many words to print\n",
        "        figsize = (18, 16); Size of the plot. (this is the argument of this line plt.figure(figsize=figsize))\n",
        "        fontsize = 32; Size of title font\n",
        "        lst_clust = []; The list of cluster to plot. If empty, all the clusters are plotted\n",
        "        title_in_plot = \"Clust_\"; title to put on top of plot \\n\n",
        "        \"\"\"\n",
        "        import numpy\n",
        "        import scipy\n",
        "        \n",
        "        if not lst_clust:\n",
        "            lst_clust = list(range(cls_kmeans.n_clusters))\n",
        "\n",
        "        for x in lst_clust:\n",
        "            DTM_temp = DTM[cls_kmeans.labels_ == x]\n",
        "            result_for_WC= prepare_data_for_WC(DTM_temp, vocab)\n",
        "            ###\n",
        "            if plot_wordcloud == True:\n",
        "                plot = wordcloud.generate_from_frequencies(result_for_WC)\n",
        "                plt.figure(figsize=figsize)\n",
        "                plt.imshow(plot)\n",
        "                plt.title(title_in_plot + str(x) + '  N. of documents=' + str(DTM_temp.shape[0]),\n",
        "                        fontsize = fontsize,\n",
        "                        bbox=dict(facecolor='red', alpha=0.5))\n",
        "                plt.axis('off')\n",
        "                plt.show()\n",
        "            print(f\"Most frequent words for cluster {x} of size {str(DTM_temp.shape[0])} docs: \", list(result_for_WC)[0:first_n_words])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiWwUuMidypo"
      },
      "source": [
        "### <font color = 'E3A440'> b. Annotation, cleaning and vectorization of documents (Reminders)</font>\n",
        "\n",
        "We use the previously written function to clean up the lexical units.\n",
        "\n",
        "For this first test, keep only the names (`NOUN`). Fill in the missing part of the code `...`.\n",
        "\n",
        "This operation will take a few seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "KnDIzjtudypo"
      },
      "outputs": [],
      "source": [
        "cleaned_20news = [CleaningText(sent, reduce = 'lemma', list_pos_to_keep = [...], Stopwords_to_add=['msg']) for sent in list(df['Text'])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcExCuEpdypp"
      },
      "source": [
        "In the vectorization step, we retain the words that appear in at least 15 documents (`min_df = 15`).\n",
        "\n",
        "Fill in the missing portion of code indicated by `...`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "deLwyrVodypp"
      },
      "outputs": [],
      "source": [
        "# Initialisation de l'objet\n",
        "def identity_tokenizer(text):\n",
        "    return text\n",
        "# Transforming the word in frequencies\n",
        "vectorized = CountVectorizer(lowercase = False, # Convert all characters to lowercase before tokenizing\n",
        "                             min_df = ..., # Ignore terms that have a document frequency strictly lower than the given threshold \n",
        "                             max_df = 1200, # Ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words)\n",
        "                             stop_words = stopwords.words('english'), # Remove the list of words provided\n",
        "                             ngram_range = (1, 3), # Get the lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted\n",
        "                             tokenizer=identity_tokenizer) # Override the string tokenization step while preserving the preprocessing and n-grams generation steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXmJqhK_EoBE"
      },
      "outputs": [],
      "source": [
        "freq_term_DTM_20news = vectorized.fit_transform([[w for w, pos in sent if len(w) > 2] for sent in cleaned_20news])\n",
        "freq_term_DTM_20news"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "El2s_eKbdypp"
      },
      "outputs": [],
      "source": [
        "# Calculate the tfidf matrix\n",
        "tfidf = TfidfTransformer(norm='l1')\n",
        "tfidf_DTM_20news = tfidf.fit_transform(freq_term_DTM_20news)\n",
        "#print(tfidf_DTM)\n",
        "\n",
        "tfidf_DF_20news = pd.DataFrame(tfidf_DTM_20news.toarray(), columns =  [k for k, v in sorted(vectorized.vocabulary_.items(), key=lambda item: item[1])] )\n",
        "print(tfidf_DF_20news)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B37DCDHqdypp"
      },
      "source": [
        "## <font color = 'E3A440'>5.2 Unsupervised method : objective of the exercise</font>\n",
        "\n",
        "1. Separate documents into homogeneous groups using clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwYavKEXT14M"
      },
      "source": [
        "### <font color = 'E3A440'> a. Evaluate the best partition</font>\n",
        "In the next chunck of code, you need to choose values ​​for the following variables:\n",
        " 1. `min_k`, which determines the smallest number of clusters to generate for evaluation.\n",
        " 2. `max_k`, which determines the largest number of clusters to generate for evaluation.\n",
        "\n",
        "Fill in the missing portions of code `...`.\n",
        "\n",
        "For the purpose of this exercise, run no more than 30 different segmentations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43eokkyp2_-I"
      },
      "outputs": [],
      "source": [
        "# Code for kl evaluation\n",
        "min_k = ...\n",
        "max_k = ...\n",
        "\n",
        "Sil=np.zeros(max_k)\n",
        "x_axis=np.zeros(max_k)\n",
        "tfidf_DTM_20news_norm = Normalizer().transform(tfidf_DTM_20news)\n",
        "\n",
        "for m in range (min_k, max_k) :\n",
        "\n",
        "    model = KMeans(n_clusters=m, max_iter=100, n_init=1, init=\"k-means++\", random_state=3).fit(tfidf_DTM_20news_norm)\n",
        "\n",
        "    # assign a cluster to each example\n",
        "    labels = model.predict(tfidf_DTM_20news_norm)\n",
        "\n",
        "    Sil[m]=silhouette_score(tfidf_DTM_20news_norm, labels, metric=\"cosine\")\n",
        "    x_axis[m]=m\n",
        "\n",
        "fig = plt.figure(figsize=(7, 7))\n",
        "a1 = fig.add_axes([0,0,1,1])\n",
        "l1 = a1.plot(x_axis, Sil,'ys-') # solid line with yellow colour and square marker\n",
        "a1.set_xlabel('k')\n",
        "a1.set_ylabel('Silhouette')\n",
        "a1.set_title('Evolution of indexes')\n",
        "a1.yaxis.grid(True)\n",
        "a1.xaxis.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eKjz4S7UcEz"
      },
      "source": [
        "### <font color = 'E3A440'> b. Run clustering with k fixed </font>\n",
        "Run the final clustering with a specific number of clusters, that you have chosen by analyzing the previous graph.\n",
        "\n",
        "For consistency with the number of themes in the database, first try with `n_cluster=3`.\n",
        "\n",
        "Fill in the three dots `...`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cyoqur3B2_-G"
      },
      "outputs": [],
      "source": [
        "tfidf_DTM_20news_norm = Normalizer().transform(tfidf_DTM_20news.toarray())\n",
        "model = KMeans(n_clusters = ..., max_iter=200, n_init=1, init=\"k-means++\", random_state=1).fit(tfidf_DTM_20news_norm)\n",
        "# assign a cluster to each example\n",
        "labels = model.predict(tfidf_DTM_20news_norm)\n",
        "\n",
        "print(\"Silhouette=\", silhouette_score(tfidf_DTM_20news_norm, labels, metric=\"cosine\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OEMhbkB5Hpi"
      },
      "outputs": [],
      "source": [
        "model.n_clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwD0TAy35fOK"
      },
      "outputs": [],
      "source": [
        "Counter(model.labels_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHLnCyp64whb"
      },
      "outputs": [],
      "source": [
        "tfidf_DTM_20news_norm = Normalizer().transform(tfidf_DTM_20news.toarray())\n",
        "plot_data_by_cluster(tfidf_DTM_20news_norm, model )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d7dHImnXOdh"
      },
      "source": [
        "### <font color = 'E3A440'> c. Look at the clustering results</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40mECfJP4TK7"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "wordcloud_par_cluster(wordcloud = WordCloud(), # WordCloud function. \n",
        "                      DTM = tfidf_DTM_20news_norm,# Document-Term Matrix \n",
        "                      cls_kmeans = model, # Insert the result of a kmeans clustering\n",
        "                      vocab = vectorized.vocabulary_, # a vocabulary from scikitlearn vectorizer\n",
        "                      first_n_words=10,#  It indicates how many words to print\n",
        "                      figsize=(12, 10),\n",
        "                      fontsize=32,\n",
        "                      plot_wordcloud = False, # Switch to True if you want to plot wordclouds\n",
        "                      lst_clust = [], # Insert a list of integer to get info about a selected number of cluster. It shows info of all the clustrs if empty list\n",
        "                      title_in_plot = \"Clust_\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPJl9Gf_46RN"
      },
      "source": [
        "### <font color = 'E3A440'> d. Evaluate the generated partition with the starting targets of the 20 newsgroup dataset</font>\n",
        "The objective of the next chunck of code is to modify the starting labels of the data, which are in the variable `target`, in order to make them coincide with the labels given by the algorithm.\n",
        "\n",
        "Thus, if a segmentation into 3 clusters has been generated, it will be necessary:\n",
        "\n",
        " 1. Interpret the results and identify which cluster corresponds to which `target` label.\n",
        " 2. Change the labels and make them match the `target`,\n",
        " \n",
        "<font color='E3A440'><bold>For example, if the three-cluster segmentation shows us that cluster `0` looks like subject `rec.autos`, identified with the value `7` in the `target` column , then we need to substitute all `7` with `0`. </bold></font>\n",
        "\n",
        "Here are the labels of the 3 groups:\n",
        " 1. `rec.autos`, identified with the value `7` in the `target` column\n",
        " 2. `rec.sport.hockey`, identified with the value `10` in the `target` column\n",
        " 3. `sci.med`, identified with the value `13` in the `target` column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1ZeB0_7bwzI"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "df['target_converted'] = df.target\n",
        "df['target_converted'] = df.target_converted.apply(lambda x: 0 if x == 7 else x)\n",
        "df['target_converted'] = df.target_converted.apply(lambda x: 1 if x == 10 else x)\n",
        "df['target_converted'] = df.target_converted.apply(lambda x: 2 if x == 13 else x)\n",
        "print(accuracy_score(df.target_converted, model.labels_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS7pyTqNPFY_"
      },
      "source": [
        "Are you able to improve the evaluation metric (`accuracy`) by modifying the inputs of the model? For example, adding bigrams and trigrams?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucIWlhmLc6cM"
      },
      "source": [
        "## <font color = 'E3A440'>5.3 Supervised method : objective of the exercise </font>\n",
        "\n",
        "1. Train a multilayer perceptron to classify documents by subject.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QolMWGXbzlb"
      },
      "source": [
        "### <font color = 'E3A440'> a. Classify messages into target from original dataset</font>\n",
        "In the next chunck of code, a supervised model is generated to recognize documents that are part of one of the following tags:\n",
        "\n",
        " 1. `rec.autos`, identified with the value `7` in the `target` column\n",
        " 2. `rec.sport.hockey`, identified with the value `10` in the `target` column\n",
        " 3. `sci.med`, identified with the value `13` in the `target` column\n",
        "\n",
        "To do this, a training corpus (`X_train`) with a list of labels that correspond to each document (`y_train`) and a test corpus (`X_test` accompanied by `y_test`) are built.\n",
        "\n",
        "The training corpus (`X_train`) will be used to train a multilayer perceptron to classify documents with the correct label. The test corpus `X_test`, will be used to evaluate the performance of the model.\n",
        "\n",
        "The `train_test_split()` function is used to create these corpora, and the `test_size` argument determines how much of the corpus will make up the text corpus (`X_test`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "nnpRZ8OGR7H0"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "tfidf_DTM_20news_norm = Normalizer().transform(tfidf_DTM_20news)\n",
        "X_train, X_test, y_train, y_test = train_test_split(tfidf_DTM_20news_norm, df.target, test_size = 0.2,  random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPwq6Ttha3Ur"
      },
      "source": [
        "Here are the document counts in the training and test corpora."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mBglHFBSiXt"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape)\n",
        "print(len(y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR2tdKEqSl8d"
      },
      "outputs": [],
      "source": [
        "print(X_test.shape)\n",
        "print(len(y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5NwcQqTa-Ti"
      },
      "source": [
        "### <font color = 'E3A440'> b. Training of the model </font>\n",
        "\n",
        "With the `MLPClassifier()` function, a multilayer perceptron is built for the classification of documents into the selected labels.\n",
        "\n",
        "Add the correct variables in the `MLPClassifier()` function to train the neural network.\n",
        "\n",
        "Fill in the empty portions of code `...`.\n",
        "\n",
        "Get inspired by section `4.1.2.` of this script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "OQxFqKC6dypm"
      },
      "outputs": [],
      "source": [
        "cls = MLPClassifier(random_state=1, max_iter=300).fit(..., ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gyLBesUwNTw"
      },
      "source": [
        "### <font color = 'E3A440'> c. Validation of learning performance</font>\n",
        "\n",
        "In the next chunck of code, the test corpus (`X_test`) is used to evaluate the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhqu_NAWwNTx"
      },
      "source": [
        "\n",
        "Add the right variables to generate the `y_valid`, which allows to evaluate the learing error rate and the `y_pred`, which allows to evaluate the prediction errors rate.\n",
        "\n",
        "Fill in the empty portions of code `...`.\n",
        "\n",
        "Take inspiration from section `4.1.3.` of this script.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "TFUCYUYsUTEU"
      },
      "outputs": [],
      "source": [
        "y_valid = cls.predict(...)\n",
        "y_pred = cls.predict(...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMeR_qQrddpS"
      },
      "outputs": [],
      "source": [
        "print(\"Learning errors\")\n",
        "print(\"    Accurary = \", accuracy_score(y_train, y_valid)*100)\n",
        "\n",
        "print(\"\\nPrediction errors\")\n",
        "print(\"    Accurary = \", accuracy_score(y_test, y_pred)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdlhRa3ZwNTx"
      },
      "outputs": [],
      "source": [
        "print(confusion_matrix(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = 'E3A440'> 6. Conclusion </font>\n",
        "\n",
        "In this workshop, you learned how to manipulate text data for semantic analysis. You have been able to experience the impacts of preprocessing on the results of learning algorithms.\n"
      ],
      "metadata": {
        "id": "Jra1YMCtlyRI"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}