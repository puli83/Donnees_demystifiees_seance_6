{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWNgJtQ3FrN8"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "<font size='10' color = 'E3A440'>**Megadata and Advanced Techniques Demystified**</font>\n",
        "=======\n",
        "<font color = 'E3A440'>*New Analysis Methods and their Implications for Megadata Management in SSH (part 2)*</font>\n",
        "=============\n",
        "\n",
        "\n",
        "This workshop is part of the training [Megadata and Advanced Techniques Demystified](https://www.4point0.ca/en/2022/08/22/formation-megadonnees-demystifiees//) (session 6).\n",
        "\n",
        "Humanities and social sciences are often confronted with the analysis of unstructured data, such as text. After preparing the data, several analysis techniques from machine learning can be used. During this workshop, participants will be introduced to the preprocessing of textual data and to supervised and unsupervised methods for analysis purposes with Python.\n",
        "\n",
        "\n",
        "Structure of the workshop :\n",
        "1. Part 1 : Examples of supervised and unsupervised methods for text mining\n",
        "2. Part 2 : Exercices\n",
        "\n",
        "### Autors: \n",
        "- Bruno Agard <bruno.agard@polymtl.ca>\n",
        "- Davide Pulizzotto <davide.pulizzotto@polymtl.ca>\n",
        "\n",
        "Département de Mathématiques et de génie industriel\n",
        "\n",
        "École Polytechnique de Montréal"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = 'E3A440'>0. Preparation environnement </font>"
      ],
      "metadata": {
        "id": "HpAX32-4Kj8U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jddhTL7EFrN_"
      },
      "outputs": [],
      "source": [
        "# Downloading of data from the GitHub project\n",
        "!rm -rf Data_techniques_demystified_webinars/\n",
        "!git clone https://github.com/4point0-ChairInnovation-Polymtl/Data_techniques_demystified_webinars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJLRFWa6FrOA"
      },
      "outputs": [],
      "source": [
        "# Import modules\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.preprocessing import Normalizer\n",
        "import matplotlib.pyplot as plt\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwFKNpPQFrN-"
      },
      "source": [
        "<a name='Section_1'></a>\n",
        "# <font color = 'E3A440'>1. *Preparation of textual data* (wrap up)</font>\n",
        "\n",
        "The pre-processing of a corpus of texts may require the implementation of several steps including: the splitting of sentences, words, cleaning, filtering, etc.\n",
        "\n",
        "In the next blocks of code, a text will be segmented into sentences and preprocessed using the `CleaningText()` function prepared in the previous session (October 27th, 2022).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOzWtcugFrOK"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"At eight o'clock, on Thursday morning, the great Arthur didn't feel VERY good.\n",
        "This morning, Arthur is feeling better.\n",
        "A dog runs in the street.\n",
        "A little boy in running in the street.\n",
        "Arthur is my dog, he sleeps every morning.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLrvRTIGdypS",
        "outputId": "a229fc63-71a1-4649-ecb9-5f47c4905caa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"At eight o'clock, on Thursday morning, the great Arthur didn't feel VERY good.\", 'This morning, Arthur is feeling better.', 'A dog runs in the street.', 'A little boy in running in the street.', 'Arthur is my dog, he sleeps every morning.']\n"
          ]
        }
      ],
      "source": [
        "# extraction os sentences\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "print(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rtngk2rFrOL"
      },
      "outputs": [],
      "source": [
        "# Cleaning fonction to preprocess text\n",
        "def CleaningText(text_as_string, language = 'english', reduce = '', list_pos_to_keep = [], Stopwords_to_add = []):\n",
        "    from nltk.corpus import stopwords\n",
        "\n",
        "    words = nltk.word_tokenize(text_as_string)\n",
        "    words_pos = nltk.pos_tag(words, tagset='universal')\n",
        "    words_pos = [(w, pos) for w, pos in words_pos if w.isalnum()]\n",
        "    words_pos = [(w.lower(), pos) for w, pos in words_pos]\n",
        "    \n",
        "    if reduce == 'stem': \n",
        "        from nltk.stem.porter import PorterStemmer\n",
        "        reduced_words_pos = [(PorterStemmer().stem(w), pos) for w, pos in words_pos]\n",
        "        \n",
        "    elif reduce == 'lemma':\n",
        "        from nltk.stem.wordnet import WordNetLemmatizer\n",
        "        reduced_words_pos = [(WordNetLemmatizer().lemmatize(w), pos) for w, pos in words_pos]\n",
        "    else:\n",
        "        import warnings\n",
        "        reduced_words_pos = words_pos\n",
        "        warnings.warn(\"Warning : any reduction was made on words! Please, use \\\"reduce\\\" argument to chosse between 'stem' or  'lemma'\")\n",
        "    if list_pos_to_keep:\n",
        "        reduced_words_pos = [(w, pos) for w, pos in reduced_words_pos if pos in list_pos_to_keep]\n",
        "    else:\n",
        "        import warnings\n",
        "        warnings.warn(\"Warning : any POS filtering was made. Please, use \\\"list_pos_to_keep\\\" to create a list of POS tag to keep.\")\n",
        "    \n",
        "    list_stopwords = stopwords.words(language) + Stopwords_to_add\n",
        "    reduced_words_pos = [(w, pos) for w, pos in reduced_words_pos if w not in list_stopwords and len(w) > 1 ]\n",
        "    return reduced_words_pos   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkpxyQfl2Wna",
        "outputId": "cced0944-9a6c-426f-8f39-069189c537ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('thursday', 'NOUN'), ('morn', 'NOUN'), ('great', 'ADJ'), ('arthur', 'NOUN'), ('feel', 'VERB'), ('good', 'ADJ')], [('morn', 'NOUN'), ('arthur', 'NOUN'), ('feel', 'VERB')], [('dog', 'NOUN'), ('run', 'VERB'), ('street', 'NOUN')], [('littl', 'ADJ'), ('boy', 'NOUN'), ('run', 'VERB'), ('street', 'NOUN')], [('arthur', 'NOUN'), ('dog', 'NOUN'), ('sleep', 'VERB'), ('morn', 'NOUN')]]\n"
          ]
        }
      ],
      "source": [
        "# nettoyage des phrases, sélection de pos-tag\n",
        "cleaned_sentences = [CleaningText(sent, reduce = 'stem', list_pos_to_keep = ['NOUN','ADJ','VERB']) for sent in sentences]\n",
        "print(cleaned_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici la liste de POS tag existant."
      ],
      "metadata": {
        "id": "Vs4Y_iuW1YyL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **POS** | **DESCRIPTION**           | **EXAMPLES**                                      |\n",
        "| ------- | ------------------------- | ------------------------------------------------- |\n",
        "| ADJ     | adjective                 | big, old, green, incomprehensible, first      |\n",
        "| ADP     | adposition                | in, to, during                                |\n",
        "| ADV     | adverb                    | very, tomorrow, down, where, there            |\n",
        "| AUX     | auxiliary                 | is, has (done), will (do), should (do)        |\n",
        "| CONJ    | conjunction               | and, or, but                                  |\n",
        "| CCONJ   | coordinating conjunction  | and, or, but                                  |\n",
        "| DET     | determiner                | a, an, the                                    |\n",
        "| INTJ    | interjection              | psst, ouch, bravo, hello                      |\n",
        "| NOUN    | noun                      | girl, cat, tree, air, beauty                  |\n",
        "| NUM     | numeral                   | 1, 2017, one, seventy-seven, IV, MMXIV        |\n",
        "| PART    | particle                  | ’s, not                                      |\n",
        "| PRON    | pronoun                   | I, you, he, she, myself, themselves, somebody |\n",
        "| PROPN   | proper noun               | Mary, John, London, NATO, HBO                 |\n",
        "| PUNCT   | punctuation               | ., (, ), ?                                    |\n",
        "| SCONJ   | subordinating conjunction | if, while, that                               |\n",
        "| SYM     | symbol                    | $, %, §, ©, +, −, ×, ÷, =, :)               |\n",
        "| VERB    | verb                      | run, runs, running, eat, ate, eating          |\n",
        "| X       | other                     | sfpksdpsxmsa                                  |\n",
        "| SPACE   | space                     |                                                   |"
      ],
      "metadata": {
        "id": "X0CQ8_vP1W-W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'>*3. Vectorization*</font>\n",
        "\n",
        "Typically, to use text in a data analysis or machine learning context, text must be transformed into an appropriate mathematical object.\n",
        "The simplest and most widespread model is the \"bags-of-words\", in which each text (or each text fragment) is defined in a vector, by a certain number of lexical units which characterize it. This model belongs to the family of vector semantics models and it has the following form:\n",
        "\n",
        "\n",
        "$$X = \\begin{bmatrix} \n",
        "x_{1,1} & x_{1,2} & \\ldots & x_{1,w} \\\\\n",
        "\\vdots & \\vdots       &  \\ddots      & \\vdots \\\\ \n",
        "x_{n,1} & x_{1,2} & \\ldots & x_{n,w} \\\\\n",
        "\\end{bmatrix}\n",
        "$$ \n",
        "\n",
        "In this matrix, the value $x_{i,j}$ represents the \"weigth\" of the word $j$ in the text fragment $i$. This weigth can be computed in several way. Thus :\n",
        "\n",
        "- $x_{i,j}$ can represents the presence of the word \"j\" in text fragment $i$,\n",
        "- $x_{i,j}$ can measures the quantoty of occurrences of a word $j$ in text fragment $i$,\n",
        "- $x_{i,j}$ can represent the **value** of the word $j$ in text fragment $i$, and this, using metric such as tf-idf :\n",
        " $$\\text{tf-idf}_{i,j}=\\text{tf}_{i,j}.log\\left(\\frac{n}{n_i}\\right)$$\n",
        " - $\\text{tf}_{i,j}$ is the frequency of word $i$ in text fragment $j$,\n",
        " - $n$ total count of text fragments,\n",
        " - $n_i$ total counts of text fragments containing the word $i$.\n"
      ],
      "metadata": {
        "id": "m0YdoOA1stB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Object initialization\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def identity_tokenizer(text):\n",
        "    return text\n",
        "\n",
        "# Transforming the word in frequencies\n",
        "vectorized = CountVectorizer(lowercase = False, # Convert all characters to lowercase before tokenizing\n",
        "                             min_df = 1, # Ignore terms that have a document frequency strictly lower than the given threshold \n",
        "                             max_df = 10, # Ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words)\n",
        "                             stop_words = stopwords.words('english'), # Remove the list of words provided\n",
        "                             ngram_range = (1, 1), # Get the lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted\n",
        "                             tokenizer=identity_tokenizer) # Override the string tokenization step while preserving the preprocessing and n-grams generation steps"
      ],
      "metadata": {
        "id": "LpFPJrGI2Nsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use of the \"vectorizer\" with a list of word lists (and not a list of word-pos tuples)."
      ],
      "metadata": {
        "id": "G9uC3usB5dtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of list of words:\n",
        "[[w for w, pos in sent] for sent in cleaned_sentences]"
      ],
      "metadata": {
        "id": "gy8_VTM67Lax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Application of the vectorizer\n",
        "freq_term_DTM = vectorized.fit_transform([[w for w, pos in sent] for sent in cleaned_sentences])\n",
        "print(pd.DataFrame(freq_term_DTM.todense(), columns =  [k for k, v in sorted(vectorized.vocabulary_.items(), key=lambda item: item[1])] ))"
      ],
      "metadata": {
        "id": "v8ydVdmC5c76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thus, we assign the result of the Tf-IDF weighting to the variable named `tfidf_DTM`. "
      ],
      "metadata": {
        "id": "pRPjGQVokg7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the tfidf matrix\n",
        "tfidf = TfidfTransformer(norm='l1')\n",
        "tfidf_DTM = tfidf.fit_transform(freq_term_DTM)\n",
        "print(pd.DataFrame(tfidf_DTM.todense(), columns =  [k for k, v in sorted(vectorized.vocabulary_.items(), key=lambda item: item[1])] ))"
      ],
      "metadata": {
        "id": "4GsktR7_Znke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QymfV4S_FrOZ"
      },
      "source": [
        "<a name=\"Section_2\"></a>\n",
        "# <font color = 'E3A440'> 2. *Exercise : Sentiment Analysis on Twitter* </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The exercise proposed in this section is based on a simple processing chain for **sentiment analysis** on Twitter data and **analysis of lexical specificities**.\n",
        "\n",
        "The corpus used was collected in 2020 by *trackmyhashtag.com* and contains 150,000 tweets for the 50 most followed profiles on Twitter. The data is in tabular format in a CSV file. For pedagogical reasons, this exercise foresees the use of a random sample of 5,000 tweets.\n",
        "\n",
        "First, the textual data of 5,000 tweets will be analyzed by a sentiment analysis module of the `nltk` module. Then the text will be preprocessed and some lexical analysis will be performed.\n",
        "\n",
        "During the exercise, the participant will be invited to fill the missing parts of the code which are indicated with `...` (three dots)."
      ],
      "metadata": {
        "id": "c8XIxHR-n0Oi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'E3A440'> 2.1 Presentation of the exercise </font>"
      ],
      "metadata": {
        "id": "FAgZaK170IHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'> a. Import data </font>\n",
        "\n",
        "The file with the data is archived in a `.zip` and contains more than 150,000 tweets. For educational reasons, we only import 5,000 random tweets. "
      ],
      "metadata": {
        "id": "IxtJNukjCXT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_DIR='Data_techniques_demystified_webinars/'\n",
        "DATA_DIR=os.path.join(ROOT_DIR, 'Data')\n",
        "import zipfile\n",
        "from datetime import datetime\n",
        "\n",
        "#Unzips the dataset and gets the TSV dataset\n",
        "with zipfile.ZipFile(os.path.join(DATA_DIR,'4POINT0_Top_50_tweet_profiles.zip'), 'r') as zip_ref:\n",
        "    zip_ref.extractall(DATA_DIR)\n",
        "\n",
        "df = pd.read_pickle(os.path.join(DATA_DIR,'Top_50_tweet_profiles.pkl')).sample(5000, random_state = 5641).reset_index()"
      ],
      "metadata": {
        "id": "Q6snBPy8EfBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here available variables in the dataset."
      ],
      "metadata": {
        "id": "hDTMmSBhFA6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "ejgQAfv8FIrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here an observation (one row of the table of data):"
      ],
      "metadata": {
        "id": "JIDKgUxtGhQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[0]"
      ],
      "metadata": {
        "id": "54zPNygEJXhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'> b. Run Sentiment Analysis </font>\n",
        "\n",
        "The `SentimentIntensityAnalyzer` object is used to perform sentiment analysis. The object must be initialized and then the `polarity_scores()` function can be applied to a string."
      ],
      "metadata": {
        "id": "ym3Y6ITXGkkj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pInoj9yrFrOZ"
      },
      "outputs": [],
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "sia = SentimentIntensityAnalyzer()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are three examples of sentiment analysis. The result of the `polarity_scores()` function returns four values:\n",
        "\n",
        " 1. `neg` : indicates the degree, on a scale from 0 to 1, of negative sentiment of the text.\n",
        " 2. `neu` : indicates the degree, on a scale of 0 to 1, of neutral sentiment of the text.\n",
        " 3. `pos` :indicates the degree, on a scale of 0 to 1, of positive sentiment of the text.\n",
        " 4. `compound` : contains a composite value of the previous three metrics with a range from -1 to 1.\n",
        "\n"
      ],
      "metadata": {
        "id": "sggH1sSIIVib"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jAgx5YgFrOa"
      },
      "outputs": [],
      "source": [
        "sia.polarity_scores(\"Wow, Montreal Canadiens is the greatest hockey team in the world!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sia.polarity_scores(\"Ottawa is not bad city!\")"
      ],
      "metadata": {
        "id": "ifi6teSvITFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpGJWGRWFrOa"
      },
      "outputs": [],
      "source": [
        "sia.polarity_scores(\"No, you cannot put pineapple on a pizza! This is disgusting!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are tweets on which we will apply the sentiment analysis:"
      ],
      "metadata": {
        "id": "3gfG4_A6I4WK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Tweet Content']"
      ],
      "metadata": {
        "id": "5yBg-jU9QLl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next block of code, we run sentiment analysis on the `Tweet Content` column, and add the resulting results to the data table (the object named `df`)."
      ],
      "metadata": {
        "id": "aCfTrGKYNOcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Running Sentiment Analysis on the Corpus\n",
        "datasent = df.apply(lambda x: sia.polarity_scores(x['Tweet Content']), 1)\n",
        "df = df.join(pd.DataFrame(list(datasent)))"
      ],
      "metadata": {
        "id": "3etQFxEACpty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result of the analysis is saved in 4 variables. Here is an example:"
      ],
      "metadata": {
        "id": "bpauZXt7Nwyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[0]"
      ],
      "metadata": {
        "id": "yfrp02peNwSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make the analysis simple, we will only use the compound metric `compound` which is automatically calculated by the `polarity_score()` function."
      ],
      "metadata": {
        "id": "wsjs2wOmbs0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['compound'].describe()"
      ],
      "metadata": {
        "id": "Xf_vn9Veebrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use the `compound` metric in a **lexical specificity analysis** context, it is necessary to constitute categories, i.e. to group the tweets under the following categories:\n",
        " 1. `negative` : which groups tweets containing negative sentiment (`compound` from -1 to -0.1) \n",
        " 2. `neu` : which groups tweets that are more neutral (`compound` from -0.5 to 0.5)\n",
        " 3. `positive` : which groups tweets containing a positive sentiment (`compound` more than 0.5)"
      ],
      "metadata": {
        "id": "A6cLssRqb81P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 Determine the values ​​to cut the compound metric\n",
        "bins = [-1, -0.1, 0.5, 1]\n",
        "# 2 Determine the names of the categories. NOTE that the numbers of category names must be less than the cut values.\n",
        "names = ['negative', 'neu', 'positive']\n",
        "# Execute slicing with pandas 'cut' function.\n",
        "df['compound_category']  = pd.cut(df['compound'], bins, labels=names, include_lowest =True)"
      ],
      "metadata": {
        "id": "jJrj4DKePhGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the distribution of tweets by category:"
      ],
      "metadata": {
        "id": "-cRjoTWDdJjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Counter(df['compound_category'])"
      ],
      "metadata": {
        "id": "UL-ibgrb4u0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'> c. Annotation, cleaning and vectorization of tweets </font>\n",
        "\n",
        "We use the previously written function to clean the lexical units of tweets. For this first test, we keep only the adjectives.\n",
        "\n",
        "This operation will take a few seconds."
      ],
      "metadata": {
        "id": "_zmlEGCKRaWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_tweets = [CleaningText(sent, reduce = 'lemma', list_pos_to_keep = ['ADJ'], Stopwords_to_add=['http']) for sent in list(df['Tweet Content'])]"
      ],
      "metadata": {
        "id": "NMmjI8IaU_JF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the vectorization step we retain the words that appear in at least 5 documents (`min_df = 5`)."
      ],
      "metadata": {
        "id": "TA9YRrWiLzw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Object initialization\n",
        "def identity_tokenizer(text):\n",
        "    return text\n",
        "# Transforming the word in frequencies\n",
        "vectorized = CountVectorizer(lowercase = False, # Convert all characters to lowercase before tokenizing\n",
        "                             min_df = 5, # Ignore terms that have a document frequency strictly lower than the given threshold \n",
        "                             max_df = 4500, # Ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words)\n",
        "                             stop_words = stopwords.words('english'), # Remove the list of words provided\n",
        "                             ngram_range = (1, 1), # Get the lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted\n",
        "                             tokenizer=identity_tokenizer) # Override the string tokenization step while preserving the preprocessing and n-grams generation steps"
      ],
      "metadata": {
        "id": "6Aj4JUdeYBbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq_term_DTM = vectorized.fit_transform([[w for w, pos in sent] for sent in cleaned_tweets])\n",
        "freq_term_DTM"
      ],
      "metadata": {
        "id": "wkSkuQvUXsvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'> d. Analysis of lexical specificities </font>\n",
        "\n",
        "The analysis of lexical specificities makes it possible to highlight the lexical units which are specific to a particular group of data. In our case, it is possible to identify the words that are more strongly associated with positive or negative feelings.\n",
        "\n",
        "To do this, we use a widely used metric in lexicometry, which is the likelihood function (log-likelihood ratio). The metric is based on this [article](https://aclanthology.org/J93-1003.pdf). Other methods can be used, such as mutual information, chi2 or tf-idf weighting."
      ],
      "metadata": {
        "id": "RtSfYp1rfUxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def GetLexicalSpecificities(freq_term_DTM, logical_vector):\n",
        "    # This code ref takes inspiration from this python module : https://pypi.org/project/corpus-toolkit/\n",
        "    # and its main script:  https://github.com/kristopherkyle/corpus_toolkit/blob/master/corpus_toolkit/corpus_tools.py\n",
        "    # which is based on this paper: https://aclanthology.org/J93-1003/\n",
        "    import math\n",
        "    df_freq_target = pd.DataFrame(np.asarray(freq_term_DTM[logical_vector].sum(0).T).reshape(-1))\n",
        "    df_freq_target.index = [word for (word,idx) in sorted(vectorized.vocabulary_.items(), key= lambda x:x[1])]\n",
        "    df_freq_target.columns = ['freq1']\n",
        "    df_freq_target['freq2'] = np.asarray(freq_term_DTM[~(logical_vector)].sum(0).T).reshape(-1)\n",
        "    df_freq_target['tot'] = df_freq_target['freq1'] + df_freq_target['freq2']\n",
        "\n",
        "    df_freq_target['freq1'] = df_freq_target['freq1'].apply(lambda x: 0.0000001 if x == 0 else x).astype(float)\n",
        "    df_freq_target['freq2'] = df_freq_target['freq2'].apply(lambda x: 0.0000001 if x == 0 else x).astype(float)\n",
        "    #\n",
        "    df_freq_target['freq1_norm'] = df_freq_target['freq1']/df_freq_target['freq1'].sum() * 1000000\n",
        "    df_freq_target['freq2_norm'] = df_freq_target['freq2']/df_freq_target['freq2'].sum() * 1000000\n",
        "    #\n",
        "    df_freq_target['fraction'] = df_freq_target['freq1_norm'] / df_freq_target['freq2_norm']\n",
        "    df_freq_target['Log-likelihood Ratio'] = df_freq_target['fraction'].apply(math.log2)\n",
        "    frequency_threshold = 10 # Insert your frequency threshold as integer\n",
        "    return df_freq_target[df_freq_target['tot'] > frequency_threshold]['Log-likelihood Ratio'].sort_values(ascending=False).iloc[range(50)]"
      ],
      "metadata": {
        "id": "39eDHLjFZcLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To perform the specificity analysis, it is necessary to create a logical vector (with binary values) which indicates with `True` the class for which we want to analyze the lexical specificity and with `False` the rest of the corpus."
      ],
      "metadata": {
        "id": "SIxwMCSahwh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logical_vector = df['compound_category'] == 'positive'\n",
        "logical_vector"
      ],
      "metadata": {
        "id": "TVhrPm7PZO5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(logical_vector)"
      ],
      "metadata": {
        "id": "ogaDNEUNZQU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the function with the frequency matrix (`freq_term_DTM`) and the logical vector we created above."
      ],
      "metadata": {
        "id": "G3lrLVXfiMMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GetLexicalSpecificities(freq_term_DTM, logical_vector)"
      ],
      "metadata": {
        "id": "8iEQNaJvJ2fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del logical_vector, freq_term_DTM"
      ],
      "metadata": {
        "id": "F9vvmgX46O7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'E3A440'> 2.2 Exercice </font>\n",
        "\n",
        "During the exercise, the participant are invited to fill the missing parts of the code which are indicated with `...` (three dots).\n",
        "\n",
        "Several manipulations and different results will be required. Each sub-exercise follows this processing chain:\n",
        "\n",
        "1. Annotation and cleaning of tweets: the participant will have to adjust some parameters of the function to choose a specific filtering.\n",
        "2. Vectorization: the participant will have to adjust some parameters of the function to choose a specific filtering.\n",
        "3. Creation of a logical vector to define the target group and the reference group.\n",
        "4. Application of the `GetLexicalSpecificities()` function to obtain the 50 most specific words for the target group.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8zRK37miNzH6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'> a. Study the impact of morphosyntactic filtering on lexical specificities </font>\n",
        "\n",
        "In point 2.1, only adjectives have been studied. Now do a study on nouns, adjectives and verbs and then on other combinations that are interesting for you.\n",
        "Here is the list of existing POS tags:"
      ],
      "metadata": {
        "id": "Niv5IUK8VJ8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **POS** | **DESCRIPTION**           | **EXAMPLES**                                      |\n",
        "| ------- | ------------------------- | ------------------------------------------------- |\n",
        "| ADJ     | adjective                 | big, old, green, incomprehensible, first      |\n",
        "| ADP     | adposition                | in, to, during                                |\n",
        "| ADV     | adverb                    | very, tomorrow, down, where, there            |\n",
        "| AUX     | auxiliary                 | is, has (done), will (do), should (do)        |\n",
        "| CONJ    | conjunction               | and, or, but                                  |\n",
        "| CCONJ   | coordinating conjunction  | and, or, but                                  |\n",
        "| DET     | determiner                | a, an, the                                    |\n",
        "| INTJ    | interjection              | psst, ouch, bravo, hello                      |\n",
        "| NOUN    | noun                      | girl, cat, tree, air, beauty                  |\n",
        "| NUM     | numeral                   | 1, 2017, one, seventy-seven, IV, MMXIV        |\n",
        "| PART    | particle                  | ’s, not                                      |\n",
        "| PRON    | pronoun                   | I, you, he, she, myself, themselves, somebody |\n",
        "| PROPN   | proper noun               | Mary, John, London, NATO, HBO                 |\n",
        "| PUNCT   | punctuation               | ., (, ), ?                                    |\n",
        "| SCONJ   | subordinating conjunction | if, while, that                               |\n",
        "| SYM     | symbol                    | $, %, §, ©, +, −, ×, ÷, =, :)               |\n",
        "| VERB    | verb                      | run, runs, running, eat, ate, eating          |\n",
        "| X       | other                     | sfpksdpsxmsa                                  |\n",
        "| SPACE   | space                     |                                                   |\n"
      ],
      "metadata": {
        "id": "CJps3VlyWcoV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insert the correct value for the `list_pos_to_keep` argument in roder to keep nouns, adjectives and verbs, or any other POS tag combination of your interest.\n",
        "Look at the three dots `...` and fill it! You should inspire to prvious code presentend during this workshop. Copy-Paste is permitted!"
      ],
      "metadata": {
        "id": "b4bLCBgqWlzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Annotation and cleaning : ADD adjective and verbs as POS tag to keep\n",
        "cleaned_tweets = [CleaningText(sent, reduce = 'lemma', list_pos_to_keep = [...], Stopwords_to_add=['http']) for sent in list(df['Tweet Content'])]"
      ],
      "metadata": {
        "id": "xtrWcUOmNxOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change the `min_df` parameters so that you don't exceed **750 words** of your matrix <font color='E3A440'>**Document-Term matrix**</font>, which is saved in the object `freq_term_DTM`.\n",
        "\n",
        "Note that in this function the `ngram_range` parameter is configured to have unigrams and bigrams (its value is: `(1,2)`)."
      ],
      "metadata": {
        "id": "g6JapeP7W2xO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Vectorisation\n",
        "def identity_tokenizer(text):\n",
        "    return text\n",
        "    \n",
        "## 2.1 initialise with parameters : \n",
        "vectorized = CountVectorizer(lowercase = False, # Convert all characters to lowercase before tokenizing\n",
        "                             min_df = ..., # Ignore terms that have a document frequency strictly lower than the given threshold \n",
        "                             max_df = 4500, # Ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words)\n",
        "                             stop_words = stopwords.words('english'), # Remove the list of words provided\n",
        "                             ngram_range = (1, 2), # Get the lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted\n",
        "                             tokenizer=identity_tokenizer) # Override the string tokenization step while preserving the preprocessing and n-grams generation steps\n",
        "\n",
        "#\n",
        "freq_term_DTM = vectorized.fit_transform([[w for w, pos in sent] for sent in cleaned_tweets])\n",
        "\n",
        "freq_term_DTM"
      ],
      "metadata": {
        "id": "EoTIyCR7daI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the work already done in point **b.** of section **2.1**, choose the categories for which you want to study the lexical specificity ex. `negative` or `positive`."
      ],
      "metadata": {
        "id": "PC98Vv87YCL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logical_vector = df['compound_category'] == ..."
      ],
      "metadata": {
        "id": "B-qMBlPhPGC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the function defined in point **d.** of section **2.1**, add the fundamental arguments of the function, i.e. the matrix <font color='E3A440'>**Document-Term matrix**</font> and the **logical vector** created in the previous code block."
      ],
      "metadata": {
        "id": "kUkik8fxYXe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function needs 2 arguemnts: 1st is the matrix, the 2nd is the logical vector\n",
        "GetLexicalSpecificities(..., ...)"
      ],
      "metadata": {
        "id": "CGFUrGPVPJWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'> b. Investigate new categories based on the number of Retweets </font>\n",
        "\n",
        "On Twitter, it is possible to retweet an existing tweet. The number of retweets can be considered an indicator of the interest a tweet has obtained.\n",
        "\n",
        "Answer the following question: what are the lexical specificities of tweets that have had a very large following?\n",
        "\n",
        "To answer, you must manipulate some line of code by performing the steps learned throughout this workshop."
      ],
      "metadata": {
        "id": "AcDMzwLhaFyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the distribution of the `Retweets received` column."
      ],
      "metadata": {
        "id": "cNHccN-mc6Gj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Retweets received'].describe()"
      ],
      "metadata": {
        "id": "3tnUU_iVQZBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following the percentiles that are displayed in the distribution of the `Retweets received` column (result of the previous chunk of code), add the missing slicing values to the `bins` list in the next chunk of code.\n",
        "Divide the number of Retweets into four categories:\n",
        "1. `low`, grouping tweets that have received a low interest\n",
        "2. `medium`, grouping tweets that have received a medium interest\n",
        "3. `high`, grouping tweets that have received a high interest\n",
        "4. `very_high`, grouping tweets that have received a very high interest"
      ],
      "metadata": {
        "id": "RPuQijA6dAR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bins = [-np.inf, 161, ..., ..., 449711]\n",
        "names = ['low', 'medium', 'high', 'very_high']\n",
        "df['Retweets_received_category']  = pd.cut(df['Retweets received'], bins, labels=names, include_lowest =True)"
      ],
      "metadata": {
        "id": "Z25CYh43QRBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose the **target category** for which you want analyze the lexical specificities. The value must be one of the four values ​​contained in the `Retweets_received_category` column generated in the previous chunk of code."
      ],
      "metadata": {
        "id": "NcugGIyneFIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logical_vector = df['Retweets_received_category'] == ..."
      ],
      "metadata": {
        "id": "WT1B3Iq-RIPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the specificity analysis."
      ],
      "metadata": {
        "id": "NQ9FdJfnf9qW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GetLexicalSpecificities(freq_term_DTM, logical_vector)"
      ],
      "metadata": {
        "id": "q6kVV9xjRM9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'> c. Study the lexical specificities ​​of different Tweet profiles </font>\n",
        "\n",
        "In the next exercise, select two or three Twitter profiles of your choice and compare the lexical specificities by studying several POS tag combinations. \n",
        "\n",
        "What are the main lexical differences between the profiles you have chosen?\n",
        "\n",
        "Here is the complete list of profiles present in the corpus and recorded under the `Profile Account` column and the number of tweets per profile."
      ],
      "metadata": {
        "id": "uIiCPShTgE8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Counter(df['Profile Account'])"
      ],
      "metadata": {
        "id": "Jfl6H0JV0v53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Annotation and cleaning : ADD adjective and verbs as POS tag to keep\n",
        "cleaned_tweets = [CleaningText(sent, reduce = 'lemma', list_pos_to_keep = [...], Stopwords_to_add=['http']) for sent in list(df['Tweet Content'])]\n",
        "\n",
        "# 2. Vectorisation\n",
        "def identity_tokenizer(text):\n",
        "    return text\n",
        "## 2.1 initialise with parameters : \n",
        "vectorized = CountVectorizer(lowercase = False, # Convert all characters to lowercase before tokenizing\n",
        "                             min_df = 10, # Ignore terms that have a document frequency strictly lower than the given threshold \n",
        "                             max_df = 4500, # Ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words)\n",
        "                             stop_words = stopwords.words('english'), # Remove the list of words provided\n",
        "                             ngram_range = (1, 1), # Get the lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted\n",
        "                             tokenizer=identity_tokenizer) # Override the string tokenization step while preserving the preprocessing and n-grams generation steps\n",
        "\n",
        "#\n",
        "freq_term_DTM = vectorized.fit_transform([[w for w, pos in sent] for sent in cleaned_tweets])\n",
        "\n",
        "freq_term_DTM"
      ],
      "metadata": {
        "id": "yYTkjK6jgWa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logical_vector = df['Profile Account'] == ...\n",
        "GetLexicalSpecificities(freq_term_DTM, logical_vector)"
      ],
      "metadata": {
        "id": "8-o-jsBI0pof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'E3A440'> 2.3 NOTES PERSONNELLES: </font>\n",
        "\n",
        "-----\n",
        "\n",
        "-----"
      ],
      "metadata": {
        "id": "IhZSp6JKjH8k"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}