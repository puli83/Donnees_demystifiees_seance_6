{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWNgJtQ3FrN8"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "<font size='10' color = 'E3A440'>**Mégadonnées et techniques avancées démystifiées**</font>\n",
        "=======\n",
        "<font color = 'E3A440'>*Nouvelles méthodes d’analyse et leur implication quant à la gestion des mégadonnées en SSH (partie 2)*</font>\n",
        "=============\n",
        "\n",
        "\n",
        "Cet atelier s’inscrit dans le cadre de la formation [Mégadonnées et techniques avancées démystifiées](https://www.4point0.ca/2022/08/22/formation-megadonnees-demystifiees/) (séance 7).\n",
        "\n",
        "Les sciences humaines et sociales sont souvent confrontées à l’analyse de données non structurées, comme le texte. Après avoir préparé les données, plusieurs techniques d’analyse venant de l’apprentissage automatique peuvent être utilisées. Pendant cet atelier, les participants seront initiés au prétraitement des données textuelles et aux méthodes supervisées et non supervisées à des buts d’analyse avec Python.\n",
        "\n",
        "Structure de l'atelier :\n",
        "\n",
        "1. Partie 1 : Exemples de méthodes non supervisées et supervisées appliquées à l'analyse de textes.\n",
        "2. Partie 2 : Exercices sur les méthodes non supervisées et supervisées\n",
        "\n",
        "### Auteurs: \n",
        "- Bruno Agard <bruno.agard@polymtl.ca>\n",
        "- Davide Pulizzotto <davide.pulizzotto@polymtl.ca>\n",
        "\n",
        "Département de Mathématiques et de génie industriel\n",
        "\n",
        "École Polytechnique de Montréal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpAX32-4Kj8U"
      },
      "source": [
        "# <font color = 'E3A440'>0. Préparation de l'environnement </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jddhTL7EFrN_"
      },
      "outputs": [],
      "source": [
        "# Downloading of data from the GitHub project\n",
        "#!rm -rf Data_techniques_demystified_webinars/\n",
        "!rm -rf Donnees_demystifiees_seance_6/\n",
        "#!git clone https://github.com/4point0-ChairInnovation-Polymtl/Data_techniques_demystified_webinars\n",
        "!git clone https://github.com/puli83/Donnees_demystifiees_seance_6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJLRFWa6FrOA"
      },
      "outputs": [],
      "source": [
        "# Import modules\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.preprocessing import Normalizer\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install wordcloud\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ges85oatFrOJ"
      },
      "source": [
        "# <font color = 'E3A440'>1. Prétraitement du corpus (rappels)</font>\n",
        "\n",
        "Le prétraitement d'un corpus de textes peut nécessiter la mise en place de plusieurs étapes dont : le découpage des phrases, des mots, le nettoyage, le filtrage, etc.\n",
        "\n",
        "Dans les prochains blocs de code, un texte sera segmenté en phrases et prétraité au moyen de la fonction `CleaningText()` préparée lors de la séance précédente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOzWtcugFrOK"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"At eight o'clock, on Thursday morning, the great Arthur didn't feel VERY good.\n",
        "This morning, Arthur is feeling better.\n",
        "A dog runs in the street.\n",
        "A little boy in running in the street.\n",
        "Arthur is my dog, he sleeps every morning.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLrvRTIGdypS"
      },
      "outputs": [],
      "source": [
        "# extraction des phrases\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "print(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rtngk2rFrOL"
      },
      "outputs": [],
      "source": [
        "# Cleaning fonction to preprocess text\n",
        "def CleaningText(text_as_string, language = 'english', reduce = '', list_pos_to_keep = [], Stopwords_to_add = []):\n",
        "    from nltk.corpus import stopwords\n",
        "\n",
        "    words = nltk.word_tokenize(text_as_string)\n",
        "    words_pos = nltk.pos_tag(words, tagset='universal')\n",
        "    words_pos = [(w, pos) for w, pos in words_pos if w.isalnum()]\n",
        "    words_pos = [(w.lower(), pos) for w, pos in words_pos]\n",
        "    \n",
        "    if reduce == 'stem': \n",
        "        from nltk.stem.porter import PorterStemmer\n",
        "        reduced_words_pos = [(PorterStemmer().stem(w), pos) for w, pos in words_pos]\n",
        "        \n",
        "    elif reduce == 'lemma':\n",
        "        from nltk.stem.wordnet import WordNetLemmatizer\n",
        "        reduced_words_pos = [(WordNetLemmatizer().lemmatize(w), pos) for w, pos in words_pos]\n",
        "    else:\n",
        "        import warnings\n",
        "        reduced_words_pos = words_pos\n",
        "        warnings.warn(\"Warning : any reduction was made on words! Please, use \\\"reduce\\\" argument to chosse between 'stem' or  'lemma'\")\n",
        "    if list_pos_to_keep:\n",
        "        reduced_words_pos = [(w, pos) for w, pos in reduced_words_pos if pos in list_pos_to_keep]\n",
        "    else:\n",
        "        import warnings\n",
        "        warnings.warn(\"Warning : any POS filtering was made. Please, use \\\"list_pos_to_keep\\\" to create a list of POS tag to keep.\")\n",
        "    \n",
        "    list_stopwords = stopwords.words(language) + Stopwords_to_add\n",
        "    reduced_words_pos = [(w, pos) for w, pos in reduced_words_pos if w not in list_stopwords and len(w) > 1 ]\n",
        "    return reduced_words_pos   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkpxyQfl2Wna"
      },
      "outputs": [],
      "source": [
        "# nettoyage des phrases, sélection de pos-tag\n",
        "cleaned_sentences = [CleaningText(sent, reduce = 'stem', list_pos_to_keep = ['NOUN','ADJ','VERB']) for sent in sentences]\n",
        "print(cleaned_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vs4Y_iuW1YyL"
      },
      "source": [
        "Voici la liste de POS tag existant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0CQ8_vP1W-W"
      },
      "source": [
        "| **POS** | **DESCRIPTION**           | **EXAMPLES**                                      |\n",
        "| ------- | ------------------------- | ------------------------------------------------- |\n",
        "| ADJ     | adjective                 | big, old, green, incomprehensible, first      |\n",
        "| ADP     | adposition                | in, to, during                                |\n",
        "| ADV     | adverb                    | very, tomorrow, down, where, there            |\n",
        "| AUX     | auxiliary                 | is, has (done), will (do), should (do)        |\n",
        "| CONJ    | conjunction               | and, or, but                                  |\n",
        "| CCONJ   | coordinating conjunction  | and, or, but                                  |\n",
        "| DET     | determiner                | a, an, the                                    |\n",
        "| INTJ    | interjection              | psst, ouch, bravo, hello                      |\n",
        "| NOUN    | noun                      | girl, cat, tree, air, beauty                  |\n",
        "| NUM     | numeral                   | 1, 2017, one, seventy-seven, IV, MMXIV        |\n",
        "| PART    | particle                  | ’s, not                                      |\n",
        "| PRON    | pronoun                   | I, you, he, she, myself, themselves, somebody |\n",
        "| PROPN   | proper noun               | Mary, John, London, NATO, HBO                 |\n",
        "| PUNCT   | punctuation               | ., (, ), ?                                    |\n",
        "| SCONJ   | subordinating conjunction | if, while, that                               |\n",
        "| SYM     | symbol                    | $, %, §, ©, +, −, ×, ÷, =, :)               |\n",
        "| VERB    | verb                      | run, runs, running, eat, ate, eating          |\n",
        "| X       | other                     | sfpksdpsxmsa                                  |\n",
        "| SPACE   | space                     |                                                   |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0YdoOA1stB2"
      },
      "source": [
        "# <font color = 'E3A440'>2. Vectorisation (rappels)</font>\n",
        "\n",
        "Le contenu des textes peut être représenté sous forme matricielle.\n",
        "\n",
        "$$X = \\begin{bmatrix} \n",
        "x_{1,1} & x_{1,2} & \\ldots & x_{1,w} \\\\\n",
        "\\vdots & \\vdots       &  \\ddots      & \\vdots \\\\ \n",
        "x_{n,1} & x_{1,2} & \\ldots & x_{n,w} \\\\\n",
        "\\end{bmatrix}\n",
        "$$ \n",
        "\n",
        "Avec :\n",
        "\n",
        "- $x_{i,j}$ peut représenter la présence du mot \"j\" dans le texte $i$,\n",
        "- $x_{i,j}$ peut mesurer le nombre d'occurences du mot $j$ dans le texte $i$,\n",
        "- $x_{i,j}$ peut représenter l'**importance** du mot $j$ dans le texte $i$, dans ce cas on utilisera par exemple la métrique tf-idf :\n",
        " $$\\text{tf-idf}_{i,j}=\\text{tf}_{i,j}.log\\left(\\frac{n}{n_i}\\right)$$\n",
        " - $\\text{tf}_{i,j}$ est la fréquence du terme $i$ dans le document $j$,\n",
        " - $n$ nombre total de documents dans l’ensemble de textes à étudier,\n",
        " - $n_i$ nombre de documents dans l’ensemble de textes qui contiennent le terme $i$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJJQ7Bm5VTGY"
      },
      "source": [
        "Attention aux arguments de la fonction `CountVectorizer()`:\n",
        "\n",
        "  1. `min_df` : la frequence documentaire minimale qu'un mot doit respecter pour être retenu dans la matrice\n",
        "  2. `max_df` : la frequence documentaire maximale qu'un mot doit respecter pour être retenu dans la matrice\n",
        "  3. `ngram_range` : permet d'ajouter les n-grammes (bigrammes, trigrammes, etc.) à la vectorisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpFPJrGI2Nsi"
      },
      "outputs": [],
      "source": [
        "# Initialisation de l'objet\n",
        "def identity_tokenizer(text):\n",
        "    return text\n",
        "\n",
        "# Transforming the word in frequencies\n",
        "vectorized = CountVectorizer(lowercase = False, # Convert all characters to lowercase before tokenizing\n",
        "                             min_df = 1, # Ignore terms that have a document frequency strictly lower than the given threshold \n",
        "                             max_df = 10, # Ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words)\n",
        "                             stop_words = stopwords.words('english'), # Remove the list of words provided\n",
        "                             ngram_range = (1, 1), # Get the lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted\n",
        "                             tokenizer=identity_tokenizer) # Override the string tokenization step while preserving the preprocessing and n-grams generation steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9uC3usB5dtB"
      },
      "source": [
        "Utilisation du \"vectorizer\" avec une liste de listes de mots (et non une liste de tuples de mots-pos) et création de la matrice avec pondération de fréquence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8ydVdmC5c76"
      },
      "outputs": [],
      "source": [
        "# Application du vectorizer\n",
        "freq_term_DTM = vectorized.fit_transform([[w for w, pos in sent] for sent in cleaned_sentences])\n",
        "\n",
        "freq_Matrix=pd.DataFrame(freq_term_DTM.toarray(), columns =  [k for k, v in sorted(vectorized.vocabulary_.items(), key=lambda item: item[1])] )\n",
        "print(freq_Matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzlUbcCJHn7k"
      },
      "source": [
        "Crération de la matrice avec pondéraiton tf-idf."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GsktR7_Znke"
      },
      "outputs": [],
      "source": [
        "# Calculate the tfidf matrix\n",
        "tfidf = TfidfTransformer(norm='l1')\n",
        "tfidf_DTM = tfidf.fit_transform(freq_term_DTM)\n",
        "\n",
        "tfidf_Matrix=pd.DataFrame(tfidf_DTM.toarray(), columns =  [k for k, v in sorted(vectorized.vocabulary_.items(), key=lambda item: item[1])] )\n",
        "print(tfidf_Matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9ZpDrk9TzOo"
      },
      "source": [
        "# <font color = 'E3A440'>3. Méthodes non supervisées</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAEHXJXZrCGi"
      },
      "source": [
        "\n",
        "## <font color = 'E3A440'>3.0. Segmentation </font>\n",
        "\n",
        "\n",
        "Le but de la segmentation est de diviser un ensemble de données en sous ensembles plus petits qui partagent certaines caractéristiques.\n",
        "\n",
        "On veut que les éléments au sein d'un même sous groupe soient le plus similaires possibles.\n",
        "\n",
        "Une grande majorité des méthodes de segmentation est alors basée sur des métriques de distance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aALJbs9YrHHx"
      },
      "source": [
        "\n",
        "### <font color = 'E3A440'>3.1. Métriques de distance </font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SAuddwtrLBS"
      },
      "source": [
        "\n",
        "\n",
        "#### <font color = 'E3A440'>3.1.1. Distances entre points </font>\n",
        "\n",
        "Il existe de nombreuses métriques pour évaluer la similarité/distance entre deux points :\n",
        "\n",
        "\n",
        "### Distance Euclidienne\n",
        "\n",
        "$$d(x_1,x_2) = \\sqrt{\\sum_i^n\\left(x_{1,i},x_{2,i}\\right)^2}$$\n",
        "\n",
        "\n",
        "### Distances de Hamming\n",
        "\n",
        "En nombre de coordonnées différentes\n",
        "\n",
        "$$d_1(x_1,x_2) = \\sum_i^n\\delta_i$$\n",
        "\n",
        "\n",
        "\\begin{split}\n",
        "    Tel\\ que\\ :&\\\\ \n",
        "    &\\delta_i=\\begin{cases}  \n",
        "    0,\\ if\\ x_{1,i} = x_{2,i}\\\\\n",
        "    Sinon\\ 1\\\\\n",
        "    \\end{cases}\n",
        "\\end{split}\n",
        "\n",
        "\n",
        "En nombre de coordonnées différentes \"positives\"\n",
        "\n",
        "$$d_2(x_1,x_2) = \\sum_i^n\\delta_i$$\n",
        "\n",
        "\n",
        "\\begin{split}\n",
        "    Tel\\ que\\ :&\\\\ \n",
        "    &\\delta_i=\\begin{cases}  \n",
        "    0,\\ if\\ x_{1,i} = x_{2,i} = 1\\\\\n",
        "    Sinon\\ 1\\\\\n",
        "    \\end{cases}\n",
        "\\end{split}\n",
        "\n",
        "En pourcentage de coordonnées différentes / \"positives\"\n",
        "\n",
        "$$d_3(x_1,x_2) = \\frac{d_1(x_1,x_2)\\ ou\\ d_2(x_1,x_2)}{n}$$\n",
        "\n",
        "Avec différentes pondérations possibles\n",
        "\n",
        "$$d_4(x_1,x_2) = \\sum_i^n\\delta_i$$\n",
        "\n",
        "\n",
        "\\begin{split}\n",
        "    Tel\\ que\\ :&\\\\ \n",
        "    &\\delta_i=\\begin{cases}  \n",
        "    p,\\ if\\ x_{1,i} = x_{2,i} = 1\\\\\n",
        "    1,\\ if\\ x_{1,i} = x_{2,i} = 0\\\\ \n",
        "    Sinon\\ 0\\\\\n",
        "    \\end{cases}\n",
        "\\end{split}\n",
        "\n",
        "\n",
        "Il existe d'autres versions 'adaptées\" de Hamming...\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Plein d'autres ...\n",
        "\n",
        "Mais aucune ne fait vraiment ce que vous aurez besoin, car votre besoin est unique.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC8c_A6XrVqa"
      },
      "source": [
        "\n",
        "#### <font color = 'E3A440'>3.1.2. Hypermétriques, distances entre groupes </font>\n",
        "\n",
        "\n",
        " - Plus proche voisin (single linkage)\n",
        " - Voisin le plus éloigné (complete linkage)\n",
        " - Distance moyenne (average linkage)\n",
        " - Distance au centre de gravité\n",
        " - ...\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXR4EIkvrdhk"
      },
      "source": [
        "### <font color = 'E3A440'>3.2. Normalisation des données </font>\n",
        "\n",
        "Pour les algorithmes basés sur la distance, il est important de normaliser les données :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pirDtYQqdypY"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial import distance\n",
        "\n",
        "X = np.array([[100000, 0, 0, 0, 0, 0, 0],\n",
        "              [100000, 1, 1, 1, 1, 1, 1],\n",
        "              [1, 1, 1, 1, 1, 1, 1],\n",
        "             ])\n",
        "\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Luj-VLTwdypZ"
      },
      "outputs": [],
      "source": [
        "print( distance.euclidean(X[0], X[1]) )\n",
        "print( distance.euclidean(X[1], X[2]) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GL1Djp_WdypZ"
      },
      "source": [
        "#### Normalisation décimale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hZmISnqdypZ"
      },
      "outputs": [],
      "source": [
        "XD=X.copy()\n",
        "XD[:, 0] = XD[:, 0]/100000\n",
        "\n",
        "print(XD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83224ioddypZ"
      },
      "outputs": [],
      "source": [
        "print( distance.euclidean(XD[0], XD[1]) )\n",
        "print( distance.euclidean(XD[1], XD[2]) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "977e0F1_dypa"
      },
      "source": [
        "#### Normalisation Min/max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLSLUvWgdypa"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "XmM=X.copy()\n",
        "scaler = MinMaxScaler().fit(XmM)\n",
        "XmM=scaler.transform(XmM)\n",
        "#X=scaler.inverse_transform(X)\n",
        "\n",
        "print(XmM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKLkwqYIdypb"
      },
      "outputs": [],
      "source": [
        "print( distance.euclidean(XmM[0], XmM[1]) )\n",
        "print( distance.euclidean(XmM[1], XmM[2]) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iy5Y2qp1dypb"
      },
      "source": [
        "#### Normalisation sur la variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JTMscHQdypb"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "XV=X.copy()\n",
        "scaler = StandardScaler().fit(XV)\n",
        "XV=scaler.transform(XV)\n",
        "#X=scaler.inverse_transform(X)\n",
        "\n",
        "print(XV)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtYUwMkfdypb"
      },
      "outputs": [],
      "source": [
        "print( distance.euclidean(XV[0], XV[1]) )\n",
        "print( distance.euclidean(XV[1], XV[2]) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvmxXbl8XXST"
      },
      "source": [
        "### <font color = 'E3A440'>3.3. Algorithmes de segmentation </font>\n",
        "\n",
        "Il existe de nombreux algorithmes pour réaliser la segmentation d'un ensemble de données. On choisi un algorithme particulier en fonction du type de données et du type de résultat que l'on cherche."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWpU4HMmwNTX"
      },
      "outputs": [],
      "source": [
        "# Draw fonction to plot results\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from numpy import unique\n",
        "from numpy import where\n",
        "\n",
        "def draw(X,yhat):\n",
        "    # retrieve unique clusters\n",
        "    clusters = unique(yhat)\n",
        "\n",
        "    #plt.figure(figsize=(10, 10))\n",
        "    # create scatter plot for samples from each cluster\n",
        "    for cluster in clusters:\n",
        "        # get row indexes for samples with this cluster\n",
        "        row_ix = where(yhat == cluster)\n",
        "        # create scatter of these samples\n",
        "        plt.scatter(X[row_ix, 0], X[row_ix, 1])\n",
        "\n",
        "    plt.title(\"Clusters\")\n",
        "    # show the plot\n",
        "    plt.show() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKm8hyy4dypc"
      },
      "outputs": [],
      "source": [
        "#dataset generator\n",
        "k=4\n",
        "n_samples=1000\n",
        "features=3\n",
        "\n",
        "# define dataset\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X, _ = make_blobs(n_samples, centers=k, n_features=features, cluster_std=0.6, random_state=0)\n",
        "\n",
        "scaler = StandardScaler().fit(X)\n",
        "X=scaler.transform(X)\n",
        "#X=scaler.inverse_transform(X)\n",
        "\n",
        "yhat=np.zeros(n_samples)\n",
        "    \n",
        "draw(X,yhat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SU00eYOjXXbW"
      },
      "source": [
        "#### <font color = 'E3A440'>3.3.1. k-means</font>\n",
        "L'application d'un algorithme de <font color='E3A440'>**segmentation**</font> à une base de données permet de segmenter les observations dans des groupes de données homogènes.\n",
        "\n",
        "Pour ce faire, l'objectif de l'algorithme k-means est de minimiser l'inertie intra-classe autour d'un représentant de chaque groupe. Pour k-means, le représentant de chaque groupe est la moyenne de ce groupe. Les représentants sont remis à jour à chaque itération lors de l'apprentissage.\n",
        "\n",
        "La méthode k-means génère un vecteur  $Y$ de taille $n$, qui contient les <font color='E3A440'>**étiquettes des groupes**</font> assignées à chaque observation. Les étiquettes peuvent aller de $0$ à $k$. $k$ est le paramètre qui permet à l'usager de déterminer le nombre de groupes à générer. \n",
        "\n",
        "$$\n",
        "Y = \\begin{bmatrix} \n",
        "c_1 \\\\\n",
        "c_2 \\\\\n",
        "\\vdots \\\\ \n",
        "c_n\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "$Y_i$ correspond au groupe attribué à $X_i$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-NUIXIqdypd"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "model = KMeans(n_clusters=4).fit(X)\n",
        "\n",
        "# assign a cluster to each example\n",
        "yhatkm = model.predict(X)\n",
        "\n",
        "draw(X,yhatkm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdXBgvQldypd"
      },
      "source": [
        "##### Que ce passe-t-il à chaque itération ?\n",
        "\n",
        "L'algorithme exécute les étapes suivantes: \n",
        " 1. **Initialisation des représentants** : k points (*k* est choisi par l'usager) sont sélectionnés aléatoirement parmis l'ensemble de données. Chacun de ces points sera utilisé comme représentant d'un groupe pour la première itération. Chaque représentant portera l'étiquette du groupe qu'il représente.\n",
        " \n",
        " 2. **Début des itérations**:\n",
        "\n",
        "   2.1. **Classification**: Assigner à chaque point de l'ensemble de données l'étiquette du représentant qui lui est le plus proche.\n",
        "\n",
        "   2.2. **Mise à jour des représentants**: Pour chaque groupe généré, calculer le \"centre\" des données associées au groupe. Ce centre sera le nouveau représentant pour le groupe.\n",
        "\n",
        "   2.3. **Répétition**: Répéter les opérations 2.1. et 2.2. jusqu'à un critère d'arrêt (stabilisation des représentants, nombre d'itérations, temps de calcul...).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jH2XFE2dype"
      },
      "outputs": [],
      "source": [
        "k=4\n",
        "\n",
        "for i in range (1,4) :\n",
        "    print(\"Iteration :\", i)\n",
        "    model = KMeans(n_clusters=k, max_iter=i, n_init=1, init=\"random\", random_state=1).fit(X)\n",
        "    # assign a cluster to each example\n",
        "    yhatkm = model.predict(X)\n",
        "    draw(X,yhatkm)\n",
        "\n",
        "print(\"...\\n...\\n...\\n\")\n",
        "\n",
        "i=10\n",
        "print(\"Iteration :\", i)\n",
        "model = KMeans(n_clusters=k, max_iter=i, n_init=1, init=\"random\", random_state=1).fit(X)\n",
        "# assign a cluster to each example\n",
        "yhatkm = model.predict(X)\n",
        "draw(X,yhatkm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc44aMJ0dype"
      },
      "source": [
        "##### Sensibilité à l'initialisation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASr1FkoMdype"
      },
      "outputs": [],
      "source": [
        "for rs in range (1,6) :\n",
        "    print(\"Random state :\", rs)\n",
        "    model = KMeans(n_clusters=k, n_init=1, init=\"random\", random_state=rs).fit(X)\n",
        "    # assign a cluster to each example\n",
        "    yhatkm = model.predict(X)\n",
        "    draw(X,yhatkm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twqpWLJGdype"
      },
      "source": [
        "Conclusions :\n",
        "\n",
        " - K-means est sensible aux choix initial des représentants, cet algorithme donne un optimum LOCAL.\n",
        " \n",
        " - L'initialisation a un impact sur les résultats du k-means, donc sur la qualité du partitionnement.\n",
        "\n",
        " - Pour compenser, on peut :\n",
        " \n",
        "     - choisir judiscieusement les points d'initialisation en fonction de notre connaissance du problème,\n",
        "\n",
        "     - appliquer k-means avec différents points de départ et garder le meilleur résultat,\n",
        "         \n",
        "     - appliquer des méthodes spécifiques d'initalisation, parmis lesquelles il y a *k-means++*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxPJOsCadype"
      },
      "source": [
        "##### Sensibilité au nombre de groupes\n",
        "\n",
        "Le paramètre *k* détermine le nombre de groupes souhaités. La méthode est très sensible à ce paramètre, et il doit être choisi judiscieusement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGpSOlwydypf"
      },
      "outputs": [],
      "source": [
        "for m in range (1,6) :\n",
        "    print(\"k = \", m)\n",
        "    model = KMeans(n_clusters=m, max_iter=20, n_init=1, init=\"random\", random_state=1).fit(X)\n",
        "    # assign a cluster to each example\n",
        "    yhatkm = model.predict(X)\n",
        "    draw(X,yhatkm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGgjhYdEdypf"
      },
      "source": [
        "Conclusion :\n",
        "\n",
        "  - un mauvais nombre de groupes peut donner des groupes qui ne font pas de sens dans le but de l'analyse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgBJhckyqa5w"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "max=30\n",
        "Sil=np.zeros(max)\n",
        "x_axis=np.zeros(max)\n",
        "\n",
        "for m in range (2,max) :\n",
        "\n",
        "    model = KMeans(n_clusters=m, max_iter=100, n_init=1, init=\"random\", random_state=3).fit(X)\n",
        "\n",
        "    # assign a cluster to each example\n",
        "    labels = model.predict(X)\n",
        "\n",
        "    Sil[m]=silhouette_score(X, labels, metric=\"euclidean\")\n",
        "    x_axis[m]=m\n",
        "\n",
        "fig = plt.figure(figsize=(6, 6))\n",
        "a1 = fig.add_axes([0,0,1,1])\n",
        "l1 = a1.plot(x_axis,Sil,'rs-') # solid line with yellow colour and square marker\n",
        "a1.set_xlabel('k')\n",
        "a1.set_ylabel('Silhouette')\n",
        "a1.set_title('Evolution of indexes')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRfH1HBTdypf"
      },
      "source": [
        "##### À partir de notre ensemble de textes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P57i7O3dwNTc"
      },
      "outputs": [],
      "source": [
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXlv6J8xZEj-"
      },
      "outputs": [],
      "source": [
        "tfidf_Matrix=tfidf_DTM.toarray()\n",
        "\n",
        "model = KMeans(n_clusters=2).fit(tfidf_Matrix)\n",
        "\n",
        "# assign a cluster to each example\n",
        "yhatkm_tfidf = model.predict(tfidf_Matrix)\n",
        "\n",
        "print(yhatkm_tfidf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8qelreldypf"
      },
      "source": [
        "#### <font color = 'E3A440'>3.3.2. Méthode hiéarchique CHA</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvcnyQxvdypg"
      },
      "outputs": [],
      "source": [
        "import scipy.cluster.hierarchy as shc\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.title(\"Customer Dendograms\")\n",
        "dend = shc.dendrogram(shc.linkage(X, metric='euclidean', method='complete'))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAeuuEApdypg"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "model=AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='complete').fit(X)\n",
        "#{“ward”, “complete”, “average”, “single”},\n",
        "\n",
        "# assign a cluster to each example\n",
        "yhatac=model.fit_predict(X)\n",
        "\n",
        "draw(X,yhatac)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhIlVYF3dypj"
      },
      "source": [
        "##### À partir de notre ensemble de textes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Gpc2eGkqa5y"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 7))\n",
        "plt.title(\"Customer Dendograms\")\n",
        "dend = shc.dendrogram(shc.linkage(tfidf_Matrix, metric='euclidean', method='complete'))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxoIO7ELqa5y"
      },
      "source": [
        "#### <font color = 'E3A440'>3.3.3. DBSCAN</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmjudHHhqa5z"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "model = DBSCAN(eps=0.2, min_samples=4, metric=\"euclidean\").fit(X)\n",
        "\n",
        "# assign a cluster to each example\n",
        "yhatdbs=model.fit_predict(X)\n",
        "\n",
        "draw(X,yhatdbs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MCWDICTdypk"
      },
      "source": [
        "#### <font color = 'E3A440'>3.3.4. Analyse du contenu des groupes</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0PKzHp3dypk"
      },
      "outputs": [],
      "source": [
        "model = KMeans(n_clusters=4, max_iter=20, n_init=1, init=\"random\", random_state=1).fit(X)\n",
        "\n",
        "# assign a cluster to each example\n",
        "labels = model.predict(X)\n",
        "\n",
        "draw(X,labels)\n",
        "\n",
        "print(\"Silhouette=\", silhouette_score(X, labels, metric=\"euclidean\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjigF2JMdypl"
      },
      "outputs": [],
      "source": [
        "clusters = unique(yhatkm)\n",
        "\n",
        "for cluster in clusters:\n",
        "    C = X[yhatkm[:] == cluster]\n",
        "    labels=yhatkm[yhatkm[:] == cluster]\n",
        "    print('Cluster %1.0f : size = %.0f' % (cluster, len(C)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHJQ7Iglqa51"
      },
      "outputs": [],
      "source": [
        "for cluster in clusters:\n",
        "    C = X[yhatkm[:] == cluster]\n",
        "    fig1, ax1 = plt.subplots()\n",
        "    ax1.set_title('Cluster %1.0f' % (cluster))\n",
        "    ax1.boxplot(C);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4wTUIPWZEX-"
      },
      "source": [
        "# <font color = 'E3A440'>4. Méthodes supervisées</font>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4HD1S5gbtXz"
      },
      "source": [
        "## <font color = 'E3A440'>4.1. Perceptron multicouches</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IqRsMPFcgn4"
      },
      "source": [
        "Avant de rentrer dans les détails, il est nécessaire de préparer le corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNzsFIL5qa51"
      },
      "outputs": [],
      "source": [
        "text2=\"\"\"At eight o'clock, on Thursday morning, the great Arthur didn't feel VERY good.\n",
        "This morning, at nine o'clock, Arthur is feeling better.\n",
        "A dog runs in the street.\n",
        "In my city, cats run in the street.\n",
        "A little boy is running in the street.\n",
        "Arthur is my dog, we love to walk in the street together.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5NFJMMpqa51"
      },
      "outputs": [],
      "source": [
        "# Clean\n",
        "sentences2 = nltk.sent_tokenize(text2)\n",
        "cleaned_sentences2 = [CleaningText(sent, reduce = 'stem', list_pos_to_keep = ['NOUN','ADJ','VERB']) for sent in sentences2]\n",
        "\n",
        "# Calculate the tfidf matrix\n",
        "freq_term_DTM2 = vectorized.fit_transform([[w for w, pos in sent] for sent in cleaned_sentences2])\n",
        "tfidf = TfidfTransformer(norm='l1')\n",
        "tfidf_DTM2 = tfidf.fit_transform(freq_term_DTM2)\n",
        "\n",
        "# convert in Dataframe for pedagogical reason\n",
        "tfidf_Matrix2 = pd.DataFrame(tfidf_DTM2.toarray(), columns =  [k for k, v in sorted(vectorized.vocabulary_.items(), key=lambda item: item[1])] )\n",
        "tfidf_Array2 = tfidf_DTM2.toarray()\n",
        "print(tfidf_Matrix2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UVTTVcVcBCA"
      },
      "source": [
        "### <font color = 'E3A440'>4.1.1. Préparation du corpus d'entraînement et de test</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour éxecuter une méthode supervisée, il est nécessaire d'obtenir un corpus d'entraînement et un corpu de test."
      ],
      "metadata": {
        "id": "PWqMFjOhPVe9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWY4EBh7qa51"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "#split the data\n",
        "X_train, X_test = tfidf_Array2[:5,:], tfidf_Array2[5:,:]\n",
        "y_train, y_test = [1,1,2,2,2],[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7mlhG2wcRQw"
      },
      "source": [
        "### <font color = 'E3A440'>4.1.2. Entraînement</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans le suivant bloc de code, l'algorithme est d'abord initialisé. \n",
        "\n",
        "Ensuite, la méthode `.ft()` est utilisée pour exécuter l'entraînemenet. `.fit()` nécessite de `X_train` (corpus d'entraînement) et de `y_train` (la liste des étiquettes qui accompagne le copus d'entraînement)."
      ],
      "metadata": {
        "id": "4TviABKePxs0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uhb02H1jqa52"
      },
      "outputs": [],
      "source": [
        "clf = MLPClassifier(random_state=1, max_iter=300, hidden_layer_sizes=5, learning_rate=\"constant\", learning_rate_init=0.01)\n",
        "clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkywIT8rct9w"
      },
      "source": [
        "### <font color = 'E3A440'>4.1.3. Évaluation du modèle</font>\n",
        "\n",
        "Les variables `y_valid`, qui permet d'évaluer le taux d'erreur dans l'aprentissage, et `y_pred`, qui permet d'évaluer les erreurs dans la phase de prédiction, sont générées dans le bloc de code suivant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5IJVA3Fyr_C"
      },
      "outputs": [],
      "source": [
        "y_valid=clf.predict(X_train)\n",
        "y_pred=clf.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkquI6lbe8l7"
      },
      "source": [
        "La fonction `accuracy_score()` permet de calculer la metrique d'évaluation, qui est l'**accuracy**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZKTB0Lwqa52"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"Learning errors\")\n",
        "print(\"    Accuracy = \", accuracy_score(y_train, y_valid)*100)\n",
        "\n",
        "print(\"\\nPrediction errors\")\n",
        "print(\"    Accuracy = \", accuracy_score(y_test, y_pred)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZjRjr3zqa53"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "print(\"\\n** Learning performances: **\") \n",
        "print(\"Confusion Matrix: \") \n",
        "print(confusion_matrix(y_train, y_valid))\n",
        "print (\"Accuracy : \", accuracy_score(y_train, y_valid)*100) \n",
        "print(\"** Prediction performances: **\") \n",
        "print(\"Confusion Matrix: \") \n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print (\"Accuracy : \", accuracy_score(y_test,y_pred)*100) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQvxM4dEdypm"
      },
      "source": [
        "# <font color = 'E3A440'>5. Exercice : Analyse de la base donnée 20 Newsgroups</font>\n",
        "\n",
        "La base de données `20 Newsgroups` est une collection d'environ 20,000 documents segmentés en plus ou moins 20 thèmatiques différentes.\n",
        "\n",
        "Les données sont téléchargées à partir du module `sklearn.datasets` et sont reorganisées en format tabulaire. Pour des raisons pédagogiques, cet exercice prévoit l'utilisation d'un échantillon d'environ 3,000 documents segmentés en 3 groupes différents : \n",
        " 1. `rec.autos`, identifié avec la valeur `7` dans la colonne `target`\n",
        " 2. `rec.sport.hockey`, identifié avec la valeur `10` dans la colonne `target`\n",
        " 3. `sci.med`, identifié avec la valeur `13` dans la colonne `target`\n",
        "\n",
        "Pendant l'exercice, le participant sera invité à remplir les parties manquantes du code qui sont indiquées avec `...` (trois points)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YALuaU53SY6"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "archive20newsgroup = fetch_20newsgroups(subset = 'all')\n",
        "df = pd.DataFrame({'Text': archive20newsgroup.data, 'target': archive20newsgroup.target})\n",
        "target_names = archive20newsgroup.target_names\n",
        "print(target_names)\n",
        "\n",
        "# subset selection\n",
        "target_selected = [7,10,13]\n",
        "df = df[df.target.isin(target_selected)]\n",
        "target_names = [x for idx, x in enumerate(target_names) if idx in target_selected]\n",
        "print(target_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PX2raKXvdOJB"
      },
      "source": [
        "L'échantillon de la base de données est composée de 2,979 documents, segmentés en 3 thématiques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1c-fDi_Ck8_"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBaFWoDGdypn"
      },
      "source": [
        "Voici les noms de variables disponibles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1udmVkRdypn"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONaA5iThdypo"
      },
      "source": [
        "Voici une observation (une ligne du tableau de données):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVhmJgQQdypo"
      },
      "outputs": [],
      "source": [
        "df.iloc[284].Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iicfl21wdypm"
      },
      "source": [
        "## <font color = 'E3A440'>5.1 Présentation de l'exercice </font>\n",
        "\n",
        "L'exercise est composé de deux parties:\n",
        "\n",
        " 1. Remplir les parties de code qui manquent (indiquées avec les `...`)\n",
        " 2. Changer les paramètres du prétraitement pour expérimenter les impacts qu'ils peuvent avoir sur les méthodes non supervisées et supervisées. En particulier, vous êtes invités à expérimenter les opérations suivantes :\n",
        "\n",
        "    2.1. Choisir une sélection de POS tag différents (fonction `CleaningText()`)\n",
        "\n",
        "    2.2. Éliminer des mots de votre choix en les ajoutant dans la liste de stopword (fonction `CleaningText()`)\n",
        "\n",
        "    2.3. Changer la seuil de fréquence minimale pour retenir un mot dans la matrice (fonction `CountVectorizer()`)\n",
        "\n",
        "    2.4 Ajouter des bigrams et trigrams lors de la vectorisation (fonction `CountVectorizer()`)\n",
        "\n",
        "\n",
        "\n",
        "Si vous voulez aller plus loin, vous pourriez aussi changer les paramètres des algorithmes non supervisés et supervisés pour anlyser les impacts sur les résultats. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb_ydSo01iFn"
      },
      "source": [
        "Rappel: voici la liste de POS tag existant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVldkntL1iFo"
      },
      "source": [
        "| **POS** | **DESCRIPTION**           | **EXAMPLES**                                      |\n",
        "| ------- | ------------------------- | ------------------------------------------------- |\n",
        "| ADJ     | adjective                 | big, old, green, incomprehensible, first      |\n",
        "| ADP     | adposition                | in, to, during                                |\n",
        "| ADV     | adverb                    | very, tomorrow, down, where, there            |\n",
        "| AUX     | auxiliary                 | is, has (done), will (do), should (do)        |\n",
        "| CONJ    | conjunction               | and, or, but                                  |\n",
        "| CCONJ   | coordinating conjunction  | and, or, but                                  |\n",
        "| DET     | determiner                | a, an, the                                    |\n",
        "| INTJ    | interjection              | psst, ouch, bravo, hello                      |\n",
        "| NOUN    | noun                      | girl, cat, tree, air, beauty                  |\n",
        "| NUM     | numeral                   | 1, 2017, one, seventy-seven, IV, MMXIV        |\n",
        "| PART    | particle                  | ’s, not                                      |\n",
        "| PRON    | pronoun                   | I, you, he, she, myself, themselves, somebody |\n",
        "| PROPN   | proper noun               | Mary, John, London, NATO, HBO                 |\n",
        "| PUNCT   | punctuation               | ., (, ), ?                                    |\n",
        "| SCONJ   | subordinating conjunction | if, while, that                               |\n",
        "| SYM     | symbol                    | $, %, §, ©, +, −, ×, ÷, =, :)               |\n",
        "| VERB    | verb                      | run, runs, running, eat, ate, eating          |\n",
        "| X       | other                     | sfpksdpsxmsa                                  |\n",
        "| SPACE   | space                     |                                                   |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXJr5qVs3x3y"
      },
      "source": [
        "### <font color = 'E3A440'> a. Construction de certaines fonctions qui serviront plus tard.</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viU9xTMeOdZj"
      },
      "outputs": [],
      "source": [
        "def plot_data_by_cluster(DTM, cls_kmeans, figsize = (16,10) ):\n",
        "    ## Reduction of dimension to 2 for visualisation reasons\n",
        "    from sklearn.manifold import TSNE\n",
        "    import matplotlib.pyplot as plt\n",
        "    import time\n",
        "    time_start = time.time()\n",
        "    tsne = TSNE(n_components=2, verbose=1, perplexity=30, n_iter=1000,metric='cosine', learning_rate=10, random_state = 794)\n",
        "    reduc_dim_results = tsne.fit_transform(DTM)\n",
        "    print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
        "\n",
        "    ## Create data structure for plotting\n",
        "    df_reduction = pd.DataFrame()\n",
        "    df_reduction['y'] =  cls_kmeans.labels_\n",
        "    df_reduction['1-dim'] = reduc_dim_results[:,0]\n",
        "    df_reduction['2-dim'] = reduc_dim_results[:,1]\n",
        "\n",
        "    ## Generate the plot\n",
        "    import seaborn as sns\n",
        "    import colorcet as cc\n",
        "    plt.figure(figsize = figsize)\n",
        "    sns.scatterplot(data = df_reduction,\n",
        "                    x=\"1-dim\",\n",
        "                    y=\"2-dim\",\n",
        "                    hue=\"y\",\n",
        "                    palette = sns.color_palette(cc.glasbey, n_colors = cls_kmeans.n_clusters),)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def prepare_data_for_WC(DTM, vocabulary_dtm):\n",
        "    import scipy\n",
        "    # compute total frequency for each word\n",
        "    values_words = sum(DTM)\n",
        "    # values_words = sum(tfidf_matrix)\n",
        "    # verify type result and prepare data for wordcloud\n",
        "    if type(values_words) is np.ndarray:\n",
        "        values_words = [float(value) for value in np.nditer(values_words)]\n",
        "    elif type(values_words) is scipy.sparse.csr.csr_matrix:\n",
        "        values_words = [float(value) for value in np.nditer(values_words.todense())]\n",
        "    else:\n",
        "        print(\"Matrix in argument DTM has to be one of these two data classes:  'scipy.sparse.csr.csr_matrix' or 'numpy.ndarray'\")\n",
        "    ##Retrieve the word fromthe vocaboulary and sorting them based on the frequency\n",
        "    list_mots = sorted(vocabulary_dtm.items(), key= lambda x:x[1])\n",
        "    list_mots = [word for (word,idx) in  list_mots]\n",
        "    words = zip(list_mots, values_words)\n",
        "    words = sorted(words, key= lambda x:x[1], reverse=True)\n",
        "    ## prepare data structure for wordcloud\n",
        "    result_for_WC = {}\n",
        "    #iterating over the tuples lists\n",
        "    for (key, value) in words:\n",
        "        result_for_WC[key] = value\n",
        "    #\n",
        "    return result_for_WC\n",
        "\n",
        "\n",
        "def wordcloud_par_cluster(wordcloud, DTM, cls_kmeans, vocab, first_n_words=10, figsize=(18, 16), fontsize=32, plot_wordcloud = True, lst_clust = [], title_in_plot = \"Clust_\"):\n",
        "\n",
        "        \"\"\"\n",
        "        wordcloud; A WordCloud function.\n",
        "        DTM; A Docuemnt-Term Matrix\n",
        "        vocab; It is a vocabulary from skllarn vectorizer\n",
        "        first_n_words = 10; How many words to print\n",
        "        figsize = (18, 16); Size of the plot. (this is the argument of this line plt.figure(figsize=figsize))\n",
        "        fontsize = 32; Size of title font\n",
        "        lst_clust = []; The list of cluster to plot. If empty, all the clusters are plotted\n",
        "        title_in_plot = \"Clust_\"; title to put on top of plot \\n\n",
        "        \"\"\"\n",
        "        import numpy\n",
        "        import scipy\n",
        "        \n",
        "        if not lst_clust:\n",
        "            lst_clust = list(range(cls_kmeans.n_clusters))\n",
        "\n",
        "        for x in lst_clust:\n",
        "            DTM_temp = DTM[cls_kmeans.labels_ == x]\n",
        "            result_for_WC= prepare_data_for_WC(DTM_temp, vocab)\n",
        "            ###\n",
        "            if plot_wordcloud == True:\n",
        "                plot = wordcloud.generate_from_frequencies(result_for_WC)\n",
        "                plt.figure(figsize=figsize)\n",
        "                plt.imshow(plot)\n",
        "                plt.title(title_in_plot + str(x) + '  N. of documents=' + str(DTM_temp.shape[0]),\n",
        "                        fontsize = fontsize,\n",
        "                        bbox=dict(facecolor='red', alpha=0.5))\n",
        "                plt.axis('off')\n",
        "                plt.show()\n",
        "            print(f\"Most frequent words for cluster {x} of size {str(DTM_temp.shape[0])} docs: \", list(result_for_WC)[0:first_n_words])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiWwUuMidypo"
      },
      "source": [
        "### <font color = 'E3A440'> b. Annotation, nettoyage et vectorisation des documents (Rappels)</font>\n",
        "\n",
        "Nous utilisons la fonction écrite précédemment pour nettoyer les unités lexicales.\n",
        "\n",
        "Pour ce premier test, conservez seulement le noms (`NOUN`). Remplir la portion de code qui manque `...`.\n",
        "\n",
        "Cette opération prendra quelques secondes. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KnDIzjtudypo"
      },
      "outputs": [],
      "source": [
        "cleaned_20news = [CleaningText(sent, reduce = 'lemma', list_pos_to_keep = [...], Stopwords_to_add=['msg']) for sent in list(df['Text'])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcExCuEpdypp"
      },
      "source": [
        "Dans l'étape de vectorisation, nous retenons les mots qui apparaissent dans au moins 15 documents (min_df = 15). \n",
        "\n",
        "Remplir la portion de code manquant indiqué par `...`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deLwyrVodypp"
      },
      "outputs": [],
      "source": [
        "# Initialisation de l'objet\n",
        "def identity_tokenizer(text):\n",
        "    return text\n",
        "# Transforming the word in frequencies\n",
        "vectorized = CountVectorizer(lowercase = False, # Convert all characters to lowercase before tokenizing\n",
        "                             min_df = ..., # Ignore terms that have a document frequency strictly lower than the given threshold \n",
        "                             max_df = 1200, # Ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words)\n",
        "                             stop_words = stopwords.words('english'), # Remove the list of words provided\n",
        "                             ngram_range = (1, 3), # Get the lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted\n",
        "                             tokenizer=identity_tokenizer) # Override the string tokenization step while preserving the preprocessing and n-grams generation steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXmJqhK_EoBE"
      },
      "outputs": [],
      "source": [
        "freq_term_DTM_20news = vectorized.fit_transform([[w for w, pos in sent if len(w) > 2] for sent in cleaned_20news])\n",
        "freq_term_DTM_20news"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "El2s_eKbdypp"
      },
      "outputs": [],
      "source": [
        "# Calculate the tfidf matrix\n",
        "tfidf = TfidfTransformer(norm='l1')\n",
        "tfidf_DTM_20news = tfidf.fit_transform(freq_term_DTM_20news)\n",
        "#print(tfidf_DTM)\n",
        "\n",
        "tfidf_DF_20news = pd.DataFrame(tfidf_DTM_20news.toarray(), columns =  [k for k, v in sorted(vectorized.vocabulary_.items(), key=lambda item: item[1])] )\n",
        "print(tfidf_DF_20news)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B37DCDHqdypp"
      },
      "source": [
        "## <font color = 'E3A440'>5.2 Méthodes non supervisées : objectif de l'exercise </font>\n",
        "\n",
        "1. Séparer les messages en groupes homogènes en utilisant un clustering\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwYavKEXT14M"
      },
      "source": [
        "### <font color = 'E3A440'> a. Évaluer la meilleure partition</font>\n",
        " Dans le prochain bloc de code, vous devez choisir les valeurs des variables suivantes:\n",
        " 1.   `min_k`, qui determine le plus petit nombre de groupes à générer pour l'évaluation.\n",
        " 2.   `max_k`, qui determine le plus grand nombre de groupes à générer pour l'évaluation.\n",
        "\n",
        "Remplir les portions de code qui manquent `...`.\n",
        "\n",
        "Aux fins de cet exercice, n'exécutez pas plus de 30 segmentations différentes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43eokkyp2_-I"
      },
      "outputs": [],
      "source": [
        "# Code for kl evaluation\n",
        "min_k = ...\n",
        "max_k = ...\n",
        "\n",
        "Sil=np.zeros(max_k)\n",
        "x_axis=np.zeros(max_k)\n",
        "tfidf_DTM_20news_norm = Normalizer().transform(tfidf_DTM_20news)\n",
        "\n",
        "for m in range (min_k, max_k) :\n",
        "\n",
        "    model = KMeans(n_clusters=m, max_iter=100, n_init=1, init=\"k-means++\", random_state=3).fit(tfidf_DTM_20news_norm)\n",
        "\n",
        "    # assign a cluster to each example\n",
        "    labels = model.predict(tfidf_DTM_20news_norm)\n",
        "\n",
        "    Sil[m]=silhouette_score(tfidf_DTM_20news_norm, labels, metric=\"cosine\")\n",
        "    x_axis[m]=m\n",
        "\n",
        "fig = plt.figure(figsize=(7, 7))\n",
        "a1 = fig.add_axes([0,0,1,1])\n",
        "l1 = a1.plot(x_axis, Sil,'ys-') # solid line with yellow colour and square marker\n",
        "a1.set_xlabel('k')\n",
        "a1.set_ylabel('Silhouette')\n",
        "a1.set_title('Evolution of indexes')\n",
        "a1.yaxis.grid(True)\n",
        "a1.xaxis.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eKjz4S7UcEz"
      },
      "source": [
        "### <font color = 'E3A440'> b. Éxecuter clustering avec k fixé</font>\n",
        "Exécutez le clustering final avec un nombre spécifique de clusters, que vous avez choisi en analysant le graphique précédent.\n",
        "\n",
        "Pour coherence avec le nombre de groupes de la base de données, essayez d'abord avec `n_cluster = 3`.\n",
        "\n",
        "Remplir les trois points `...`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cyoqur3B2_-G"
      },
      "outputs": [],
      "source": [
        "tfidf_DTM_20news_norm = Normalizer().transform(tfidf_DTM_20news.toarray())\n",
        "model = KMeans(n_clusters = ..., max_iter=200, n_init=1, init=\"k-means++\", random_state=1).fit(tfidf_DTM_20news_norm)\n",
        "# assign a cluster to each example\n",
        "labels = model.predict(tfidf_DTM_20news_norm)\n",
        "\n",
        "print(\"Silhouette=\", silhouette_score(tfidf_DTM_20news_norm, labels, metric=\"cosine\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OEMhbkB5Hpi"
      },
      "outputs": [],
      "source": [
        "model.n_clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwD0TAy35fOK"
      },
      "outputs": [],
      "source": [
        "Counter(model.labels_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHLnCyp64whb"
      },
      "outputs": [],
      "source": [
        "tfidf_DTM_20news_norm = Normalizer().transform(tfidf_DTM_20news.toarray())\n",
        "plot_data_by_cluster(tfidf_DTM_20news_norm, model )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d7dHImnXOdh"
      },
      "source": [
        "### <font color = 'E3A440'> c. Regarder les résultats des clusters</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40mECfJP4TK7"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "wordcloud_par_cluster(wordcloud = WordCloud(), # WordCloud function. \n",
        "                      DTM = tfidf_DTM_20news_norm,# Document-Term Matrix \n",
        "                      cls_kmeans = model, # Insert the result of a kmeans clustering\n",
        "                      vocab = vectorized.vocabulary_, # a vocabulary from scikitlearn vectorizer\n",
        "                      first_n_words=10,#  It indicates how many words to print\n",
        "                      figsize=(12, 10),\n",
        "                      fontsize=32,\n",
        "                      plot_wordcloud = False, # Switch to True if you want to plot wordclouds\n",
        "                      lst_clust = [], # Insert a list of integer to get info about a selected number of cluster. It shows info of all the clustrs if empty list\n",
        "                      title_in_plot = \"Clust_\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPJl9Gf_46RN"
      },
      "source": [
        "### <font color = 'E3A440'> d. Évaluer la partition générée avec les targets de 20 newsgroup database</font>\n",
        "L'objectif du prochain bloc de code est de modifier les étiquettes de départ des données, qui sont dans la variable `target`, afin de les faire coïncider avec les étiquettes données par l'algorithme. \n",
        "\n",
        "Ainsi, si une segmentaiton en 3 clusters a été générée, il faudra:\n",
        "\n",
        " 1.   Interpéter les résultats et identifier quel cluster correspond à quelle quelle étiquette `target`.\n",
        " 2.   Modifier les étiquettes et les faire correspondre au `target`, \n",
        " \n",
        "<font color = 'E3A440'><bold>Par exemple, si la segmentation à trois clusters nous montre que le cluster `0` ressemble au sujet `rec.autos`, identifié avec la valeur `7` dans la colonne `target`, alors nous devons substituer tous les `7` par `0`. </bold></font>\n",
        "\n",
        "Voici les étiquettes des 3 groupes: \n",
        " 1. `rec.autos`, identifié avec la valeur `7` dans la colonne `target`\n",
        " 2. `rec.sport.hockey`, identifié avec la valeur `10` dans la colonne `target`\n",
        " 3. `sci.med`, identifié avec la valeur `13` dans la colonne `target`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1ZeB0_7bwzI"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "df['target_converted'] = df.target\n",
        "df['target_converted'] = df.target_converted.apply(lambda x: 0 if x == 7 else x)\n",
        "df['target_converted'] = df.target_converted.apply(lambda x: 1 if x == 10 else x)\n",
        "df['target_converted'] = df.target_converted.apply(lambda x: 2 if x == 13 else x)\n",
        "print(accuracy_score(df.target_converted, model.labels_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS7pyTqNPFY_"
      },
      "source": [
        "Êtes-vous en mesure d'ameliorer la métrique d'évaluation (`accuracy`) en modifiant les inputs du modèle? Par exemple, en ajoutant des bigrammes et trigrammes?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucIWlhmLc6cM"
      },
      "source": [
        "## <font color = 'E3A440'>5.3 Méthodes supervisées : objectif de l'exercise </font>\n",
        "\n",
        "1. Entraîner un perceptron multicouche à classifier des docuemnts par sujet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QolMWGXbzlb"
      },
      "source": [
        "### <font color = 'E3A440'> a. Classifier les messages dans les groupes de départ</font>\n",
        "Dans le prochain bloc de code, un modèle supervisé est généré pour reconnaitre les documents qui font partie d'une des étiquettes suivantes :\n",
        "\n",
        " 1. `rec.autos`, identifié avec la valeur `7` dans la colonne `target`\n",
        " 2. `rec.sport.hockey`, identifié avec la valeur `10` dans la colonne `target`\n",
        " 3. `sci.med`, identifié avec la valeur `13` dans la colonne `target`\n",
        "\n",
        "Pour se faire, un corpus d'entrainement (`X_train`) accompagné par les étiquettes qui correspondent à chaque document (`y_train`) et un corpus de test (`X_test`accompagné par `y_test`) sont construits. \n",
        "\n",
        "Le corpus d'entrainemenet (`X_train`) servira pour apprendre un perceptron multicouche à classifier les documents avec la bonne étiquette. Le corpus de test `X_test`, servira à évaluer la performace du modèle. \n",
        "\n",
        "La fonction `train_test_split()` est utilisée pour créer ces corpus, et l'argument `test_size` détermine la propotion du corpus qui constituira le corpus de text (`X_test`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnpRZ8OGR7H0"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "tfidf_DTM_20news_norm = Normalizer().transform(tfidf_DTM_20news)\n",
        "X_train, X_test, y_train, y_test = train_test_split(tfidf_DTM_20news_norm, df.target, test_size = 0.2,  random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPwq6Ttha3Ur"
      },
      "source": [
        "Voici les nombres de documents dans les corpus d'entrainement et de test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mBglHFBSiXt"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape)\n",
        "print(len(y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR2tdKEqSl8d"
      },
      "outputs": [],
      "source": [
        "print(X_test.shape)\n",
        "print(len(y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5NwcQqTa-Ti"
      },
      "source": [
        "### <font color = 'E3A440'> b. Entrainement du modèle</font>\n",
        "\n",
        "Avec la fonction `MLPClassifier()`, un réseaux de neurones est construit pour la reconnaissance de documents selon les étiquettes selectionnées.\n",
        "\n",
        "Ajoutez les bons variables dans la fonction `MLPClassifier()` pour entraîner le réseaux de neurones. \n",
        "\n",
        "Remplir les portions de code vides `...`.\n",
        "\n",
        "Insipirez vous par la section `4.1.2.`du présent script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQxFqKC6dypm"
      },
      "outputs": [],
      "source": [
        "cls = MLPClassifier(random_state=1, max_iter=300).fit(..., ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gyLBesUwNTw"
      },
      "source": [
        "### <font color = 'E3A440'> c. Validation des performances d'apprentissage</font>\n",
        "\n",
        "Dans les prochains blocs de code, le corpus de test (`X_test`) est utilisé pour évaluer le modèle. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhqu_NAWwNTx"
      },
      "source": [
        "\n",
        "Ajoutez les bonnes variables pour générer la `y_valid`, qui permet d'évaluer le taux d'erreur dans l'apprentissage et le `y_pred`, qui permet d'évaluer les erreurs dans la phase de prédiciton.\n",
        "\n",
        "Remplir les portions de code vides `...`.\n",
        "\n",
        "Inspirez vous par la section `4.1.3.` du présent script.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFUCYUYsUTEU"
      },
      "outputs": [],
      "source": [
        "y_valid = cls.predict(...)\n",
        "y_pred = cls.predict(...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMeR_qQrddpS"
      },
      "outputs": [],
      "source": [
        "print(\"Learning errors\")\n",
        "print(\"    Accurary = \", accuracy_score(y_train, y_valid)*100)\n",
        "\n",
        "print(\"\\nPrediction errors\")\n",
        "print(\"    Accurary = \", accuracy_score(y_test, y_pred)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdlhRa3ZwNTx"
      },
      "outputs": [],
      "source": [
        "print(confusion_matrix(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhZSp6JKjH8k"
      },
      "source": [
        "# <font color = 'E3A440'> 6. Conclusion </font>\n",
        "Dans cet atelier, vous avez appris à manipuler des données textuelles à des fins d'analyse sémantique. Vous avez pu expérimenter les impacts du prétraitement sur les résultats des algorithmes d'apprentissage.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}