{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWNgJtQ3FrN8"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "<font size='10' color = 'E3A440'>**Mégadonnées et techniques avancées démystifiées**</font>\n",
        "=======\n",
        "<font color = 'E3A440'>*Nouvelles méthodes d’analyse et leur implication quant à la gestion des mégadonnées en SSH (partie 1)*</font>\n",
        "=============\n",
        "\n",
        "\n",
        "Cet atelier s’inscrit dans le cadre de la formation [Mégadonnées et techniques avancées démystifiées](https://www.4point0.ca/2022/08/22/formation-megadonnees-demystifiees/) (séance 6).\n",
        "\n",
        "Les sciences humaines et sociales sont souvent confrontées à l’analyse de données non structurées, comme le texte. Après avoir préparé les données, plusieurs techniques d’analyse venant de l’apprentissage automatique peuvent être utilisées. Pendant cet atelier, les participants seront initiés aux méthodes supervisées et non supervisées à des buts d’analyse avec Python.\n",
        "\n",
        "Note : Cet atelier se poursuit lors d’une 2e séance le 10 novembre.\n",
        "\n",
        "Structure de l'atelier :\n",
        "1. Presentation of sections 1 and 2 in a plenary mode (20 minutes)\n",
        "2. Individual work on section 3 (20 minutes)\n",
        "3. Group work on section 4 (60 minutes)\n",
        "4. Plenary session with groups presentations (20 minutes)\n",
        "\n",
        "Ce tutoriel ne peut pas être consideré exaustif .... \n",
        "\n",
        "### Authors: \n",
        "- Bruno Agard <bruno.agard@polymtl.ca>\n",
        "- Davide Pulizzotto <davide.pulizzotto@polymtl.ca>\n",
        "\n",
        "### Table of Contents\n",
        "Bruno Agard\n",
        "\n",
        "Département de Mathématiques et de génie Industriel\n",
        "\n",
        "École Polytechnique de Montréal"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Préparation environnement"
      ],
      "metadata": {
        "id": "HpAX32-4Kj8U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jddhTL7EFrN_",
        "outputId": "e5478167-f677-4ddf-8687-de32769b939d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Donnees_demystifiees_seance_6'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ],
      "source": [
        "# Downloading of data from the GitHub project\n",
        "!rm -rf Donnees_demystifiees_seance_6/\n",
        "!git clone https://github.com/puli83/Donnees_demystifiees_seance_6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJLRFWa6FrOA",
        "outputId": "9743ce8a-35a2-4bc6-f645-3a70f260a5d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Import modules\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwFKNpPQFrN-"
      },
      "source": [
        "# <font color = 'E3A440'>*Préparation des données textulles*</font>\n",
        "\n",
        "L'analyse de données textuelles implique la transformation d'un texte en un objet mathematique qui peut être utilisé par des algorithmes et des modèles statistique. Cette étape est importante car permet de **structurer** des données non structurées, comme le texte.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDS10IXPFrOB"
      },
      "source": [
        "###  <font color = 'E3A440'>**Étapes fondamentales du prétraitement**</font>\n",
        "\n",
        "Prenons une phrase pour decortiquer les passages que nous pemrettent de la transformer en information structurée."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "h9AVaykcFrOB"
      },
      "outputs": [],
      "source": [
        "sentence = \"\"\"At eight o'clock, on Thursday morning, the great Arthur didn't feel VERY good.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpFqEU4iFrOC",
        "outputId": "47ef0ec3-cfb0-416e-bc9a-a2b319f54a01"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "78"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "len(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWQNjvKIFrOD"
      },
      "source": [
        "#### <font color = 'E3A440'>*1. Tokenisation*</font>\n",
        "\n",
        "Cette étape consiste à couper la phrase en unités linguistiques élémantaire et dotées de sens, ce qui est gnralement appelé le \"mot\".\n",
        "\n",
        "Dans le module `nltk`, il existe une fonction qui permet cette opération, soit `word_tokenize()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzF3fiUWFrOE",
        "outputId": "938dfc0d-91a1-4b21-af76-241e4cd7e52b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['At', 'eight', \"o'clock\", ',', 'on', 'Thursday', 'morning', ',', 'the', 'great', 'Arthur', 'did', \"n't\", 'feel', 'VERY', 'good', '.']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# La function word_tokenize() prend la phrase comme argument.\n",
        "words = nltk.word_tokenize(sentence)\n",
        "print(words)\n",
        "len(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color = 'E3A440'>*2. Analyse morphosyntaxique*</font>\n",
        "\n",
        "Après avoir identifé tous les mots, il est possible de analyser leur rôle morphosyntaxique, à des fins d'analyse et/ou filtrage. "
      ],
      "metadata": {
        "id": "CvM6JKj1hLX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # La function word_tokenize() prend la liste de mots comme argument.\n",
        "words_pos = nltk.pos_tag(words, tagset='universal')\n",
        "print(words_pos)\n",
        "len(words_pos)"
      ],
      "metadata": {
        "id": "qy06di1ygtmF",
        "outputId": "ceb9bd8b-18f0-43ec-8903-0b81c213b2dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('At', 'ADP'), ('eight', 'NUM'), (\"o'clock\", 'NOUN'), (',', '.'), ('on', 'ADP'), ('Thursday', 'NOUN'), ('morning', 'NOUN'), (',', '.'), ('the', 'DET'), ('great', 'ADJ'), ('Arthur', 'NOUN'), ('did', 'VERB'), (\"n't\", 'ADV'), ('feel', 'VERB'), ('VERY', 'ADV'), ('good', 'ADJ'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici la liste de possible POS tags:"
      ],
      "metadata": {
        "id": "6cECT-Gp2obD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **POS** | **DESCRIPTION**           | **EXAMPLES**                                      |\n",
        "| ------- | ------------------------- | ------------------------------------------------- |\n",
        "| ADJ     | adjective                 | big, old, green, incomprehensible, first      |\n",
        "| ADP     | adposition                | in, to, during                                |\n",
        "| ADV     | adverb                    | very, tomorrow, down, where, there            |\n",
        "| AUX     | auxiliary                 | is, has (done), will (do), should (do)        |\n",
        "| CONJ    | conjunction               | and, or, but                                  |\n",
        "| CCONJ   | coordinating conjunction  | and, or, but                                  |\n",
        "| DET     | determiner                | a, an, the                                    |\n",
        "| INTJ    | interjection              | psst, ouch, bravo, hello                      |\n",
        "| NOUN    | noun                      | girl, cat, tree, air, beauty                  |\n",
        "| NUM     | numeral                   | 1, 2017, one, seventy-seven, IV, MMXIV        |\n",
        "| PART    | particle                  | ’s, not                                      |\n",
        "| PRON    | pronoun                   | I, you, he, she, myself, themselves, somebody |\n",
        "| PROPN   | proper noun               | Mary, John, London, NATO, HBO                 |\n",
        "| PUNCT   | punctuation               | ., (, ), ?                                    |\n",
        "| SCONJ   | subordinating conjunction | if, while, that                               |\n",
        "| SYM     | symbol                    | $, %, §, ©, +, −, ×, ÷, =, :)               |\n",
        "| VERB    | verb                      | run, runs, running, eat, ate, eating          |\n",
        "| X       | other                     | sfpksdpsxmsa                                  |\n",
        "| SPACE   | space                     |                                                   |\n"
      ],
      "metadata": {
        "id": "kUDRUVYe2kLi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4f_8mvLFrOE"
      },
      "source": [
        "#### <font color = 'E3A440'>*3. Retirer la ponctuation*</font>\n",
        "\n",
        "Une autre opération consiste à retirer la ponctuation. Ce type de filtrage reduit le nombre de sugne graphique qui participent le moin à la constructuion de la sémantique de la prhase. \n",
        "Dans certain contexte, comme en stylometrie, ce processus est appliquée avec de terchniques plus sofistiquées. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0P7SkM5RFrOF",
        "outputId": "f78970a7-3c61-4058-a96c-f8c33fb2f526"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('At', 'ADP'), ('eight', 'NUM'), ('on', 'ADP'), ('Thursday', 'NOUN'), ('morning', 'NOUN'), ('the', 'DET'), ('great', 'ADJ'), ('Arthur', 'NOUN'), ('did', 'VERB'), ('feel', 'VERB'), ('VERY', 'ADV'), ('good', 'ADJ')]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# La ligne de code suivant itére sur chaque signe graphique et retient ceux qui contiennet de caracteres alphanumérique.\n",
        "words_pos = [(w, pos) for w, pos in words_pos if w.isalnum()]\n",
        "print(words_pos)\n",
        "len(words_pos)\n",
        "# Ikl est possible aussi d'utiilser le résultat de l'analyse morphosyntaxique pour éliminer la ponctuaction\n",
        "# words_pos = [(w, pos) for w, pos in words_pos if pos != '.']\n",
        "# print(words_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwcORV3vFrOG"
      },
      "source": [
        "#### <font color = 'E3A440'>*4. Convertir chaque caractère en minuscule*</font>\n",
        "\n",
        "Cette étape constitue une première opéraiton de normalisation des mots et leur réduction à une forme graphique unique. Ce genre d'étape permet de regrouper chaque occurence d'un mot sous une seule forme."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8tdwmniFrOG",
        "outputId": "350309c6-67f9-4ac5-8fdd-acee6323feae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('at', 'ADP'), ('eight', 'NUM'), ('on', 'ADP'), ('thursday', 'NOUN'), ('morning', 'NOUN'), ('the', 'DET'), ('great', 'ADJ'), ('arthur', 'NOUN'), ('did', 'VERB'), ('feel', 'VERB'), ('very', 'ADV'), ('good', 'ADJ')]\n"
          ]
        }
      ],
      "source": [
        "# La ligne de code suivant itére sur chaque signe graphique et le transforme en minuscule.\n",
        "words_pos = [(w.lower(), pos) for w, pos in words_pos]\n",
        "print(words_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd1RxnCnFrOH"
      },
      "source": [
        "#### <font color = 'E3A440'>*5. retirer les stopwrods (mots vides)*</font>\n",
        "\n",
        "Une autre opération de filtrage constitue dan l'élimination de mots fonctionnels. Cette liste de mots contient tout les connecteurs de phrase, comme \"et\", \"mais\", \"toutefois\" et de mots avec faible valeur sémantique, comem les verbes modaux. \n",
        "Comme d'autres opéraiton de filtrage, l'enjeuy est celui de nettoyer le plus possible le vocabulaire et de reduyire toutes les occurrences d'un mot sous une forme graphique unique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icU72m20FrOH",
        "outputId": "c3162af7-e04b-42d6-a353-be85c7df5ee7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ],
      "source": [
        "# Nous importons la liste de stopword en anglais\n",
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words(\"english\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ws2STtGKFrOH",
        "outputId": "c8ea2f11-c8ff-40f0-e026-39c71051620d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('eight', 'NUM'), ('thursday', 'NOUN'), ('morning', 'NOUN'), ('great', 'ADJ'), ('arthur', 'NOUN'), ('feel', 'VERB'), ('good', 'ADJ')]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# La ligne de code suivant itére sur chaque signe graphique et garde ce qui n sont pas dans la liste de stopword.\n",
        "words_pos = [(w, pos) for w, pos in words_pos if w not in stopwords.words(\"english\")]\n",
        "print(words_pos)\n",
        "len(words_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8acb4VWFrOI"
      },
      "source": [
        "#### <font color = 'E3A440'>*6. Rammener les mots à leur racine*</font> \n",
        "\n",
        "En suivant le même objectif, nous retirons le suffixe morphologique des mots, ce qui augmente le niveau de réduction de chaque occurrence d'un mot à une unique forme graphique.\n",
        "\n",
        "Ils existent deux méthode fondamentales: la racinisaiton et la lemmatisation.\n",
        "La première reduit les occurence à une racine qui est inférée au moyen de plusieur techniques, l'autre est la réduciton de l'occurrence à son lemme. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_mhuA7zFrOI",
        "outputId": "b6712ef1-a875-4f41-8dab-24b64a094cae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('eight', 'NUM'), ('thursday', 'NOUN'), ('morn', 'NOUN'), ('great', 'ADJ'), ('arthur', 'NOUN'), ('feel', 'VERB'), ('good', 'ADJ')]\n"
          ]
        }
      ],
      "source": [
        "# Racinisation: technique Porter\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmed_pos = [(PorterStemmer().stem(w), pos) for w, pos in words_pos]\n",
        "print(stemmed_pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnsV3pSZFrOI",
        "outputId": "1a559c9a-07d5-4d00-a811-3c5c91cb8179"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('eight', 'NUM'), ('thursday', 'NOUN'), ('morn', 'NOUN'), ('gre', 'ADJ'), ('arth', 'NOUN'), ('feel', 'VERB'), ('good', 'ADJ')]\n"
          ]
        }
      ],
      "source": [
        "# Racinisation: technique Lancaster\n",
        "from nltk.stem import LancasterStemmer\n",
        "stemmed_pos = [(LancasterStemmer().stem(w), pos) for w, pos in words_pos]\n",
        "print(stemmed_pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN7uAtUgFrOJ",
        "outputId": "c228556d-e2ee-4461-b5a3-f56152f406a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('eight', 'NUM'), ('thursday', 'NOUN'), ('morning', 'NOUN'), ('great', 'ADJ'), ('arthur', 'NOUN'), ('feel', 'VERB'), ('good', 'ADJ')]\n"
          ]
        }
      ],
      "source": [
        "# Lemmatisaiton: utilisant le thesaurus wordnet\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lemmed_pos = [(WordNetLemmatizer().lemmatize(w), pos) for w, pos in words_pos]\n",
        "print(lemmed_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color = 'E3A440'>*7. Filtrage selon le rôle morphosyntaxique*</font>\n",
        "\n",
        "Le filtrage des unités lexicales peut s'étendre jusqu'à l'élimination d'unités qui ne font pas partie d'une liste de rôles morphosyntaxique prédéfinie. "
      ],
      "metadata": {
        "id": "n-48nGApkl_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retenir seulement les noms et les adjectifs\n",
        "lemmed_pos = [(w, pos) for w, pos in words_pos if pos in ['NOUN','ADJ']]\n",
        "print(lemmed_pos)"
      ],
      "metadata": {
        "id": "FrOSEBCzk08r",
        "outputId": "c78437a4-b52c-4d74-ee25-ce811f5c54b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('thursday', 'NOUN'), ('morning', 'NOUN'), ('great', 'ADJ'), ('arthur', 'NOUN'), ('good', 'ADJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ges85oatFrOJ"
      },
      "source": [
        "## <font color = 'E3A440'>Traitement d'un texte</font>\n",
        "\n",
        "Le prétraitement d'un corpus de recherche peut mettre en place plusieurs autres étapes. La plus importante est la **segmentation**. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'>*1. Segmentation*</font>\n",
        "\n",
        "Tout dépendant de l'objectif de l'analyse, un segment peut être un document, un paragraphe, une concordance, un groupe de phrases, une phrase, etc.\n",
        "\n"
      ],
      "metadata": {
        "id": "GZqenN7Rcc2D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOzWtcugFrOK",
        "outputId": "ee2204c4-6ee5-445f-a1e8-aedacf2f7f3b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "175"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "text = \"\"\"At eight o'clock, on Thursday morning, the great Arthur didn't feel VERY good.\n",
        "          The following morning, at nine, Arthur felt better.\n",
        "          A dog run in the street.\"\"\"\n",
        "len(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans le bloc de code suivant, nous faisons une segmentation par pharse."
      ],
      "metadata": {
        "id": "sy8Qow43dHKv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Lihe1FNjFrOK",
        "outputId": "2d7515da-68f1-4e98-bf4b-f9168963ac68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"At eight o'clock, on Thursday morning, the great Arthur didn't feel VERY good.\", 'The following morning, at nine, Arthur felt better.', 'A dog run in the street.']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "sentences = nltk.sent_tokenize(text)\n",
        "print(sentences)\n",
        "len(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'>*2. Annotation et nettoyage*</font>\n",
        "\n",
        "Tout dépendant de l'objectif de l'analyse, un segment peut être un document, un paragraphe, une concordance, un groupe de phrases, une phrase, etc.\n"
      ],
      "metadata": {
        "id": "gHi0hAxZdKx4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color = 'E3A440'>*2.1 Création d'une fonction*</font>\n",
        "\n",
        "La cération de funciton est utile pour plusieurs raison. Dans notre casm, nous voulons englober Souvent, il est utile de créer de fonctions pour executer accomplir pluseurs étapes"
      ],
      "metadata": {
        "id": "waMXvFxhgH0u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "6rtngk2rFrOL"
      },
      "outputs": [],
      "source": [
        "# To run this function proprlely, you need to import modules needed\n",
        "def CleaningText(text_as_string, language = 'english', reduce = '', list_pos_to_keep = [], Stopwords_to_add = []):\n",
        "    from nltk.corpus import stopwords\n",
        "\n",
        "    words = nltk.word_tokenize(text_as_string)\n",
        "    words_pos = nltk.pos_tag(words, tagset='universal')\n",
        "    words_pos = [(w, pos) for w, pos in words_pos if w.isalnum()]\n",
        "    words_pos = [(w.lower(), pos) for w, pos in words_pos]\n",
        "    \n",
        "    if reduce == 'stem': \n",
        "        from nltk.stem.porter import PorterStemmer\n",
        "        reduced_words_pos = [(PorterStemmer().stem(w), pos) for w, pos in words_pos]\n",
        "        \n",
        "    elif reduce == 'lemma':\n",
        "        from nltk.stem.wordnet import WordNetLemmatizer\n",
        "        reduced_words_pos = [(WordNetLemmatizer().lemmatize(w), pos) for w, pos in words_pos]\n",
        "    else:\n",
        "        import warnings\n",
        "        reduced_words_pos = words_pos\n",
        "        warnings.warn(\"Warning : any reduction was made on words! Please, use \\\"reduce\\\" argument to chosse between 'stem' or  'lemma'\")\n",
        "    if list_pos_to_keep:\n",
        "        reduced_words_pos = [(w, pos) for w, pos in reduced_words_pos if pos in list_pos_to_keep]\n",
        "    else:\n",
        "        import warnings\n",
        "        warnings.warn(\"Warning : any POS filtering was made. Pleae, use \\\"list_pos_to_keep\\\" to create a list of POS tag to keep.\")\n",
        "    \n",
        "    list_stopwords = stopwords.words(language) + Stopwords_to_add\n",
        "    reduced_words_pos = [(w, pos) for w, pos in reduced_words_pos if w not in list_stopwords and len(w) > 1 ]\n",
        "    return reduced_words_pos\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = nltk.word_tokenize(text_as_string)\n",
        "words_pos = nltk.pos_tag(words, tagset='universal')\n",
        "words_pos = [(w, pos) for w, pos in words_pos if w.isalnum()]\n",
        "words_pos = [(w.lower(), pos) for w, pos in words_pos]\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "reduced_words_pos = [(WordNetLemmatizer().lemmatize(w), pos) for w, pos in words_pos]\n",
        "reduced_words_pos = [(w, pos) for w, pos in reduced_words_pos if pos in ['NOUN','ADJ','VERB']]\n",
        "list_stopwords = stopwords.words(language) + ['http']\n",
        "words_pos = [(w, pos) for w, pos in reduced_words_pos if w not in list_stopwords ]\n",
        "reduced_words_pos\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-7eG8dITqdz",
        "outputId": "b3b40339-3230-491d-fe46-11871e038425"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('entire', 'ADJ'),\n",
              " ('swiss', 'ADJ'),\n",
              " ('football', 'NOUN'),\n",
              " ('league', 'NOUN'),\n",
              " ('is', 'VERB'),\n",
              " ('hold', 'NOUN'),\n",
              " ('postponing', 'VERB'),\n",
              " ('game', 'NOUN'),\n",
              " ('professional', 'ADJ'),\n",
              " ('amateur', 'ADJ'),\n",
              " ('level', 'NOUN'),\n",
              " ('coronavirus', 'ADJ'),\n",
              " ('http', 'NOUN')]"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color = 'E3A440'>*2.2 Application function nettoyage*</font>\n",
        "\n",
        "Maintenant, nous pouvons apliquer cette function à chacun de not segment. dans notre cas il s'agit de phrases."
      ],
      "metadata": {
        "id": "kWkIfcMwpqe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_sentences = [CleaningText(sent) for sent in sentences]\n",
        "print(cleaned_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVcF-aGGppjd",
        "outputId": "2e910320-d5f8-4c30-a0a0-09a6e296674d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('eight', 'NUM'), ('thursday', 'NOUN'), ('morning', 'NOUN'), ('great', 'ADJ'), ('arthur', 'NOUN'), ('feel', 'VERB'), ('good', 'ADJ')], [('following', 'ADJ'), ('morning', 'NOUN'), ('nine', 'NUM'), ('arthur', 'NOUN'), ('felt', 'VERB'), ('better', 'ADV')], [('dog', 'NOUN'), ('run', 'NOUN'), ('street', 'NOUN')]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: UserWarning: Warning : any reduction was made on words! Please, use \"reduce\" argument to chosse between 'stem' or  'lemma'\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: UserWarning: Warning : any POS filtering was made. Pleae, use \"list_pos_to_keep\" to create a list of POS tag to keep.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_sentences = [CleaningText(sent, reduce = 'lemma', list_pos_to_keep = ['']) for sent in sentences]\n",
        "print(cleaned_sentences)"
      ],
      "metadata": {
        "id": "vkpxyQfl2Wna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color = 'E3A440'>*2.3 Fréquence mots*</font>\n",
        "\n",
        "Quelle est la fréquence des mots de notre texte? Commencon par retirer le POS tag, pour obtenir une liste de listes de mots ( et non une liste de tuple mot-pos)."
      ],
      "metadata": {
        "id": "TBNxooILr8-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freqs_in_text = nltk.FreqDist([w for sent in cleaned_sentences for w, pos in sent ])\n",
        "freqs_in_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFPhg9Sm6Lbq",
        "outputId": "5012cd6c-2351-4552-ffb0-5d0c9f5f245a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'morning': 2, 'arthur': 2, 'eight': 1, 'thursday': 1, 'great': 1, 'feel': 1, 'good': 1, 'following': 1, 'nine': 1, 'felt': 1, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'>*3. Vectorisation*</font>\n",
        "\n",
        "Généralement, pour utiliser le texte dans un contexte d'analyse de données ou d'apprentissage automatique, ce texte doit être transformé dans un objet mathématique approprié. \n",
        "Le modèle le plus simple et connu est le \"bags-of-words\", dans lequel chaque document (ou chaque mot) est défini par un certain nombre d'unités lexicales qui le caractérise. "
      ],
      "metadata": {
        "id": "m0YdoOA1stB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisation de l'objet\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def identity_tokenizer(text):\n",
        "    return text\n",
        "\n",
        "# Transforming the word in frequencies\n",
        "vectorized = CountVectorizer(lowercase = False, # Convert all characters to lowercase before tokenizing\n",
        "                             min_df = 1, # Ignore terms that have a document frequency strictly lower than the given threshold \n",
        "                             max_df = 10, # Ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words)\n",
        "                             stop_words = stopwords.words('english'), # Remove the list of words provided\n",
        "                             ngram_range = (1, 1), # Get the lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted\n",
        "                             tokenizer=identity_tokenizer) # Override the string tokenization step while preserving the preprocessing and n-grams generation steps"
      ],
      "metadata": {
        "id": "LpFPJrGI2Nsi"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilisation du \"vectorizer\" avec une liste de listes de mot (et non une liste de tuple de mots-pos)."
      ],
      "metadata": {
        "id": "G9uC3usB5dtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Liste de liste de mots:\n",
        "[[w for w, pos in sent] for sent in cleaned_sentences]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gy8_VTM67Lax",
        "outputId": "466c8c22-e795-44c3-ca45-1b0f375dd43e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['eight', 'thursday', 'morning', 'great', 'arthur', 'feel', 'good'],\n",
              " ['following', 'morning', 'nine', 'arthur', 'felt', 'better'],\n",
              " ['dog', 'run', 'street']]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pplication du vectorizer\n",
        "freq_term_DTM = vectorized.fit_transform([[w for w, pos in sent] for sent in cleaned_sentences])\n",
        "print(pd.DataFrame(freq_term_DTM.todense(), columns =  [k for k, v in sorted(vectorized.vocabulary_.items(), key=lambda item: item[1])] ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8ydVdmC5c76",
        "outputId": "2d941164-981a-40f9-be97-3958bc15b3e9"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   arthur  better  dog  eight  feel  felt  following  good  great  morning  \\\n",
            "0       1       0    0      1     1     0          0     1      1        1   \n",
            "1       1       1    0      0     0     1          1     0      0        1   \n",
            "2       0       0    1      0     0     0          0     0      0        0   \n",
            "\n",
            "   nine  run  street  thursday  \n",
            "0     0    0       0         1  \n",
            "1     1    0       0         0  \n",
            "2     0    1       1         0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QymfV4S_FrOZ"
      },
      "source": [
        "## Exercise : Sentiment analysis on a COVID-19 twetter dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_DIR='Donnees_demystifiees_seance_6/'\n",
        "DATA_DIR=os.path.join(ROOT_DIR, 'Data')\n",
        "os.listdir(DATA_DIR)"
      ],
      "metadata": {
        "id": "ysoSVGc_soym",
        "outputId": "a7a89a31-c858-458e-b918-357fe6d9a47c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-118-3ee1169a22ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mROOT_DIR\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Donnees_demystifiees_seance_6/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mDATA_DIR\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROOT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Donnees_demystifiees_seance_6/Data'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "pgYI5oVat10-",
        "outputId": "9c34d74f-8b5d-47f0-9800-af607232f4a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import gzip\n",
        "import zipfile\n",
        "!pip install wget\n",
        "import wget\n",
        "from datetime import datetime\n",
        "\n",
        "#dataset_URL = \"http://www.trackmyhashtag.com/data/COVID-19.zip\" #@param {type:\"string\"}\n",
        "dataset_URL = \"https://drive.google.com/drive/folders/11w4geFB6p17hFlWseBpHJQbhARINvTOc\" #@param {type:\"string\"}\n",
        "#Downloads the dataset\n",
        "wget.download(dataset_URL, out='Top_50_profile-test.zip')\n",
        "#Unzips the dataset and gets the TSV dataset\n",
        "with zipfile.ZipFile('Top_50_profile-test.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "\n",
        "\n",
        "#dateparse = lambda x: datetime.strptime(x, '%d %b %Y %H:%M:%S')\n",
        "#df = pd.read_csv(infile, parse_dates=['datetime'], date_parser=dateparse)\n",
        "#Import dataset\n",
        "#df = pd.read_csv('COVID.csv', parse_dates=['Tweet Posted Time (UTC)', 'User Account Creation Date'], date_parser=dateparse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "Q6snBPy8EfBX",
        "outputId": "f623e679-293e-42eb-e806-26ef7df0b6f8"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "BadZipFile",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-108-14d53db66da7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mwget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_URL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Top_50_profile-test.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#Unzips the dataset and gets the TSV dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Top_50_profile-test.zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel)\u001b[0m\n\u001b[1;32m   1256\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1258\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RealGetContents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1259\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m                 \u001b[0;31m# set the modified flag so central directory gets written\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m_RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1323\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mendrec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1325\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1326\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadZipFile\u001b[0m: File is not a zip file"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir()"
      ],
      "metadata": {
        "id": "qYivF2rAhptB",
        "outputId": "5c4d9fbe-d6ac-4b1e-b955-4b2cfc55ed15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'COVID-19 (1).zip',\n",
              " 'COVID-images.csv',\n",
              " 'Top_50_profile.zip',\n",
              " 'Top_50_profile (1).zip',\n",
              " 'COVID-videos.csv',\n",
              " 'COVID.csv',\n",
              " 'COVID-19.zip',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "#Select english\n",
        "df=df[df['Tweet Language']=='English'].reset_index()\n",
        "df"
      ],
      "metadata": {
        "id": "Px_yuxpuKADu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejgQAfv8FIrC",
        "outputId": "ef7d0818-7b29-4162-c81c-500b9e74773e"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "index                                  int64\n",
              "Tweet Id                              object\n",
              "Tweet URL                             object\n",
              "Tweet Posted Time (UTC)       datetime64[ns]\n",
              "Tweet Content                         object\n",
              "Tweet Type                            object\n",
              "Client                                object\n",
              "Retweets Received                      int64\n",
              "Likes Received                         int64\n",
              "Tweet Location                        object\n",
              "Lat                                  float64\n",
              "Long                                 float64\n",
              "Tweet Language                        object\n",
              "User Id                               object\n",
              "Name                                  object\n",
              "Screen Name                           object\n",
              "User Bio                              object\n",
              "Verified or Non-Verified              object\n",
              "Profile URL                           object\n",
              "Protected or Non-protected            object\n",
              "User Followers                         int64\n",
              "User Following                         int64\n",
              "User Account Creation Date    datetime64[ns]\n",
              "neg                                  float64\n",
              "neu                                  float64\n",
              "pos                                  float64\n",
              "compound                             float64\n",
              "neg_category                        category\n",
              "pos_category                        category\n",
              "compound_category                   category\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54zPNygEJXhe",
        "outputId": "c87f00d7-95fe-4165-d886-75c9d3b503c6"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "index                                                                         0\n",
              "Tweet Id                                                  \"1233417783175778304\"\n",
              "Tweet URL                     https://twitter.com/Giussi92/status/1233417783...\n",
              "Tweet Posted Time (UTC)                                     2020-02-28 15:44:49\n",
              "Tweet Content                 Also the entire Swiss Football League is on ho...\n",
              "Tweet Type                                                                Tweet\n",
              "Client                                                       Twitter for iPhone\n",
              "Retweets Received                                                             0\n",
              "Likes Received                                                                0\n",
              "Tweet Location                                                              NaN\n",
              "Lat                                                                         NaN\n",
              "Long                                                                        NaN\n",
              "Tweet Language                                                          English\n",
              "User Id                                                            \"1556856595\"\n",
              "Name                                                           Giuseppe Gentile\n",
              "Screen Name                                                            Giussi92\n",
              "User Bio                                                                    NaN\n",
              "Verified or Non-Verified                                               Verified\n",
              "Profile URL                                        https://twitter.com/Giussi92\n",
              "Protected or Non-protected                                        Non-Protected\n",
              "User Followers                                                             3071\n",
              "User Following                                                              100\n",
              "User Account Creation Date                                  2013-06-30 00:27:50\n",
              "neg                                                                       0.077\n",
              "neu                                                                       0.923\n",
              "pos                                                                         0.0\n",
              "compound                                                                 -0.128\n",
              "neg_category                                                              lower\n",
              "pos_category                                                              lower\n",
              "compound_category                                                           neu\n",
              "Name: 0, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyRsWf41JhsH",
        "outputId": "48c3fd63-4a78-49a2-8255-08cd6698978c"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(33174, 22)"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Tweet Content']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yBg-jU9QLl8",
        "outputId": "3c0b9d95-9899-446c-8bf6-932711b6431f"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        Also the entire Swiss Football League is on ho...\n",
              "1        World Health Org Official: Trump’s press confe...\n",
              "2        I mean, Liberals are cheer-leading this #Coron...\n",
              "3        Under repeated questioning, Pompeo refuses to ...\n",
              "4        #coronavirus comments now from @larry_kudlow h...\n",
              "                               ...                        \n",
              "33169    RT @timhquotes: It's my party, you're invited!...\n",
              "33170    RT @timhquotes: It's my party, you're invited!...\n",
              "33171    It's my party, you're invited!\\n\\nPS, this is ...\n",
              "33172    Amy’s a survivor! #bariclab #pnnl #movingon #c...\n",
              "33173    A review of asymptomatic and sub-clinical Midd...\n",
              "Name: Tweet Content, Length: 33174, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cette opération prendra 2 minutes, donc, nous vous suggérons de continuer la lecture dce l'exercices pour vous faire une idée de ce qui s'en vient."
      ],
      "metadata": {
        "id": "_zmlEGCKRaWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_tweets = [CleaningText(sent, reduce = 'lemma', list_pos_to_keep = ['NOUN', 'ADJ', 'VERB'], Stopwords_to_add=['http']) for sent in list(df['Tweet Content'])]\n"
      ],
      "metadata": {
        "id": "NMmjI8IaU_JF"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisation de l'objet\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def identity_tokenizer(text):\n",
        "    return text\n",
        "\n",
        "# Transforming the word in frequencies\n",
        "vectorized = CountVectorizer(lowercase = False, # Convert all characters to lowercase before tokenizing\n",
        "                             min_df = 25, # Ignore terms that have a document frequency strictly lower than the given threshold \n",
        "                             max_df = 25000, # Ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words)\n",
        "                             stop_words = stopwords.words('english'), # Remove the list of words provided\n",
        "                             ngram_range = (1, 1), # Get the lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted\n",
        "                             tokenizer=identity_tokenizer) # Override the string tokenization step while preserving the preprocessing and n-grams generation steps"
      ],
      "metadata": {
        "id": "6Aj4JUdeYBbE"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq_term_DTM = vectorized.fit_transform([[w for w, pos in sent] for sent in cleaned_tweets])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkSkuQvUXsvs",
        "outputId": "5994434c-89b8-4dd7-e08e-fb8dd924edd8"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'b', 'c', 'e', 'f', 'g', 'h', 'j', 'l', 'n', 'p', 'r', 'u', 'v', 'w'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "freq_term_DTM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLii5X9bXytj",
        "outputId": "db08ae7f-174b-4d61-e685-5702d3d38bca"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<33174x2352 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 419189 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Tweet Content'].iloc[[0,1,2,3]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjVh-diKU4O-",
        "outputId": "17b037b3-4ab6-4f98-af33-44aea557fbc6"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    Also the entire Swiss Football League is on ho...\n",
              "1    World Health Org Official: Trump’s press confe...\n",
              "2    I mean, Liberals are cheer-leading this #Coron...\n",
              "3    Under repeated questioning, Pompeo refuses to ...\n",
              "Name: Tweet Content, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(CleaningText(df['Tweet Content'].iloc[0], reduce = 'lemma', list_pos_to_keep = ['NOUN', 'ADJ', 'VERB'], Stopwords_to_add=['http']))\n",
        "\n",
        "text_as_string = df['Tweet Content'].iloc[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMcuOlfzRqjk",
        "outputId": "10519d0b-4ab1-47e6-c4b1-5fb2f487a763"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'http']\n",
            "[('entire', 'ADJ'), ('swiss', 'ADJ'), ('football', 'NOUN'), ('league', 'NOUN'), ('hold', 'NOUN'), ('postponing', 'VERB'), ('game', 'NOUN'), ('professional', 'ADJ'), ('amateur', 'ADJ'), ('level', 'NOUN'), ('coronavirus', 'ADJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pInoj9yrFrOZ",
        "outputId": "fdd327c7-78f2-473d-bcc5-13557871cd3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'neg': 0.0, 'neu': 0.295, 'pos': 0.705, 'compound': 0.8012}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "sia.polarity_scores(\"Wow, NLTK is really powerful!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6jAgx5YgFrOa",
        "outputId": "971783e4-5ec5-4345-fd9f-1f78b1045de7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'neg': 0.0, 'neu': 0.488, 'pos': 0.512, 'compound': 0.484}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "sia.polarity_scores(\"NLTK is not bad!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kpGJWGRWFrOa",
        "outputId": "ad3c6676-75a7-4b98-fbf7-a9467477def4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'neg': 0.655, 'neu': 0.345, 'pos': 0.0, 'compound': -0.5848}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "sia.polarity_scores(\"NLTK is bad!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[[0,1,2,3]]"
      ],
      "metadata": {
        "id": "sa4wiodIZIIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasent = df.apply(lambda x: sia.polarity_scores(x['Tweet Content']), 1)\n",
        "df = df.join(pd.DataFrame(list(datasent)))\n",
        "df"
      ],
      "metadata": {
        "id": "3etQFxEACpty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "Xf_vn9Veebrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['neg','neu','pos','compound']].sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZaZxT-2bTrK",
        "outputId": "580f923b-a9a9-4190-e563-90e199bc316a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "neg          2699.5530\n",
              "neu         28289.4240\n",
              "pos          2184.1080\n",
              "compound    -2187.4991\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bins = [0, 0.4, 0.50, 0.6]\n",
        "names = ['lower', 'medium', 'high']\n",
        "df['neg_category']  = pd.cut(df['neg'], bins, labels=names, include_lowest =True)\n",
        "df['pos_category']  = pd.cut(df['pos'], bins, labels=names, include_lowest =True)\n"
      ],
      "metadata": {
        "id": "m4fm56nserhy"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['neg_category'] == 'high'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsrrXdIshaxA",
        "outputId": "8f4b4d4b-f282-4923-cbb1-b53a0c124040"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12, 29)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['pos_category'] == 'high'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPrxRe1lOgM-",
        "outputId": "35672edc-1c4c-48ac-97a3-21558ddd2f4d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15, 29)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['compound'].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhZ4v8p5ghXJ",
        "outputId": "4daf26c4-9167-42c3-84ab-647fea7bf35d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    33174.000000\n",
              "mean        -0.065940\n",
              "std          0.461793\n",
              "min         -0.982800\n",
              "25%         -0.421500\n",
              "50%          0.000000\n",
              "75%          0.318200\n",
              "max          0.986200\n",
              "Name: compound, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bins = [-np.inf, -0.5, 0.5, 1]\n",
        "names = ['high_negative', 'neu', 'high_positive']\n",
        "df['compound_category']  = pd.cut(df['compound'], bins, labels=names, include_lowest =True)"
      ],
      "metadata": {
        "id": "jJrj4DKePhGd"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['compound_category'] == 'high_positive']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mwSxS3kiAaz",
        "outputId": "c4c1b91b-72a1-4fb5-c7a0-e68d2f9c7d9c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7        “When the virus hit, I felt that I should stay...\n",
              "40       The closure of #Iran's parliament comes after ...\n",
              "57       Join me on this webinar today. It's free and o...\n",
              "64       How can public health advocates encourage citi...\n",
              "67       Some beards are better then others if wearing ...\n",
              "                               ...                        \n",
              "33120    RT @hayesluk: Point 3 is especially insightful...\n",
              "33124    RT @hayesluk: Point 3 is especially insightful...\n",
              "33128    RT @hayesluk: Point 3 is especially insightful...\n",
              "33129    RT @hayesluk: Point 3 is especially insightful...\n",
              "33133    Point 3 is especially insightful after Prof. D...\n",
              "Name: Tweet Content, Length: 4119, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logical_vector = df['compound_category'] == 'high_positive'"
      ],
      "metadata": {
        "id": "TVhrPm7PZO5E"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(logical_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogaDNEUNZQU0",
        "outputId": "43c33434-13e4-4a97-b0c9-96335b2584e4"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4119"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_freq_target = pd.DataFrame(np.asarray(freq_term_DTM[logical_vector].sum(0).T).reshape(-1))"
      ],
      "metadata": {
        "id": "39eDHLjFZcLU"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_freq_target.index = [word for (word,idx) in sorted(vectorized.vocabulary_.items(), key= lambda x:x[1])]"
      ],
      "metadata": {
        "id": "a3GRoD5dZvCM"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_freq_target.columns = ['freq1']\n",
        "df_freq_target['freq2'] = np.asarray(freq_term_DTM[~(logical_vector)].sum(0).T).reshape(-1)"
      ],
      "metadata": {
        "id": "7T-uQHt3aAXD"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_freq_target['tot'] = df_freq_target['freq1'] + df_freq_target['freq2']"
      ],
      "metadata": {
        "id": "WBE1DowGbIcF"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_freq_target['freq1'] = df_freq_target['freq1'].apply(lambda x: 0.0000001 if x == 0 else x).astype(float)\n",
        "df_freq_target['freq2'] = df_freq_target['freq2'].apply(lambda x: 0.0000001 if x == 0 else x).astype(float)\n",
        "#\n",
        "df_freq_target['freq1_norm'] = df_freq_target['freq1']/df_freq_target['freq1'].sum() * 1000000\n",
        "df_freq_target['freq2_norm'] = df_freq_target['freq2']/df_freq_target['freq2'].sum() * 1000000\n",
        "#\n",
        "df_freq_target['fraction'] = df_freq_target['freq1_norm'] / df_freq_target['freq2_norm']\n",
        "df_freq_target['Log-likelihood Ratio'] = df_freq_target['fraction'].apply(math.log2)"
      ],
      "metadata": {
        "id": "WQrJluDTbOg1"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "df_freq_target['Log-likelihood Ratio'] = df_freq_target['fraction'].apply(math.log2)"
      ],
      "metadata": {
        "id": "GqTbw6xwboou"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frequency_threshold = 10 # Insert your frequency threshold as integer\n",
        "df_freq_target[df_freq_target['tot'] > frequency_threshold]['Log-likelihood Ratio'].sort_values(ascending=False).iloc[range(50)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocFTL9KAbymc",
        "outputId": "7b19cc0e-a4df-4187-b3da-367836b7f2a1"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "choculate             33.384676\n",
              "relax                 32.079821\n",
              "tornadotrump          31.773718\n",
              "tombstonewyatt        31.773718\n",
              "arveecasil09          31.484212\n",
              "vid                   31.418623\n",
              "retested              31.349911\n",
              "followthemoney        31.079821\n",
              "redistribution        31.079821\n",
              "ahhh                  31.079821\n",
              "reinforces            30.799713\n",
              "linkupfr              30.799713\n",
              "fwasteria             30.799713\n",
              "atomaraullo           30.747246\n",
              "stormsabine           30.692798\n",
              "dgb                   30.692798\n",
              "rajeev                30.692798\n",
              "lipa                  30.692798\n",
              "pennyzhou025          30.692798\n",
              "linkup                30.692798\n",
              "firefightaustralia    30.692798\n",
              "tempeteciara          30.692798\n",
              "fernando              30.692798\n",
              "creative               8.466782\n",
              "profound               8.323824\n",
              "jason                  7.908787\n",
              "tide                   7.693058\n",
              "speaker                7.470666\n",
              "dua                    7.439302\n",
              "patrol                 7.270243\n",
              "epoch                  6.908787\n",
              "commitment             6.908787\n",
              "hair                   6.596843\n",
              "volcano                6.439302\n",
              "surprising             6.439302\n",
              "festival               6.411287\n",
              "walking                6.182780\n",
              "village                6.123526\n",
              "equipped               5.963828\n",
              "saving                 5.948315\n",
              "responder              5.876365\n",
              "pheltzcomics           5.738862\n",
              "sf                     5.717488\n",
              "beautiful              5.693058\n",
              "facemasks              5.664861\n",
              "useful                 5.586859\n",
              "wealth                 5.504397\n",
              "streaming              5.493749\n",
              "武汉加油                   5.382718\n",
              "loved                  5.198293\n",
              "Name: Log-likelihood Ratio, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_freq_target['freq1'].sort_values(ascending=False).iloc[range(20)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoTIyCR7daI7",
        "outputId": "28cb7889-110c-48f1-979a-9afc9ddc3135"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "rt           5232.0\n",
              "china        2615.0\n",
              "people       1928.0\n",
              "wuhan        1168.0\n",
              "doctor       1145.0\n",
              "death        1063.0\n",
              "health       1022.0\n",
              "case         1007.0\n",
              "city          936.0\n",
              "ha            896.0\n",
              "lady          883.0\n",
              "infected      882.0\n",
              "wa            867.0\n",
              "patient       859.0\n",
              "new           852.0\n",
              "virus         760.0\n",
              "confirmed     702.0\n",
              "public        699.0\n",
              "chinese       697.0\n",
              "amp           667.0\n",
              "Name: freq1, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_freq_target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "ymwrApw8aT7r",
        "outputId": "c8a99cff-5def-4e58-a247-81205a8e90ac"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                        freq1  freq2  tot  freq1_norm   freq2_norm  \\\n",
              "abc7             1.000000e-07  102.0  102    0.000002   303.936876   \n",
              "abc7newsbayarea  7.000000e+00  155.0  162  142.377708   461.864861   \n",
              "able             1.000000e+01   56.0   66  203.396725   166.867305   \n",
              "abscbnnews       1.000000e-07  117.0  117    0.000002   348.633476   \n",
              "abuse            1.000000e-07  619.0  619    0.000002  1844.479672   \n",
              "...                       ...    ...  ...         ...          ...   \n",
              "york             4.000000e+00   55.0   59   81.358690   163.887531   \n",
              "youtube          1.500000e+01  119.0  134  305.095088   354.593023   \n",
              "zero             1.000000e-07   56.0   56    0.000002   166.867305   \n",
              "zerohedge        5.000000e+00   72.0   77  101.698363   214.543678   \n",
              "zone             2.000000e+01  152.0  172  406.793451   452.925541   \n",
              "\n",
              "                     fraction  Log-likelihood Ratio  \n",
              "abc7             6.692071e-09            -27.154900  \n",
              "abc7newsbayarea  3.082670e-01             -1.697748  \n",
              "able             1.218913e+00              0.285595  \n",
              "abscbnnews       5.834113e-09            -27.352839  \n",
              "abuse            1.102732e-09            -29.756270  \n",
              "...                       ...                   ...  \n",
              "york             4.964300e-01             -1.010338  \n",
              "youtube          8.604092e-01             -0.216905  \n",
              "zero             1.218913e-08            -26.289830  \n",
              "zerohedge        4.740217e-01             -1.076975  \n",
              "zone             8.981464e-01             -0.154977  \n",
              "\n",
              "[1443 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f581c0b3-6ec8-44f3-95c6-5c7bb7df8a63\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>freq1</th>\n",
              "      <th>freq2</th>\n",
              "      <th>tot</th>\n",
              "      <th>freq1_norm</th>\n",
              "      <th>freq2_norm</th>\n",
              "      <th>fraction</th>\n",
              "      <th>Log-likelihood Ratio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>abc7</th>\n",
              "      <td>1.000000e-07</td>\n",
              "      <td>102.0</td>\n",
              "      <td>102</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>303.936876</td>\n",
              "      <td>6.692071e-09</td>\n",
              "      <td>-27.154900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>abc7newsbayarea</th>\n",
              "      <td>7.000000e+00</td>\n",
              "      <td>155.0</td>\n",
              "      <td>162</td>\n",
              "      <td>142.377708</td>\n",
              "      <td>461.864861</td>\n",
              "      <td>3.082670e-01</td>\n",
              "      <td>-1.697748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>able</th>\n",
              "      <td>1.000000e+01</td>\n",
              "      <td>56.0</td>\n",
              "      <td>66</td>\n",
              "      <td>203.396725</td>\n",
              "      <td>166.867305</td>\n",
              "      <td>1.218913e+00</td>\n",
              "      <td>0.285595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>abscbnnews</th>\n",
              "      <td>1.000000e-07</td>\n",
              "      <td>117.0</td>\n",
              "      <td>117</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>348.633476</td>\n",
              "      <td>5.834113e-09</td>\n",
              "      <td>-27.352839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>abuse</th>\n",
              "      <td>1.000000e-07</td>\n",
              "      <td>619.0</td>\n",
              "      <td>619</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>1844.479672</td>\n",
              "      <td>1.102732e-09</td>\n",
              "      <td>-29.756270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>york</th>\n",
              "      <td>4.000000e+00</td>\n",
              "      <td>55.0</td>\n",
              "      <td>59</td>\n",
              "      <td>81.358690</td>\n",
              "      <td>163.887531</td>\n",
              "      <td>4.964300e-01</td>\n",
              "      <td>-1.010338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>youtube</th>\n",
              "      <td>1.500000e+01</td>\n",
              "      <td>119.0</td>\n",
              "      <td>134</td>\n",
              "      <td>305.095088</td>\n",
              "      <td>354.593023</td>\n",
              "      <td>8.604092e-01</td>\n",
              "      <td>-0.216905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zero</th>\n",
              "      <td>1.000000e-07</td>\n",
              "      <td>56.0</td>\n",
              "      <td>56</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>166.867305</td>\n",
              "      <td>1.218913e-08</td>\n",
              "      <td>-26.289830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zerohedge</th>\n",
              "      <td>5.000000e+00</td>\n",
              "      <td>72.0</td>\n",
              "      <td>77</td>\n",
              "      <td>101.698363</td>\n",
              "      <td>214.543678</td>\n",
              "      <td>4.740217e-01</td>\n",
              "      <td>-1.076975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zone</th>\n",
              "      <td>2.000000e+01</td>\n",
              "      <td>152.0</td>\n",
              "      <td>172</td>\n",
              "      <td>406.793451</td>\n",
              "      <td>452.925541</td>\n",
              "      <td>8.981464e-01</td>\n",
              "      <td>-0.154977</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1443 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f581c0b3-6ec8-44f3-95c6-5c7bb7df8a63')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f581c0b3-6ec8-44f3-95c6-5c7bb7df8a63 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f581c0b3-6ec8-44f3-95c6-5c7bb7df8a63');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def lexical_keyness(DTM, df, categories, vocabulary_vectorize, terget_category):\n",
        "    import math\n",
        "    # This code ref takes inspiration from this python module : https://pypi.org/project/corpus-toolkit/\n",
        "    # and its main script:  https://github.com/kristopherkyle/corpus_toolkit/blob/master/corpus_toolkit/corpus_tools.py\n",
        "    # which is based on this paper: https://aclanthology.org/J93-1003/\n",
        "\n",
        "    cluster_keyness = n_cluster\n",
        "    logical_vector = df['compound_category'] == 'high_positive'\n",
        "    df_freq_target = pd.DataFrame(np.asarray(DTM[cls_kmeans.labels_ == cluster_keyness].sum(0).T).reshape(-1))#, columns = [word for (word,idx) in sorted(vectorized.vocabulary_.items(), key= lambda x:x[1])]))\n",
        "    df_freq_target.index = [word for (word,idx) in sorted(vocabulary_vectorize.items(), key= lambda x:x[1])]\n",
        "    df_freq_target.index\n",
        "    df_freq_target.columns = ['freq1']\n",
        "    df_freq_target['freq2'] = np.asarray(DTM[~(cls_kmeans.labels_ == cluster_keyness)].sum(0).T).reshape(-1)\n",
        "    #\n",
        "    df_freq_target['tot'] = df_freq_target['freq1'] + df_freq_target['freq2']\n",
        "    #\n",
        "    df_freq_target['freq1'] = df_freq_target['freq1'].apply(lambda x: 0.0000001 if x == 0 else x).astype(float)\n",
        "    df_freq_target['freq2'] = df_freq_target['freq2'].apply(lambda x: 0.0000001 if x == 0 else x).astype(float)\n",
        "    #\n",
        "    df_freq_target['freq1_norm'] = df_freq_target['freq1']/df_freq_target['freq1'].sum() * 1000000\n",
        "    df_freq_target['freq2_norm'] = df_freq_target['freq2']/df_freq_target['freq2'].sum() * 1000000\n",
        "    #\n",
        "    df_freq_target['fraction'] = df_freq_target['freq1_norm'] / df_freq_target['freq2_norm']\n",
        "    df_freq_target['Log-likelihood Ratio'] = df_freq_target['fraction'].apply(math.log2)\n",
        "    return df_freq_target\n"
      ],
      "metadata": {
        "id": "AqILvRH7Tspd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UNK1isOmV2PK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "jRutULJCFrOb",
        "outputId": "d6d85c85-277d-4e8b-922c-42f128a0d3f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    {'neg': 0.077, 'neu': 0.923, 'pos': 0.0, 'comp...\n",
              "1    {'neg': 0.0, 'neu': 0.917, 'pos': 0.083, 'comp...\n",
              "2    {'neg': 0.0, 'neu': 0.839, 'pos': 0.161, 'comp...\n",
              "3    {'neg': 0.092, 'neu': 0.789, 'pos': 0.119, 'co...\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ],
      "source": [
        "datasent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51pYWFi5FrOc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}