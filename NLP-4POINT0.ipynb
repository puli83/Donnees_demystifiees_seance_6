{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWNgJtQ3FrN8"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "<font size='10' color = 'E3A440'>**Mégadonnées et techniques avancées démystifiées**</font>\n",
        "=======\n",
        "<font color = 'E3A440'>*Nouvelles méthodes d’analyse et leur implication quant à la gestion des mégadonnées en SSH (partie 1)*</font>\n",
        "=============\n",
        "\n",
        "\n",
        "Cet atelier s’inscrit dans le cadre de la formation [Mégadonnées et techniques avancées démystifiées](https://www.4point0.ca/2022/08/22/formation-megadonnees-demystifiees/) (séance 6).\n",
        "\n",
        "Les sciences humaines et sociales sont souvent confrontées à l’analyse de données non structurées, comme le texte. Après avoir préparé les données, plusieurs techniques d’analyse venant de l’apprentissage automatique peuvent être utilisées. Pendant cet atelier, les participants seront initiés aux méthodes supervisées et non supervisées à des buts d’analyse avec Python.\n",
        "\n",
        "Note : Cet atelier se poursuit lors d’une 2e séance le 10 novembre.\n",
        "\n",
        "Structure de l'atelier :\n",
        "1. Presentation of sections 1 and 2 in a plenary mode (20 minutes)\n",
        "2. Individual work on section 3 (20 minutes)\n",
        "3. Group work on section 4 (60 minutes)\n",
        "4. Plenary session with groups presentations (20 minutes)\n",
        "\n",
        "Ce tutoriel ne peut pas être consideré exaustif .... \n",
        "\n",
        "### Authors: \n",
        "- Bruno Agard <bruno.agard@polymtl.ca>\n",
        "- Davide Pulizzotto <davide.pulizzotto@polymtl.ca>\n",
        "\n",
        "### Table of Contents\n",
        "Bruno Agard\n",
        "\n",
        "Département de Mathématiques et de génie Industriel\n",
        "\n",
        "École Polytechnique de Montréal"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Préparation environnement"
      ],
      "metadata": {
        "id": "HpAX32-4Kj8U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jddhTL7EFrN_",
        "outputId": "5fbb1c0c-ea6e-4196-e427-ae36b5d99ed6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Donnees_demystifiees_seance_6'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ],
      "source": [
        "# Downloading of data from the GitHub project\n",
        "!rm -rf Donnees_demystifiees_seance_6/\n",
        "!git clone https://github.com/puli83/Donnees_demystifiees_seance_6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4E7zFXjFrN_"
      },
      "outputs": [],
      "source": [
        "# Install packages\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJLRFWa6FrOA",
        "outputId": "f2c76983-8b35-4f82-fe77-d887a82942f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "# Import modules\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwFKNpPQFrN-"
      },
      "source": [
        "# 1 er 45 minutes préparation des textes\n",
        "# <font color = 'E3A440'>*Préparation des données textulles*</font>\n",
        "\n",
        "L'analyse de données textuelles implique la transformation d'un texte en un objet mathematique qui peut être utilisé par des algorithmes et des modèles statistique. Cette étape est importante car permet de **structurer** des données non structurées, comme le texte.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDS10IXPFrOB"
      },
      "source": [
        "###  <font color = 'E3A440'>**Exemple d'analyse d'une phrase**</font>\n",
        "\n",
        "Prenons une phrase pour decortiquer les passages que nous pemrettent de la transformer en information structurée."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "h9AVaykcFrOB"
      },
      "outputs": [],
      "source": [
        "sentence = \"\"\"At eight o'clock, on Thursday morning, the great Arthur didn't feel VERY good.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpFqEU4iFrOC",
        "outputId": "256bb46b-1d67-474c-a6ea-4553b6320e7d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "78"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "len(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWQNjvKIFrOD"
      },
      "source": [
        "#### <font color = 'E3A440'>*1. Tokenisation*</font>\n",
        "\n",
        "Cette étape consiste à couper la phrase en unités linguistiques élémantaire et dotées de sens, ce qui est gnralement appelé le \"mot\".\n",
        "\n",
        "Dans le module `nltk`, il existe une fonction qui permet cette opération, soit `word_tokenize()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzF3fiUWFrOE",
        "outputId": "9d833921-b139-4866-b9b4-3a32a5893784"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['At', 'eight', \"o'clock\", ',', 'on', 'Thursday', 'morning', ',', 'the', 'great', 'Arthur', 'did', \"n't\", 'feel', 'VERY', 'good', '.']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "# La function word_tokenize() prend la phrase comme argument.\n",
        "words = nltk.word_tokenize(sentence)\n",
        "print(words)\n",
        "len(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color = 'E3A440'>*2. Analyse morphosyntaxique*</font>\n",
        "\n",
        "Après avoir identifé tous les mots, il est possible de analyser leur rôle morphosyntaxique, à des fins d'analyse et/ou filtrage. "
      ],
      "metadata": {
        "id": "CvM6JKj1hLX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # La function word_tokenize() prend la liste de mots comme argument.\n",
        " words_pos = nltk.pos_tag(words, tagset='universal')\n",
        " print(words_pos)\n",
        " len(words_pos)"
      ],
      "metadata": {
        "id": "qy06di1ygtmF",
        "outputId": "5c9f94a6-a98a-4c41-e85f-f9ce69b33934",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('At', 'ADP'), ('eight', 'NUM'), (\"o'clock\", 'NOUN'), (',', '.'), ('on', 'ADP'), ('Thursday', 'NOUN'), ('morning', 'NOUN'), (',', '.'), ('the', 'DET'), ('great', 'ADJ'), ('Arthur', 'NOUN'), ('did', 'VERB'), (\"n't\", 'ADV'), ('feel', 'VERB'), ('VERY', 'ADV'), ('good', 'ADJ'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4f_8mvLFrOE"
      },
      "source": [
        "#### <font color = 'E3A440'>*3. Retirer la ponctuation*</font>\n",
        "\n",
        "Une autre opération consiste à retirer la ponctuation. Ce type de filtrage reduit le nombre de sugne graphique qui participent le moin à la constructuion de la sémantique de la prhase. \n",
        "Dans certain contexte, comme en stylometrie, ce processus est appliquée avec de terchniques plus sofistiquées. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0P7SkM5RFrOF",
        "outputId": "22b07907-c61c-4aab-e438-8ea980eec507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('At', 'ADP'), ('eight', 'NUM'), ('on', 'ADP'), ('Thursday', 'NOUN'), ('morning', 'NOUN'), ('the', 'DET'), ('great', 'ADJ'), ('Arthur', 'NOUN'), ('did', 'VERB'), ('feel', 'VERB'), ('VERY', 'ADV'), ('good', 'ADJ')]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "# La ligne de code suivant itére sur chaque signe graphique et retient ceux qui contiennet de caracteres alphanumérique.\n",
        "words_pos = [(w, pos) for w, pos in words_pos if w.isalnum()]\n",
        "print(words_pos)\n",
        "len(words_pos)\n",
        "# Ikl est possible aussi d'utiilser le résultat de l'analyse morphosyntaxique pour éliminer la ponctuaction\n",
        "# words_pos = [(w, pos) for w, pos in words_pos if pos != '.']\n",
        "# print(words_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwcORV3vFrOG"
      },
      "source": [
        "#### <font color = 'E3A440'>*4. Convertir chaque caractère en minuscule*</font>\n",
        "\n",
        "Cette étape constitue une première opéraiton de normalisation des mots et leur réduction à une forme graphique unique. Ce genre d'étape permet de regrouper chaque occurence d'un mot sous une seule forme."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8tdwmniFrOG",
        "outputId": "952880ea-661d-429e-856a-a96fcaf9b3fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('at', 'ADP'), ('eight', 'NUM'), ('on', 'ADP'), ('thursday', 'NOUN'), ('morning', 'NOUN'), ('the', 'DET'), ('great', 'ADJ'), ('arthur', 'NOUN'), ('did', 'VERB'), ('feel', 'VERB'), ('very', 'ADV'), ('good', 'ADJ')]\n"
          ]
        }
      ],
      "source": [
        "# La ligne de code suivant itére sur chaque signe graphique et le transforme en minuscule.\n",
        "words_pos = [(w.lower(), pos) for w, pos in words_pos]\n",
        "print(words_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd1RxnCnFrOH"
      },
      "source": [
        "#### <font color = 'E3A440'>*5. retirer les stopwrods (mots vides)*</font>\n",
        "\n",
        "Une autre opération de filtrage constitue dan l'élimination de mots fonctionnels. Cette liste de mots contient tout les connecteurs de phrase, comme \"et\", \"mais\", \"toutefois\" et de mots avec faible valeur sémantique, comem les verbes modaux. \n",
        "Comme d'autres opéraiton de filtrage, l'enjeuy est celui de nettoyer le plus possible le vocabulaire et de reduyire toutes les occurrences d'un mot sous une forme graphique unique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icU72m20FrOH",
        "outputId": "878b9eec-579c-4564-cd88-fb7d0ea46c2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ],
      "source": [
        "# Nous importons la liste de stopword en anglais\n",
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words(\"english\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ws2STtGKFrOH",
        "outputId": "732171f0-993e-489b-ac3d-3c86cd4d1ae3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('eight', 'NUM'), ('thursday', 'NOUN'), ('morning', 'NOUN'), ('great', 'ADJ'), ('arthur', 'NOUN'), ('feel', 'VERB'), ('good', 'ADJ')]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "# La ligne de code suivant itére sur chaque signe graphique et garde ce qui n sont pas dans la liste de stopword.\n",
        "words_pos = [(w, pos) for w, pos in words_pos if w not in stopwords.words(\"english\")]\n",
        "print(words_pos)\n",
        "len(words_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8acb4VWFrOI"
      },
      "source": [
        "#### <font color = 'E3A440'>*6. Rammener les mots à leur racine*</font> \n",
        "\n",
        "En suivant le même objectif, nous retirons le suffixe morphologique des mots, ce qui augmente le niveau de réduction de chaque occurrence d'un mot à une unique forme graphique.\n",
        "\n",
        "Ils existent deux méthode fondamentales: la racinisaiton et la lemmatisation.\n",
        "La première reduit les occurence à une racine qui est inférée au moyen de plusieur techniques, l'autre est la réduciton de l'occurrence à son lemme. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_mhuA7zFrOI",
        "outputId": "1cd34d7a-fb54-43dd-a6d8-8cd5d73f3a95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('eight', 'NUM'), ('thursday', 'NOUN'), ('morn', 'NOUN'), ('great', 'ADJ'), ('arthur', 'NOUN'), ('feel', 'VERB'), ('good', 'ADJ')]\n"
          ]
        }
      ],
      "source": [
        "# Racinisation: technique Porter\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmed_pos = [(PorterStemmer().stem(w), pos) for w, pos in words_pos]\n",
        "print(stemmed_pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnsV3pSZFrOI",
        "outputId": "d58d904e-7411-40df-e382-f8dc2cabcf42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('eight', 'NUM'), ('thursday', 'NOUN'), ('morn', 'NOUN'), ('gre', 'ADJ'), ('arth', 'NOUN'), ('feel', 'VERB'), ('good', 'ADJ')]\n"
          ]
        }
      ],
      "source": [
        "# Racinisation: technique Lancaster\n",
        "from nltk.stem import LancasterStemmer\n",
        "stemmed_pos = [(LancasterStemmer().stem(w), pos) for w, pos in words_pos]\n",
        "print(stemmed_pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN7uAtUgFrOJ",
        "outputId": "7bc34196-d0cc-4ebc-e782-c72ae5939862"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('eight', 'NUM'), ('thursday', 'NOUN'), ('morning', 'NOUN'), ('great', 'ADJ'), ('arthur', 'NOUN'), ('feel', 'VERB'), ('good', 'ADJ')]\n"
          ]
        }
      ],
      "source": [
        "# Lemmatisaiton: utilisant le thesaurus wordnet\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lemmed_pos = [(WordNetLemmatizer().lemmatize(w), pos) for w, pos in words_pos]\n",
        "print(lemmed_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color = 'E3A440'>*7. Filtrage selon le rôle morphosyntaxique*</font>"
      ],
      "metadata": {
        "id": "n-48nGApkl_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retenir seulement les noms et les adjectifs\n",
        "lemmed_pos = [(w, pos) for w, pos in words_pos if pos in ['NOUN','ADJ']]\n",
        "print(lemmed_pos)"
      ],
      "metadata": {
        "id": "FrOSEBCzk08r",
        "outputId": "4547b072-5233-4776-f3ce-569839944c0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('thursday', 'NOUN'), ('morning', 'NOUN'), ('great', 'ADJ'), ('arthur', 'NOUN'), ('good', 'ADJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ges85oatFrOJ"
      },
      "source": [
        "# Analyse d'un texte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOzWtcugFrOK",
        "outputId": "be6cdfa8-7879-490b-993d-ebf95f8f98fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "175"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "text = \"\"\"At eight o'clock, on Thursday morning, the great Arthur didn't feel VERY good.\n",
        "          The following morning, at nine, Arthur felt better.\n",
        "          A dog run in the street.\"\"\"\n",
        "len(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lihe1FNjFrOK",
        "outputId": "73b92441-6856-4251-9773-d6e536e2953f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['At', 'eight', \"o'clock\", ',', 'on', 'Thursday', 'morning', ',', 'the', 'great', 'Arthur', 'did', \"n't\", 'feel', 'VERY', 'good', '.', 'The', 'following', 'morning', ',', 'at', 'nine', ',', 'Arthur', 'felt', 'better', '.', 'A', 'dog', 'run', 'in', 'the', 'street', '.']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "35"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "words_in_text = nltk.word_tokenize(text)\n",
        "print(words_in_text)\n",
        "len(words_in_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rtngk2rFrOL",
        "outputId": "c7709b91-b479-40d0-b48c-bdc61672a3d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['At', 'eight', 'on', 'Thursday', 'morning', 'the', 'great', 'Arthur', 'did', 'feel', 'VERY', 'good', 'The', 'following', 'morning', 'at', 'nine', 'Arthur', 'felt', 'better', 'A', 'dog', 'run', 'in', 'the', 'street']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ponctuation\n",
        "words_in_text = [w for w in words_in_text if w.isalnum()]\n",
        "print(words_in_text)\n",
        "len(words_in_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opWZu3FMFrOL",
        "outputId": "77053df6-aaa1-4aad-ed17-8ef37ba189cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FreqDist({'morning': 2, 'the': 2, 'Arthur': 2, 'At': 1, 'eight': 1, 'on': 1, 'Thursday': 1, 'great': 1, 'did': 1, 'feel': 1, ...})"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "freqs_in_text = nltk.FreqDist(words_in_text)\n",
        "freqs_in_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9MZT3hfFrOL",
        "outputId": "6af17df3-4e20-47df-f3f8-c7bd8614ea3e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FreqDist({'the': 3, 'at': 2, 'morning': 2, 'arthur': 2, 'eight': 1, 'on': 1, 'thursday': 1, 'great': 1, 'did': 1, 'feel': 1, ...})"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# majuscules : The/the\n",
        "words_in_text = [w.lower() for w in words_in_text]\n",
        "freqs_in_text = nltk.FreqDist(words_in_text)\n",
        "freqs_in_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdG6tJu4FrOM"
      },
      "source": [
        "### Couper les Phrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7U5XH19FrOM",
        "outputId": "2ad99740-01ec-478b-9a52-18cb5447955c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"At eight o'clock, on Thursday morning, the great Arthur didn't feel VERY good.\", 'The following morning, at nine, Arthur felt better.', 'A dog run in the street.']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "sentence = nltk.sent_tokenize(text)\n",
        "print(sentence)\n",
        "len(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwBJv2brFrOM",
        "outputId": "da2755a0-9d5b-4378-d73e-9e29490d03c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At eight o'clock, on Thursday morning, the great Arthur didn't feel VERY good.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "78"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "print(sentence[0])\n",
        "len(sentence[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEZ-cx5jFrON",
        "outputId": "31f7b4ee-329d-43ec-aa19-8bc8121c6450"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['at', 'eight', \"o'clock\", ',', 'on', 'thursday', 'morning', ',', 'the', 'great', 'arthur', 'did', \"n't\", 'feel', 'very', 'good', '.'], ['the', 'following', 'morning', ',', 'at', 'nine', ',', 'arthur', 'felt', 'better', '.'], ['a', 'dog', 'run', 'in', 'the', 'street', '.']]\n"
          ]
        }
      ],
      "source": [
        "# majuscules / minuscules\n",
        "sentence = [w.lower() for w in sentence]\n",
        "# mots par phrase\n",
        "words_in_sentence = [nltk.word_tokenize(s) for s in sentence]\n",
        "print(words_in_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_NmX11TFrON",
        "outputId": "553999a4-8853-4df3-ec6d-b70b7cf0e41b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['at', 'eight', 'on', 'thursday', 'morning', 'the', 'great', 'arthur', 'did', 'feel', 'very', 'good'], ['the', 'following', 'morning', 'at', 'nine', 'arthur', 'felt', 'better'], ['a', 'dog', 'run', 'in', 'the', 'street']]\n"
          ]
        }
      ],
      "source": [
        "# ponctuation\n",
        "words_in_sentence = [[w for w in words if w.isalnum()] for words in words_in_sentence]\n",
        "print(words_in_sentence)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPoo50WwFrON",
        "outputId": "c8e451cd-e426-49c0-8d15-fd2013f2086f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['eight', 'thursday', 'morning', 'great', 'arthur', 'feel', 'good'], ['following', 'morning', 'nine', 'arthur', 'felt', 'better'], ['dog', 'run', 'street']]\n"
          ]
        }
      ],
      "source": [
        "# stop words\n",
        "words_in_sentence = [[w for w in words if w not in stopwords.words(\"english\")] for words in words_in_sentence]\n",
        "print(words_in_sentence) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oa25cM6WFrOO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEayD1H8FrOO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6KsXCtlFrOO"
      },
      "source": [
        "on peut alors :\n",
        " - analyser la frequence d'apparition de chaque terme dans chaque phrase,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeqyYwMNFrOO",
        "outputId": "b74de7fb-52f8-4114-fb4e-6dbd149ac91b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[FreqDist({'eight': 1, 'thursday': 1, 'morning': 1, 'great': 1, 'arthur': 1, 'feel': 1, 'good': 1}), FreqDist({'following': 1, 'morning': 1, 'nine': 1, 'arthur': 1, 'felt': 1, 'better': 1}), FreqDist({'dog': 1, 'run': 1, 'street': 1})]\n"
          ]
        }
      ],
      "source": [
        "freqs_in_sentence = [nltk.FreqDist(s) for s in words_in_sentence]\n",
        "print(freqs_in_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZP3FPkDFrOO"
      },
      "outputs": [],
      "source": [
        "stats sur les mots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTsNcamOFrOO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeUsOnw5FrOP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTO0XGsCFrOP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uo9gObvxFrOP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26AJgu6qFrOP"
      },
      "source": [
        "# En francais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hK0x2tQ0FrOQ"
      },
      "outputs": [],
      "source": [
        "texte=\"Bonjour, les gentils étudiants. Comment allez vous ?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UNTITcjFrOQ"
      },
      "outputs": [],
      "source": [
        "phrases=nltk.sent_tokenize(texte,\"french\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMe5lGqFFrOQ",
        "outputId": "3b155081-4b5f-496c-977c-ba8d356152fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Bonjour, les gentils étudiants.', 'Comment allez vous ?']\n"
          ]
        }
      ],
      "source": [
        "print(phrases)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZRr_T5RFrOQ",
        "outputId": "7554fe0e-e280-4ddc-ea28-5fe83420d18b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Bonjour', ',', 'les', 'gentils', 'étudiants', '.', 'Comment', 'allez', 'vous', '?']\n"
          ]
        }
      ],
      "source": [
        "mots=nltk.word_tokenize(texte,\"french\")\n",
        "print(mots)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCWVbHowFrOR",
        "outputId": "96fdc2bc-7ffb-43fd-c325-3b25d4a7a625"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Bonjour', 'les', 'gentils', 'étudiants', 'Comment', 'allez', 'vous']\n"
          ]
        }
      ],
      "source": [
        "mots= [w for w in mots if w.isalnum()] # garde seulement if contain alphanumeric characters\n",
        "print(mots)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-V_WZaFFrOR",
        "outputId": "81b0d1ee-e052-4dcc-a2ea-132058cfcfc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['bonjour', 'les', 'gentils', 'étudiants', 'comment', 'allez', 'vous']\n"
          ]
        }
      ],
      "source": [
        "mots = [w.lower() for w in mots]\n",
        "print(mots)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HBMGkJnFrOR",
        "outputId": "b65fa3c1-ac3a-47e4-ea51-79e7d85cdb37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['bonjour', 'gentils', 'étudiants', 'comment', 'allez']\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "mots = [w for w in mots if w not in stopwords.words(\"french\")]\n",
        "print(mots)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfC8nLbnFrOR",
        "outputId": "f6faa0a0-6ed6-4b76-b7bc-c44e40dede4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['bonjour', 'gentil', 'étudi', 'comment', 'allez']\n"
          ]
        }
      ],
      "source": [
        "#racine\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer('french')\n",
        "\n",
        "stemmed = [stemmer.stem(w) for w in mots]\n",
        "print(stemmed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HibvBLokFrOS"
      },
      "source": [
        "Actuellement, pas de tag en francais dans NLTK, mais il y en a un ici:\n",
        "https://nlp.stanford.edu/software/tagger.shtml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGrKkm7fFrOS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1KyUSDJFrOS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HNgfEAuFrOS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMIkkaKKFrOS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bbFVZWJFrOT"
      },
      "source": [
        "# 2ème 45 minutes apprentissage non supervisé et supervisé"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwzrQgDWFrOT"
      },
      "outputs": [],
      "source": [
        "vrai ensemble de textes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJr_wQOrFrOT"
      },
      "source": [
        "# Data mining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VgpJ5zdFrOT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmgF8MdmFrOT"
      },
      "outputs": [],
      "source": [
        "#pip install wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuV-v97CFrOU"
      },
      "outputs": [],
      "source": [
        "#from wordcloud import WordCloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2bXiPvNFrOU"
      },
      "outputs": [],
      "source": [
        "#from wordcloud import WordCloud\n",
        "\n",
        "# Create and generate a word cloud image:\n",
        "#wordcloud = WordCloud().generate_from_frequencies(freqs_in_sentence)\n",
        "# Display the generated image:\n",
        "#plt.imshow(wordcloud, interpolation='bilinear')\n",
        "#plt.axis(\"off\")\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHzR6H7HFrOU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XgCUD6tFrOU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJ_CnOSEFrOU"
      },
      "outputs": [],
      "source": [
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "te = TransactionEncoder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ER6o9l8CFrOV",
        "outputId": "5ec0ef51-703c-4ff1-e02f-0712cced7650"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3, 14)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1],\n",
              "       [1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0],\n",
              "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0]])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "te_ary = te.fit(words_in_sentence).transform(words_in_sentence)\n",
        "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
        "\n",
        "print(te_ary.shape)\n",
        "te_ary.astype(\"int\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_0YANHMFrOV"
      },
      "outputs": [],
      "source": [
        "# tf_idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtyrfOZCFrOV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLZEy-NOFrOW"
      },
      "source": [
        "on peut alors :\n",
        " \n",
        " - évaluer la similitude des phrases,\n",
        " - faire des regroupements de textes,\n",
        " - faire de la prédiction de contenu, de mots clefs, de thèmes ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMXoiOdvFrOX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJGZL2qlFrOX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DI74a6OFrOX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aMaPA4_FrOX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0fW8GCBFrOX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBEi4xHbFrOX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTCWZZZ9FrOX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xh-Up_RcFrOX"
      },
      "source": [
        "# non suppervisé"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e13cSzVqFrOX"
      },
      "source": [
        "## distance entre mots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a9F_YlsFrOX"
      },
      "source": [
        "## distance entre phrases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqB-sa5OFrOX"
      },
      "source": [
        "## classifier des textes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToOJX5jVFrOY"
      },
      "source": [
        "## regles d association"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOn5FY4UFrOY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrBhMssSFrOY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEM6NvV8FrOZ"
      },
      "source": [
        "# Supervisé"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvGRxwgtFrOZ"
      },
      "source": [
        "## classification de texte"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QymfV4S_FrOZ"
      },
      "source": [
        "## Sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pInoj9yrFrOZ",
        "outputId": "01aa534b-69e7-4752-932e-890f9dea86e0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'neg': 0.0, 'neu': 0.295, 'pos': 0.705, 'compound': 0.8012}"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "sia.polarity_scores(\"Wow, NLTK is really powerful!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_t9W_eAFrOa",
        "outputId": "54a5f4ee-8015-4216-f296-0a2fd1d35a3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'neg': 0.0, 'neu': 0.275, 'pos': 0.725, 'compound': 0.8367}"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sia.polarity_scores(\"Wow, NLTK is REALLY powerful!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jAgx5YgFrOa",
        "outputId": "f06a4bce-dc8e-44a7-c6e7-b2812a8f6ab4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'neg': 0.0, 'neu': 0.488, 'pos': 0.512, 'compound': 0.484}"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sia.polarity_scores(\"NLTK is not bad!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpGJWGRWFrOa",
        "outputId": "51c6e8a9-5892-41a3-d73a-d5e6271914ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'neg': 0.655, 'neu': 0.345, 'pos': 0.0, 'compound': -0.5848}"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sia.polarity_scores(\"NLTK is bad!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRutULJCFrOb",
        "outputId": "5ec2ee0a-9987-49e3-81cd-f7bad5bb92ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'neg': 0.668, 'neu': 0.332, 'pos': 0.0, 'compound': -0.6155}"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sia.polarity_scores(\"NLTK is AWFUL!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51pYWFi5FrOc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4jaFOXOFrOc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TltwRrRHFrOc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJPUWL8WFrOc"
      },
      "source": [
        "### Structure d'une phrase (retirer pour le séminaire ?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ml7_-5XQFrOc"
      },
      "outputs": [],
      "source": [
        "sentence = \"\"\"At eight o'clock, on Thursday morning, the great Arthur didn't feel VERY good.\"\"\"\n",
        "\n",
        "from nltk import pos_tag, word_tokenize, RegexpParser\n",
        "\n",
        "# Find all parts of speech in above sentence\n",
        "tagged = pos_tag(word_tokenize(sentence))\n",
        "\n",
        "grammar = r\"\"\"\n",
        "  NP: {<DT|JJ|NN.*>+} # Chunk sequences of DT, JJ, NN\n",
        "  PP: {<IN><NP>} # Chunk prepositions followed by NP\n",
        "  VP: {<VB.*><NP|PP|CLAUSE>+$} # Chunk verbs and their arguments\n",
        "\"\"\"\n",
        "\n",
        "chunker = RegexpParser(grammar)\n",
        "\n",
        "output = chunker.parse(tagged)\n",
        "\n",
        "output.draw()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}