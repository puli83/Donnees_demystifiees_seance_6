{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWNgJtQ3FrN8"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "<font size='10' color = 'E3A440'>**Mégadonnées et techniques avancées démystifiées**</font>\n",
        "=======\n",
        "<font color = 'E3A440'>*Nouvelles méthodes d’analyse et leur implication quant à la gestion des mégadonnées en SSH (partie 1)*</font>\n",
        "=============\n",
        "\n",
        "\n",
        "Cet atelier s’inscrit dans le cadre de la formation [Mégadonnées et techniques avancées démystifiées](https://www.4point0.ca/2022/08/22/formation-megadonnees-demystifiees/) (séance 6).\n",
        "\n",
        "Les sciences humaines et sociales sont souvent confrontées à l’analyse de données non structurées, comme le texte. Après avoir préparé les données, plusieurs techniques d’analyse venant de l’apprentissage automatique peuvent être utilisées. Pendant cet atelier, les participants seront initiés aux méthodes supervisées et non supervisées à des buts d’analyse avec Python.\n",
        "\n",
        "Note : Cet atelier se poursuit lors d’une 2e séance le 10 novembre.\n",
        "\n",
        "Structure de l'atelier :\n",
        "1. Presentation of sections 1 and 2 in a plenary mode (20 minutes)\n",
        "2. Individual work on section 3 (20 minutes)\n",
        "3. Group work on section 4 (60 minutes)\n",
        "4. Plenary session with groups presentations (20 minutes)\n",
        "\n",
        "Ce tutoriel ne peut pas être consideré exaustif .... \n",
        "\n",
        "### Authors: \n",
        "- Bruno Agard <bruno.agard@polymtl.ca>\n",
        "- Davide Pulizzotto <davide.pulizzotto@polymtl.ca>\n",
        "\n",
        "### Table of Contents\n",
        "Bruno Agard\n",
        "\n",
        "Département de Mathématiques et de génie Industriel\n",
        "\n",
        "École Polytechnique de Montréal"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = 'E3A440'>0. Préparation environnement </font>"
      ],
      "metadata": {
        "id": "HpAX32-4Kj8U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jddhTL7EFrN_",
        "outputId": "cfddaec0-adf0-452e-bdf8-3f1cb353aacb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Donnees_demystifiees_seance_6'...\n",
            "remote: Enumerating objects: 75, done.\u001b[K\n",
            "remote: Counting objects: 100% (75/75), done.\u001b[K\n",
            "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
            "remote: Total 75 (delta 26), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (75/75), done.\n"
          ]
        }
      ],
      "source": [
        "# Downloading of data from the GitHub project\n",
        "!rm -rf Donnees_demystifiees_seance_6/\n",
        "!git clone https://github.com/puli83/Donnees_demystifiees_seance_6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJLRFWa6FrOA",
        "outputId": "a401977d-9bd1-4c2f-823d-5b111cd9c6cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Import modules\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwFKNpPQFrN-"
      },
      "source": [
        "# <font color = 'E3A440'>1. *Préparation des données textulles*</font>\n",
        "\n",
        "L'analyse de données textuelles implique la transformation d'un texte en un objet mathematique qui peut être utilisé par des algorithmes et des modèles statistique. Cette étape est importante car permet de **structurer** des données non structurées, comme le texte.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDS10IXPFrOB"
      },
      "source": [
        "###  <font color = 'E3A440'>**1.1 Étapes fondamentales du prétraitement**</font>\n",
        "\n",
        "Prenons une phrase pour decortiquer les passages que nous pemrettent de la transformer en information structurée."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "h9AVaykcFrOB"
      },
      "outputs": [],
      "source": [
        "sentence = \"\"\"At eight o'clock, on Thursday morning, the great Arthur didn't feel VERY good.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpFqEU4iFrOC",
        "outputId": "0b5b6236-f673-4539-a805-8cc8ff53f4a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "78"
            ]
          },
          "metadata": {},
          "execution_count": 190
        }
      ],
      "source": [
        "len(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWQNjvKIFrOD"
      },
      "source": [
        "#### <font color = 'E3A440'>*a. Tokenisation*</font>\n",
        "\n",
        "Cette étape consiste à couper la phrase en unités linguistiques élémantaire et dotées de sens, ce qui est gnralement appelé le \"mot\".\n",
        "\n",
        "Dans le module `nltk`, il existe une fonction qui permet cette opération, soit `word_tokenize()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzF3fiUWFrOE",
        "outputId": "83467031-c397-4df7-f066-3a858ffbe5c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['At', 'eight', \"o'clock\", ',', 'on', 'Thursday', 'morning', ',', 'the', 'great', 'Arthur', 'did', \"n't\", 'feel', 'VERY', 'good', '.']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 191
        }
      ],
      "source": [
        "# La function word_tokenize() prend la phrase comme argument.\n",
        "words = nltk.word_tokenize(sentence)\n",
        "print(words)\n",
        "len(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color = 'E3A440'>*b. Analyse morphosyntaxique*</font>\n",
        "\n",
        "Après avoir identifé tous les mots, il est possible de analyser leur rôle morphosyntaxique, à des fins d'analyse et/ou filtrage. "
      ],
      "metadata": {
        "id": "CvM6JKj1hLX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # La function word_tokenize() prend la liste de mots comme argument.\n",
        "words_pos = nltk.pos_tag(words, tagset='universal')\n",
        "print(words_pos)\n",
        "len(words_pos)"
      ],
      "metadata": {
        "id": "qy06di1ygtmF",
        "outputId": "dfe9799e-2b64-4e0d-81f4-cd6aa94357bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('At', 'ADP'), ('eight', 'NUM'), (\"o'clock\", 'NOUN'), (',', '.'), ('on', 'ADP'), ('Thursday', 'NOUN'), ('morning', 'NOUN'), (',', '.'), ('the', 'DET'), ('great', 'ADJ'), ('Arthur', 'NOUN'), ('did', 'VERB'), (\"n't\", 'ADV'), ('feel', 'VERB'), ('VERY', 'ADV'), ('good', 'ADJ'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici la liste de possible POS tags:"
      ],
      "metadata": {
        "id": "6cECT-Gp2obD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **POS** | **DESCRIPTION**           | **EXAMPLES**                                      |\n",
        "| ------- | ------------------------- | ------------------------------------------------- |\n",
        "| ADJ     | adjective                 | big, old, green, incomprehensible, first      |\n",
        "| ADP     | adposition                | in, to, during                                |\n",
        "| ADV     | adverb                    | very, tomorrow, down, where, there            |\n",
        "| AUX     | auxiliary                 | is, has (done), will (do), should (do)        |\n",
        "| CONJ    | conjunction               | and, or, but                                  |\n",
        "| CCONJ   | coordinating conjunction  | and, or, but                                  |\n",
        "| DET     | determiner                | a, an, the                                    |\n",
        "| INTJ    | interjection              | psst, ouch, bravo, hello                      |\n",
        "| NOUN    | noun                      | girl, cat, tree, air, beauty                  |\n",
        "| NUM     | numeral                   | 1, 2017, one, seventy-seven, IV, MMXIV        |\n",
        "| PART    | particle                  | ’s, not                                      |\n",
        "| PRON    | pronoun                   | I, you, he, she, myself, themselves, somebody |\n",
        "| PROPN   | proper noun               | Mary, John, London, NATO, HBO                 |\n",
        "| PUNCT   | punctuation               | ., (, ), ?                                    |\n",
        "| SCONJ   | subordinating conjunction | if, while, that                               |\n",
        "| SYM     | symbol                    | $, %, §, ©, +, −, ×, ÷, =, :)               |\n",
        "| VERB    | verb                      | run, runs, running, eat, ate, eating          |\n",
        "| X       | other                     | sfpksdpsxmsa                                  |\n",
        "| SPACE   | space                     |                                                   |\n"
      ],
      "metadata": {
        "id": "kUDRUVYe2kLi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4f_8mvLFrOE"
      },
      "source": [
        "#### <font color = 'E3A440'>*c. Retirer la ponctuation*</font>\n",
        "\n",
        "Une autre opération consiste à retirer la ponctuation. Ce type de filtrage reduit le nombre de sugne graphique qui participent le moin à la constructuion de la sémantique de la prhase. \n",
        "Dans certain contexte, comme en stylometrie, ce processus est appliquée avec de terchniques plus sofistiquées. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0P7SkM5RFrOF",
        "outputId": "f78970a7-3c61-4058-a96c-f8c33fb2f526"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('At', 'ADP'), ('eight', 'NUM'), ('on', 'ADP'), ('Thursday', 'NOUN'), ('morning', 'NOUN'), ('the', 'DET'), ('great', 'ADJ'), ('Arthur', 'NOUN'), ('did', 'VERB'), ('feel', 'VERB'), ('VERY', 'ADV'), ('good', 'ADJ')]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# La ligne de code suivant itére sur chaque signe graphique et retient ceux qui contiennet de caracteres alphanumérique.\n",
        "words_pos = [(w, pos) for w, pos in words_pos if w.isalnum()]\n",
        "print(words_pos)\n",
        "len(words_pos)\n",
        "# Ikl est possible aussi d'utiilser le résultat de l'analyse morphosyntaxique pour éliminer la ponctuaction\n",
        "# words_pos = [(w, pos) for w, pos in words_pos if pos != '.']\n",
        "# print(words_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwcORV3vFrOG"
      },
      "source": [
        "#### <font color = 'E3A440'>*d. Convertir chaque caractère en minuscule*</font>\n",
        "\n",
        "Cette étape constitue une première opéraiton de normalisation des mots et leur réduction à une forme graphique unique. Ce genre d'étape permet de regrouper chaque occurence d'un mot sous une seule forme."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8tdwmniFrOG",
        "outputId": "350309c6-67f9-4ac5-8fdd-acee6323feae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('at', 'ADP'), ('eight', 'NUM'), ('on', 'ADP'), ('thursday', 'NOUN'), ('morning', 'NOUN'), ('the', 'DET'), ('great', 'ADJ'), ('arthur', 'NOUN'), ('did', 'VERB'), ('feel', 'VERB'), ('very', 'ADV'), ('good', 'ADJ')]\n"
          ]
        }
      ],
      "source": [
        "# La ligne de code suivant itére sur chaque signe graphique et le transforme en minuscule.\n",
        "words_pos = [(w.lower(), pos) for w, pos in words_pos]\n",
        "print(words_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd1RxnCnFrOH"
      },
      "source": [
        "#### <font color = 'E3A440'>*e. Retirer les stopwords (mots vides)*</font>\n",
        "\n",
        "Une autre opération de filtrage constitue dan l'élimination de mots fonctionnels. Cette liste de mots contient tout les connecteurs de phrase, comme \"et\", \"mais\", \"toutefois\" et de mots avec faible valeur sémantique, comem les verbes modaux. \n",
        "Comme d'autres opéraiton de filtrage, l'enjeuy est celui de nettoyer le plus possible le vocabulaire et de reduyire toutes les occurrences d'un mot sous une forme graphique unique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icU72m20FrOH",
        "outputId": "c3162af7-e04b-42d6-a353-be85c7df5ee7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ],
      "source": [
        "# Nous importons la liste de stopword en anglais\n",
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words(\"english\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ws2STtGKFrOH",
        "outputId": "c8ea2f11-c8ff-40f0-e026-39c71051620d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('eight', 'NUM'), ('thursday', 'NOUN'), ('morning', 'NOUN'), ('great', 'ADJ'), ('arthur', 'NOUN'), ('feel', 'VERB'), ('good', 'ADJ')]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# La ligne de code suivant itére sur chaque signe graphique et garde ce qui n sont pas dans la liste de stopword.\n",
        "words_pos = [(w, pos) for w, pos in words_pos if w not in stopwords.words(\"english\")]\n",
        "print(words_pos)\n",
        "len(words_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8acb4VWFrOI"
      },
      "source": [
        "#### <font color = 'E3A440'>*f. Rammener les mots à leur racine*</font> \n",
        "\n",
        "En suivant le même objectif, nous retirons le suffixe morphologique des mots, ce qui augmente le niveau de réduction de chaque occurrence d'un mot à une unique forme graphique.\n",
        "\n",
        "Ils existent deux méthode fondamentales: la racinisaiton et la lemmatisation.\n",
        "La première reduit les occurence à une racine qui est inférée au moyen de plusieur techniques, l'autre est la réduciton de l'occurrence à son lemme. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_mhuA7zFrOI",
        "outputId": "b6712ef1-a875-4f41-8dab-24b64a094cae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('eight', 'NUM'), ('thursday', 'NOUN'), ('morn', 'NOUN'), ('great', 'ADJ'), ('arthur', 'NOUN'), ('feel', 'VERB'), ('good', 'ADJ')]\n"
          ]
        }
      ],
      "source": [
        "# Racinisation: technique Porter\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmed_pos = [(PorterStemmer().stem(w), pos) for w, pos in words_pos]\n",
        "print(stemmed_pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnsV3pSZFrOI",
        "outputId": "1a559c9a-07d5-4d00-a811-3c5c91cb8179"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('eight', 'NUM'), ('thursday', 'NOUN'), ('morn', 'NOUN'), ('gre', 'ADJ'), ('arth', 'NOUN'), ('feel', 'VERB'), ('good', 'ADJ')]\n"
          ]
        }
      ],
      "source": [
        "# Racinisation: technique Lancaster\n",
        "from nltk.stem import LancasterStemmer\n",
        "stemmed_pos = [(LancasterStemmer().stem(w), pos) for w, pos in words_pos]\n",
        "print(stemmed_pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN7uAtUgFrOJ",
        "outputId": "c228556d-e2ee-4461-b5a3-f56152f406a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('eight', 'NUM'), ('thursday', 'NOUN'), ('morning', 'NOUN'), ('great', 'ADJ'), ('arthur', 'NOUN'), ('feel', 'VERB'), ('good', 'ADJ')]\n"
          ]
        }
      ],
      "source": [
        "# Lemmatisaiton: utilisant le thesaurus wordnet\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lemmed_pos = [(WordNetLemmatizer().lemmatize(w), pos) for w, pos in words_pos]\n",
        "print(lemmed_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color = 'E3A440'>*g. Filtrage selon le rôle morphosyntaxique*</font>\n",
        "\n",
        "Le filtrage des unités lexicales peut s'étendre jusqu'à l'élimination d'unités qui ne font pas partie d'une liste de rôles morphosyntaxique prédéfinie. "
      ],
      "metadata": {
        "id": "n-48nGApkl_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retenir seulement les noms et les adjectifs\n",
        "lemmed_pos = [(w, pos) for w, pos in words_pos if pos in ['NOUN','ADJ']]\n",
        "print(lemmed_pos)"
      ],
      "metadata": {
        "id": "FrOSEBCzk08r",
        "outputId": "c78437a4-b52c-4d74-ee25-ce811f5c54b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('thursday', 'NOUN'), ('morning', 'NOUN'), ('great', 'ADJ'), ('arthur', 'NOUN'), ('good', 'ADJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ges85oatFrOJ"
      },
      "source": [
        "## <font color = 'E3A440'>**1.2 Traitement d'un corpus**</font>\n",
        "\n",
        "Le prétraitement d'un corpus de recherche peut mettre en place plusieurs autres étapes. La plus importante est la **segmentation**. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'>*1. Découpage du texte*</font>\n",
        "\n",
        "Tout dépendant de l'objectif de l'analyse, le texte peut être découpé en plusieurs fragments, chacun desquels peut être un document, un paragraphe, une concordance, un groupe de phrases, une phrase simple, un syntagme, etc.\n",
        "\n"
      ],
      "metadata": {
        "id": "GZqenN7Rcc2D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOzWtcugFrOK",
        "outputId": "77fa374c-1925-4548-ad95-5fc34db532d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "175"
            ]
          },
          "metadata": {},
          "execution_count": 261
        }
      ],
      "source": [
        "text = \"\"\"At eight o'clock, on Thursday morning, the great Arthur didn't feel VERY good.\n",
        "          The following morning, at nine, Arthur felt better.\n",
        "          A dog run in the street.\"\"\"\n",
        "len(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans le bloc de code suivant, nous faisons une segmentation par pharse."
      ],
      "metadata": {
        "id": "sy8Qow43dHKv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {
        "id": "Lihe1FNjFrOK",
        "outputId": "2401c458-62b3-4cec-eb3b-bd7e6d46d7eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"At eight o'clock, on Thursday morning, the great Arthur didn't feel VERY good.\", 'The following morning, at nine, Arthur felt better.', 'A dog run in the street.']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 265
        }
      ],
      "source": [
        "sentences = nltk.sent_tokenize(text)\n",
        "print(sentences)\n",
        "len(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'>*2. Annotation et nettoyage*</font>\n",
        "\n",
        "Les opérations précedentes d'annotation morphosyntaxique et de filtrage seront appliquées à chaque fragement du corpus qui a été créé.\n"
      ],
      "metadata": {
        "id": "gHi0hAxZdKx4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color = 'E3A440'>*a. Création d'une fonction*</font>\n",
        "\n",
        "Dans le code suivant, une fonction est créé pour englober toutes les opéraitons nécessaires pour l'annotaiton et le nettoyage."
      ],
      "metadata": {
        "id": "waMXvFxhgH0u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6rtngk2rFrOL"
      },
      "outputs": [],
      "source": [
        "# To run this function proprlely, you need to import modules needed\n",
        "def CleaningText(text_as_string, language = 'english', reduce = '', list_pos_to_keep = [], Stopwords_to_add = []):\n",
        "    from nltk.corpus import stopwords\n",
        "\n",
        "    words = nltk.word_tokenize(text_as_string)\n",
        "    words_pos = nltk.pos_tag(words, tagset='universal')\n",
        "    words_pos = [(w, pos) for w, pos in words_pos if w.isalnum()]\n",
        "    words_pos = [(w.lower(), pos) for w, pos in words_pos]\n",
        "    \n",
        "    if reduce == 'stem': \n",
        "        from nltk.stem.porter import PorterStemmer\n",
        "        reduced_words_pos = [(PorterStemmer().stem(w), pos) for w, pos in words_pos]\n",
        "        \n",
        "    elif reduce == 'lemma':\n",
        "        from nltk.stem.wordnet import WordNetLemmatizer\n",
        "        reduced_words_pos = [(WordNetLemmatizer().lemmatize(w), pos) for w, pos in words_pos]\n",
        "    else:\n",
        "        import warnings\n",
        "        reduced_words_pos = words_pos\n",
        "        warnings.warn(\"Warning : any reduction was made on words! Please, use \\\"reduce\\\" argument to chosse between 'stem' or  'lemma'\")\n",
        "    if list_pos_to_keep:\n",
        "        reduced_words_pos = [(w, pos) for w, pos in reduced_words_pos if pos in list_pos_to_keep]\n",
        "    else:\n",
        "        import warnings\n",
        "        warnings.warn(\"Warning : any POS filtering was made. Pleae, use \\\"list_pos_to_keep\\\" to create a list of POS tag to keep.\")\n",
        "    \n",
        "    list_stopwords = stopwords.words(language) + Stopwords_to_add\n",
        "    reduced_words_pos = [(w, pos) for w, pos in reduced_words_pos if w not in list_stopwords and len(w) > 1 ]\n",
        "    return reduced_words_pos\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color = 'E3A440'>*b. Application function nettoyage*</font>\n",
        "\n",
        "Maintenant, nous pouvons apliquer cette function à chaque fragment de texte."
      ],
      "metadata": {
        "id": "kWkIfcMwpqe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_sentences = [CleaningText(sent) for sent in sentences]\n",
        "print(cleaned_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVcF-aGGppjd",
        "outputId": "e4d0b9b4-687d-4f3d-83e0-6e0e89f75d89"
      },
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('eight', 'NUM'), ('thursday', 'NOUN'), ('morning', 'NOUN'), ('great', 'ADJ'), ('arthur', 'NOUN'), ('feel', 'VERB'), ('good', 'ADJ')], [('following', 'ADJ'), ('morning', 'NOUN'), ('nine', 'NUM'), ('arthur', 'NOUN'), ('felt', 'VERB'), ('better', 'ADV')], [('dog', 'NOUN'), ('run', 'NOUN'), ('street', 'NOUN')]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: UserWarning: Warning : any reduction was made on words! Please, use \"reduce\" argument to chosse between 'stem' or  'lemma'\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: UserWarning: Warning : any POS filtering was made. Pleae, use \"list_pos_to_keep\" to create a list of POS tag to keep.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_sentences = [CleaningText(sent, reduce = 'lemma', list_pos_to_keep = ['NOUN','ADJ','VERB']) for sent in sentences]\n",
        "print(cleaned_sentences)"
      ],
      "metadata": {
        "id": "vkpxyQfl2Wna",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8d5fdd2-8d18-48cd-9b87-c21a6f7d7ee0"
      },
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('thursday', 'NOUN'), ('morning', 'NOUN'), ('great', 'ADJ'), ('arthur', 'NOUN'), ('feel', 'VERB'), ('good', 'ADJ')], [('following', 'ADJ'), ('morning', 'NOUN'), ('arthur', 'NOUN'), ('felt', 'VERB')], [('dog', 'NOUN'), ('run', 'NOUN'), ('street', 'NOUN')]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color = 'E3A440'>*c. Fréquence mots*</font>\n",
        "\n",
        "Quelle est la fréquence des mots de notre corpus? Pour répondre, nous créons une liste de mots en retirant l'annotation morphosyntaxique."
      ],
      "metadata": {
        "id": "TBNxooILr8-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freqs_in_text = nltk.FreqDist([w for sent in cleaned_sentences for w, pos in sent ])\n",
        "freqs_in_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFPhg9Sm6Lbq",
        "outputId": "5d6d09af-5e35-4e22-ee22-329844d5cfb1"
      },
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'morning': 2, 'arthur': 2, 'thursday': 1, 'great': 1, 'feel': 1, 'good': 1, 'following': 1, 'felt': 1, 'dog': 1, 'run': 1, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 268
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'>*3. Vectorisation*</font>\n",
        "\n",
        "Généralement, pour utiliser le texte dans un contexte d'analyse de données ou d'apprentissage automatique, ce texte doit être transformé dans un objet mathématique approprié. \n",
        "Le modèle le plus simple et diffusé est le \"bags-of-words\", dans lequel chaque fragments de texte (ou chaque mot) est défini par un certain nombre d'unités lexicales qui le caractérise et converti dans une vecteur. Ce modèle appartien à la famille de modèles de la sémantique vectorielle.\n",
        "\n",
        "\n",
        "The main step in text mining is to convert unstructured textual data into a mathematical model to be used in statistical learning. Thus, we need to create a <font color='E3A440'>**Document-Term matrix**</font>, a matrix $n \\times w$, where $n$ is the number of text segments and $w$ is the number of textual features selected.The textual feature can have different nature. In the simplest model, these features correspond to the set of types that resume each token of the corpus. In other terms, $w$ is the number of features that characterize a segment of text. The matrix is generally represented as follows:  \n",
        " \n",
        "$$X = \\begin{bmatrix} \n",
        "x_{11} & x_{12} & \\ldots & x_{1w} \\\\\n",
        "\\vdots & \\vdots       &  \\ddots      & \\vdots \\\\ \n",
        "x_{n1} & x_{12} & \\ldots & x_{nw} \\\\\n",
        "\\end{bmatrix}\n",
        "$$ \n",
        " \n",
        "\\\\"
      ],
      "metadata": {
        "id": "m0YdoOA1stB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisation de l'objet\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def identity_tokenizer(text):\n",
        "    return text\n",
        "\n",
        "# Transforming the word in frequencies\n",
        "vectorized = CountVectorizer(lowercase = False, # Convert all characters to lowercase before tokenizing\n",
        "                             min_df = 1, # Ignore terms that have a document frequency strictly lower than the given threshold \n",
        "                             max_df = 10, # Ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words)\n",
        "                             stop_words = stopwords.words('english'), # Remove the list of words provided\n",
        "                             ngram_range = (1, 1), # Get the lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted\n",
        "                             tokenizer=identity_tokenizer) # Override the string tokenization step while preserving the preprocessing and n-grams generation steps"
      ],
      "metadata": {
        "id": "LpFPJrGI2Nsi"
      },
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilisation du \"vectorizer\" avec une liste de listes de mot (et non une liste de tuple de mots-pos)."
      ],
      "metadata": {
        "id": "G9uC3usB5dtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Liste de liste de mots:\n",
        "[[w for w, pos in sent] for sent in cleaned_sentences]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gy8_VTM67Lax",
        "outputId": "3a56eaad-cc2a-46b8-cb52-c4c6ede70169"
      },
      "execution_count": 270,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['thursday', 'morning', 'great', 'arthur', 'feel', 'good'],\n",
              " ['following', 'morning', 'arthur', 'felt'],\n",
              " ['dog', 'run', 'street']]"
            ]
          },
          "metadata": {},
          "execution_count": 270
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pplication du vectorizer\n",
        "freq_term_DTM = vectorized.fit_transform([[w for w, pos in sent] for sent in cleaned_sentences])\n",
        "print(pd.DataFrame(freq_term_DTM.todense(), columns =  [k for k, v in sorted(vectorized.vocabulary_.items(), key=lambda item: item[1])] ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8ydVdmC5c76",
        "outputId": "3d9da34e-2c50-4e27-e11e-277fa160a7c4"
      },
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   arthur  dog  feel  felt  following  good  great  morning  run  street  \\\n",
            "0       1    0     1     0          0     1      1        1    0       0   \n",
            "1       1    0     0     1          1     0      0        1    0       0   \n",
            "2       0    1     0     0          0     0      0        0    1       1   \n",
            "\n",
            "   thursday  \n",
            "0         1  \n",
            "1         0  \n",
            "2         0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "In the next chunk of code, we create another matrix using `scikit-learn` but this time we use the <font color = 'E3A440'>Tf-IDF</font> as weighting scheme of each word. The Tf-IDF is calculated as follows:\n",
        "\n",
        "$\n",
        "\\begin{align}\n",
        "&\\text{Let}\\ t = \\text{Term}\\\\\n",
        "&\\text{Let}\\ IDF = \\text{Inverse Document Frequency}\\\\\n",
        "&    \\text{Let}\\ TF =\\text{Term Frequency}\\\\[2em]\n",
        "&    TF \\: =\\: \\frac{\\text{term frequency in document}}{\\text{total words in document}}\\\\[1em]\n",
        "&    IDF(t) \\: =\\: \\log_2\\left(\\frac{\\text{total documents in corpus}}{\\text{documents with term}}\\right)\n",
        "\\end{align}\n",
        "$\n",
        "\n",
        "Then Tf–IDF is calculated as :\n",
        "$$tfidf( t, d, D ) = tf( t, d ) \\times idf( t, D )$$"
      ],
      "metadata": {
        "id": "x5-rXm5OZoc-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thus, we assign the result of the Tf-IDF weighting to the variable named `tfidf_DTM`. "
      ],
      "metadata": {
        "id": "pRPjGQVokg7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the tfidf matrix\n",
        "tfidf = TfidfTransformer(norm='l1')\n",
        "tfidf_DTM = tfidf.fit_transform(freq_term_DTM)\n",
        "print(pd.DataFrame(tfidf_DTM.todense(), columns =  [k for k, v in sorted(vectorized.vocabulary_.items(), key=lambda item: item[1])] ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GsktR7_Znke",
        "outputId": "193e6f5e-3152-430f-999b-39e2d061ed8f"
      },
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     arthur       dog      feel      felt  following      good     great  \\\n",
            "0  0.137750  0.000000  0.181125  0.000000   0.000000  0.181125  0.181125   \n",
            "1  0.215994  0.000000  0.000000  0.284006   0.284006  0.000000  0.000000   \n",
            "2  0.000000  0.333333  0.000000  0.000000   0.000000  0.000000  0.000000   \n",
            "\n",
            "    morning       run    street  thursday  \n",
            "0  0.137750  0.000000  0.000000  0.181125  \n",
            "1  0.215994  0.000000  0.000000  0.000000  \n",
            "2  0.000000  0.333333  0.333333  0.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QymfV4S_FrOZ"
      },
      "source": [
        "# <font color = 'E3A440'> 2. *Exercise : Analyse de sentiment sur Twitter* </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'exercice qui est proposé dans cette section est basé sur une simple chaîne de traitement pour l'**analyse de sentiment** sur de données de Twitter et sur l'**anayse de spécificité lexicales**. \n",
        "\n",
        "Le corpus utilisé a été collecté en 2020 par *trackmyhashtag.com* et contient 3 200 tweets pour les 50 profiles les plus suivis de Tweeter. Les données sont en format tabulaire dans un fichier CSV. Pour des raisons pédagogiques, cet exercice prevoit l;'utilisant d'un echantillon aleatoire de 5 000 Tweets.\n",
        "\n",
        "Dans un premier temps, les données textuelles de 5 000 tweets seront analysées par un module d'analyse de sentiment du module `nltk`. Ensuite, le texte sera pretraité et certaines analyses lexicales serent executées.\n",
        "\n",
        "Pendans l'exercice, la participant sera invité à remplir les parties manquentes du code qui sont indique avec `...` (trois points)."
      ],
      "metadata": {
        "id": "c8XIxHR-n0Oi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'E3A440'> 2.1 Présentation de l' exércice </font>"
      ],
      "metadata": {
        "id": "FAgZaK170IHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'> a. Importer les données </font>\n",
        "\n",
        "Le fichier avec les données est archivé dans un `.zip` et contient plus de 150 000 tweets. Pour de raisons pédagogiques, nouis importons seulement 5 000 tweets de façon aleatoire. "
      ],
      "metadata": {
        "id": "IxtJNukjCXT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_DIR='Donnees_demystifiees_seance_6/'\n",
        "DATA_DIR=os.path.join(ROOT_DIR, 'Data')\n",
        "import zipfile\n",
        "from datetime import datetime\n",
        "\n",
        "#Unzips the dataset and gets the TSV dataset\n",
        "with zipfile.ZipFile(os.path.join(DATA_DIR,'4POINT0_Top_50_tweet_profiles.zip'), 'r') as zip_ref:\n",
        "    zip_ref.extractall(DATA_DIR)\n",
        "\n",
        "df = pd.read_pickle(os.path.join(DATA_DIR,'Top_50_tweet_profiles.pkl')).sample(5000, random_state = 5641).reset_index()"
      ],
      "metadata": {
        "id": "Q6snBPy8EfBX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici les noms de variables disponibles et leur typologie."
      ],
      "metadata": {
        "id": "hDTMmSBhFA6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejgQAfv8FIrC",
        "outputId": "27631f51-8a16-40c7-dd53-017c8a9e9db9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "index                                  int64\n",
              "Tweet Id                              object\n",
              "Tweet URL                             object\n",
              "Tweet Posted Time             datetime64[ns]\n",
              "Tweet Content                         object\n",
              "Tweet Type                            object\n",
              "Client                                object\n",
              "Retweets received                      int64\n",
              "Likes received                         int64\n",
              "User Id                               object\n",
              "Name                                  object\n",
              "Username                              object\n",
              "Verified or Non-Verified              object\n",
              "Profile URL                           object\n",
              "Protected or Not Protected            object\n",
              "Profile Account                       object\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici un exemple:"
      ],
      "metadata": {
        "id": "JIDKgUxtGhQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[0]"
      ],
      "metadata": {
        "id": "54zPNygEJXhe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef21584e-0b77-4d3a-a97e-cbdf649ae903"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "index                                                                     71560\n",
              "Tweet Id                                                     656538552327630848\n",
              "Tweet URL                     https://twitter.com/billboard/status/656538552...\n",
              "Tweet Posted Time                                           2015-10-20 18:32:43\n",
              "Tweet Content                 .@JustinBieber, @Skrillex and @Bloodpop's #Sor...\n",
              "Tweet Type                                                              Retweet\n",
              "Client                                                       Twitter Web Client\n",
              "Retweets received                                                         20260\n",
              "Likes received                                                            22109\n",
              "User Id                                                                 9695312\n",
              "Name                                                                  billboard\n",
              "Username                                                              billboard\n",
              "Verified or Non-Verified                                               Verified\n",
              "Profile URL                                       https://twitter.com/billboard\n",
              "Protected or Not Protected                                        Not Protected\n",
              "Profile Account                                                    justinbieber\n",
              "Name: 0, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'> b. Exécuter l'analyse de sentiment</font>\n",
        "\n",
        "L'objet `SentimentIntensityAnalyzer` est utilisé pour executer l'analyse de sentiment. L'ionbjet doit être initialisé et, ensuite, la fonction `polarity_scores()` peut être appliquée à une chaîne de caractères."
      ],
      "metadata": {
        "id": "ym3Y6ITXGkkj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pInoj9yrFrOZ",
        "outputId": "422dc361-c97c-4034-fc67-e51f30a6f143",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "sia = SentimentIntensityAnalyzer()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici trois exemple d'analyse de sentiment. Le résulat de la fonction `polarity_scores()` retourne quatre valeurs: \n",
        "\n",
        " 1. `neg` : qui indique le dégrée, dans une echelle de 0 à 1, de sentiment négatif su texte.\n",
        " 2. `neu` : qui indique le dégrée, dans une echelle de 0 à 1, de sentiment neutre su texte.\n",
        " 3. `pos` : qui indique le dégrée, dans une echelle de 0 à 1, de sentiment positif su texte.\n",
        " 4. `compound` : qui contient le valeur d'une métrique composé des trois métriques précedentes et qui va de -1 à 1.\n",
        "\n"
      ],
      "metadata": {
        "id": "sggH1sSIIVib"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6jAgx5YgFrOa",
        "outputId": "583c06a2-da24-44f7-edbc-da548b576f8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'neg': 0.0, 'neu': 0.295, 'pos': 0.705, 'compound': 0.8012}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "sia.polarity_scores(\"Wow, NLTK is really powerful!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sia.polarity_scores(\"NLTK is not bad!\")"
      ],
      "metadata": {
        "id": "ifi6teSvITFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpGJWGRWFrOa",
        "outputId": "a4860911-d560-4ac6-f0f0-9bb274be6eba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'neg': 0.655, 'neu': 0.345, 'pos': 0.0, 'compound': -0.5848}"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "sia.polarity_scores(\"NLTK is bad!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici le tweets sur lesquels nous apliquons l'analyse de sentiment:"
      ],
      "metadata": {
        "id": "3gfG4_A6I4WK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Tweet Content']"
      ],
      "metadata": {
        "id": "5yBg-jU9QLl8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e66b977c-1375-48ee-f152-b6b1f5146f27"
      },
      "execution_count": 332,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       .@JustinBieber, @Skrillex and @Bloodpop's #Sor...\n",
              "1       👏 ¡@SergioRamos y @hazardeden10 se encuentran ...\n",
              "2       Here's how the market may predict the next pre...\n",
              "3       “Children are magical on road trips. They have...\n",
              "4       .@MelissaMcCarthy told me about the moment she...\n",
              "                              ...                        \n",
              "4995    So saddened to hear of the tragic theatre shoo...\n",
              "4996    always takes the road less traveled... @ New O...\n",
              "4997    #HustleHart #MoveWithHart https://t.co/GkQHkhKhR3\n",
              "4998    This is the letter the US Attorney General sen...\n",
              "4999    The Week on Instagram | 276\\nhttps://t.co/9kIt...\n",
              "Name: Tweet Content, Length: 5000, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 332
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans le prochain bloc de code, nous exécutons l'anayse de sentiment dur la colonne `Tweet Content`, et nous ajoutons  les résulats obtenus au tableau des données, l'objet nommé `df`."
      ],
      "metadata": {
        "id": "aCfTrGKYNOcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Éxecution de l'analyse de sentiment sur tout le corpus\n",
        "datasent = df.apply(lambda x: sia.polarity_scores(x['Tweet Content']), 1)\n",
        "df = df.join(pd.DataFrame(list(datasent)))"
      ],
      "metadata": {
        "id": "3etQFxEACpty"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le resultas de l'analyse est enregistré sous forme de variables. Voici un exemple:"
      ],
      "metadata": {
        "id": "bpauZXt7Nwyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfrp02peNwSu",
        "outputId": "74ba5d48-1004-4f5a-d2c9-aa788ca64304"
      },
      "execution_count": 334,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "index                                                                     71560\n",
              "Tweet Id                                                     656538552327630848\n",
              "Tweet URL                     https://twitter.com/billboard/status/656538552...\n",
              "Tweet Posted Time                                           2015-10-20 18:32:43\n",
              "Tweet Content                 .@JustinBieber, @Skrillex and @Bloodpop's #Sor...\n",
              "Tweet Type                                                              Retweet\n",
              "Client                                                       Twitter Web Client\n",
              "Retweets received                                                         20260\n",
              "Likes received                                                            22109\n",
              "User Id                                                                 9695312\n",
              "Name                                                                  billboard\n",
              "Username                                                              billboard\n",
              "Verified or Non-Verified                                               Verified\n",
              "Profile URL                                       https://twitter.com/billboard\n",
              "Protected or Not Protected                                        Not Protected\n",
              "Profile Account                                                    justinbieber\n",
              "neg                                                                        0.12\n",
              "neu                                                                        0.88\n",
              "pos                                                                         0.0\n",
              "compound                                                                 -0.128\n",
              "Name: 0, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 334
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour rendre simple l'analyse, nous utiliserons seulement la metrique composée `compound` qui est automatiquement calculé par la fonction  `polarity_score()`."
      ],
      "metadata": {
        "id": "wsjs2wOmbs0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['compound'].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xf_vn9Veebrg",
        "outputId": "cf006922-5359-40df-9926-d635c7e692c9"
      },
      "execution_count": 340,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    5000.000000\n",
              "mean        0.199087\n",
              "std         0.417216\n",
              "min        -0.972600\n",
              "25%         0.000000\n",
              "50%         0.000000\n",
              "75%         0.557400\n",
              "max         0.980200\n",
              "Name: compound, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 340
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour utiilser la métrique `compound` dans un conteste d'**analyse de spécificité lexicale**, il est nécessaire de constituer des categroies, soit de regourper les tweets sous les categories suivantes: :\n",
        " 1. `negative` : qui regroupe les tweets contenant un sentiment negatif (`compound` de -1 à -0.5)  \n",
        " 2. `neu` : qui regroupe les tweets plustôt neutres (`compound` de -0.5 à 0.5)\n",
        " 3. `positive` : qui regroupe les tweets contenant un sentimnet positif (`compound` plus de 0.5)"
      ],
      "metadata": {
        "id": "A6cLssRqb81P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 Déterminer les valeurs pour couper la métrique compound\n",
        "bins = [-1, -0.5, 0.5, 1]\n",
        "# 2 Déterminer les noms des categoris. NOTEZ que les nombres de noms de catégories doivent être inferieure aux valeur de découpage.\n",
        "names = ['negative', 'neu', 'positive']\n",
        "# Exécuter le decoupage avec la fonction 'cut' de pandas.\n",
        "df['compound_category']  = pd.cut(df['compound'], bins, labels=names, include_lowest =True)"
      ],
      "metadata": {
        "id": "jJrj4DKePhGd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici la distribution des tweets par categorie:"
      ],
      "metadata": {
        "id": "-cRjoTWDdJjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Counter(df['compound_category'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UL-ibgrb4u0M",
        "outputId": "7bcff8e5-7846-4348-d9b6-219b5bcfb78a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'neu': 3316, 'positive': 1401, 'negative': 283})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'> c. Annotation, nettoyage et vectorisation des tweets </font>\n",
        "\n",
        "Nous utilisons la fonction écrite précedemement pour nettoyer les unités lexicales de tweets. Pour ce premier test nous conservons seulement les adjectifs.\n",
        "\n",
        "Cette opération prendra quleque secondes. "
      ],
      "metadata": {
        "id": "_zmlEGCKRaWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_tweets = [CleaningText(sent, reduce = 'lemma', list_pos_to_keep = ['ADJ'], Stopwords_to_add=['http']) for sent in list(df['Tweet Content'])]"
      ],
      "metadata": {
        "id": "NMmjI8IaU_JF"
      },
      "execution_count": 335,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans l'étape de vectorisaiton nous retenons les mots qui apparaissent dans au moins 5 documents (`min_df = 5`)."
      ],
      "metadata": {
        "id": "TA9YRrWiLzw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisation de l'objet\n",
        "def identity_tokenizer(text):\n",
        "    return text\n",
        "# Transforming the word in frequencies\n",
        "vectorized = CountVectorizer(lowercase = False, # Convert all characters to lowercase before tokenizing\n",
        "                             min_df = 5, # Ignore terms that have a document frequency strictly lower than the given threshold \n",
        "                             max_df = 4500, # Ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words)\n",
        "                             stop_words = stopwords.words('english'), # Remove the list of words provided\n",
        "                             ngram_range = (1, 1), # Get the lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted\n",
        "                             tokenizer=identity_tokenizer) # Override the string tokenization step while preserving the preprocessing and n-grams generation steps"
      ],
      "metadata": {
        "id": "6Aj4JUdeYBbE"
      },
      "execution_count": 342,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq_term_DTM = vectorized.fit_transform([[w for w, pos in sent] for sent in cleaned_tweets])\n",
        "freq_term_DTM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkSkuQvUXsvs",
        "outputId": "5adeb5c2-ee08-4e7e-d7e1-3ebcace4e318"
      },
      "execution_count": 343,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<5000x172 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 2661 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 343
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'> d. Analyse des specificités lexicales </font>\n",
        "\n",
        "L'analyse de spécificités lexicales permet de metter en evidence les unité lexicales qui sont spécifique à un particulier groupe de données. Dans notre cas, il est possible de identifier les mot qui sont plus fortement associés avec des sentiment positif ou négatif. \n",
        "\n",
        "Pour se faire, nous utilisons une métrique très diffusé en lexicometrie, qui est la fonction de vraisemblance (log-likelihood Ratio). La metrique est basée sur cet [article](https://aclanthology.org/J93-1003.pdf). D'autres méthodes peuvent être utilisées, comme l'infiormation mutuelle, le chi2 ou la ponderation Tf-Idf."
      ],
      "metadata": {
        "id": "RtSfYp1rfUxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def GetLexicalSpecificities(freq_term_DTM, logical_vector):\n",
        "    # This code ref takes inspiration from this python module : https://pypi.org/project/corpus-toolkit/\n",
        "    # and its main script:  https://github.com/kristopherkyle/corpus_toolkit/blob/master/corpus_toolkit/corpus_tools.py\n",
        "    # which is based on this paper: https://aclanthology.org/J93-1003/\n",
        "    import math\n",
        "    df_freq_target = pd.DataFrame(np.asarray(freq_term_DTM[logical_vector].sum(0).T).reshape(-1))\n",
        "    df_freq_target.index = [word for (word,idx) in sorted(vectorized.vocabulary_.items(), key= lambda x:x[1])]\n",
        "    df_freq_target.columns = ['freq1']\n",
        "    df_freq_target['freq2'] = np.asarray(freq_term_DTM[~(logical_vector)].sum(0).T).reshape(-1)\n",
        "    df_freq_target['tot'] = df_freq_target['freq1'] + df_freq_target['freq2']\n",
        "\n",
        "    df_freq_target['freq1'] = df_freq_target['freq1'].apply(lambda x: 0.0000001 if x == 0 else x).astype(float)\n",
        "    df_freq_target['freq2'] = df_freq_target['freq2'].apply(lambda x: 0.0000001 if x == 0 else x).astype(float)\n",
        "    #\n",
        "    df_freq_target['freq1_norm'] = df_freq_target['freq1']/df_freq_target['freq1'].sum() * 1000000\n",
        "    df_freq_target['freq2_norm'] = df_freq_target['freq2']/df_freq_target['freq2'].sum() * 1000000\n",
        "    #\n",
        "    df_freq_target['fraction'] = df_freq_target['freq1_norm'] / df_freq_target['freq2_norm']\n",
        "    df_freq_target['Log-likelihood Ratio'] = df_freq_target['fraction'].apply(math.log2)\n",
        "    frequency_threshold = 10 # Insert your frequency threshold as integer\n",
        "    return df_freq_target[df_freq_target['tot'] > frequency_threshold]['Log-likelihood Ratio'].sort_values(ascending=False).iloc[range(50)]"
      ],
      "metadata": {
        "id": "39eDHLjFZcLU"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Poour exécuter l'analyse de spécificité est nécessaire de créaer un vecteur logique (avec de valeurs binaires) qui indique comme `True` la classe pour laquelle nous voulons analyser sa spécifcité lexicale et avec `False`le reste du corpus. "
      ],
      "metadata": {
        "id": "SIxwMCSahwh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logical_vector = df['compound_category'] == 'positive'\n",
        "logical_vector"
      ],
      "metadata": {
        "id": "TVhrPm7PZO5E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce17427e-e3f4-440a-e008-0aa5783f7319"
      },
      "execution_count": 372,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       False\n",
              "1       False\n",
              "2       False\n",
              "3       False\n",
              "4       False\n",
              "        ...  \n",
              "4995    False\n",
              "4996    False\n",
              "4997    False\n",
              "4998    False\n",
              "4999    False\n",
              "Name: compound_category, Length: 5000, dtype: bool"
            ]
          },
          "metadata": {},
          "execution_count": 372
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(logical_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogaDNEUNZQU0",
        "outputId": "4547e366-3955-4763-cc6b-9beefcaa659f"
      },
      "execution_count": 373,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1401"
            ]
          },
          "metadata": {},
          "execution_count": 373
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Executer la fonciton avec la matrice des fréquences (`freq_term_DTM`) et le vectur logique que nous avons créé plus haut."
      ],
      "metadata": {
        "id": "G3lrLVXfiMMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GetLexicalSpecificities(freq_term_DTM, logical_vector)"
      ],
      "metadata": {
        "id": "8iEQNaJvJ2fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'E3A440'> 2.2 Exercice </font>\n",
        "\n",
        "Pendans l'exercice, la participant sera invité à remplir les parties manquentes du code qui sont indique avec `...` (trois points).\n",
        "\n",
        "Plusieurs manipulations et différent résultats seront demandés. Chaque sous éxercice suit la suivante chaîne de traitement:\n",
        "\n",
        "1. Annotation et nettoyage des tweets : la participant devra ajuster quelque parametre de la fonction pour choisir un filtrage spécifique\n",
        "2. Vectorisation:  la participant devra ajuster quelque parametre de la fonction pour choisir un filtrage spécifique\n",
        "3. Création d'un vecteur logique pour défnir le groupe target et le groupede réference. \n",
        "4. Application de la fonction `GetLexicalSpecificities()` pour obtenir les 50 mots les plus spécifiques au groupe target.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8zRK37miNzH6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'> a. Étudfier l'impact du filtrage morphosyntaxique sur les spécificités lexicales </font>\n",
        "\n",
        "Au point 2.1, seulement les adjectifs ont été étudiés. Faites maintenant une étude sur les noms, adjectifs et verbes et ensuite sur d'autres combination que vous interesse. \n",
        "Voici la liste des POS tag existant:"
      ],
      "metadata": {
        "id": "Niv5IUK8VJ8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **POS** | **DESCRIPTION**           | **EXAMPLES**                                      |\n",
        "| ------- | ------------------------- | ------------------------------------------------- |\n",
        "| ADJ     | adjective                 | big, old, green, incomprehensible, first      |\n",
        "| ADP     | adposition                | in, to, during                                |\n",
        "| ADV     | adverb                    | very, tomorrow, down, where, there            |\n",
        "| AUX     | auxiliary                 | is, has (done), will (do), should (do)        |\n",
        "| CONJ    | conjunction               | and, or, but                                  |\n",
        "| CCONJ   | coordinating conjunction  | and, or, but                                  |\n",
        "| DET     | determiner                | a, an, the                                    |\n",
        "| INTJ    | interjection              | psst, ouch, bravo, hello                      |\n",
        "| NOUN    | noun                      | girl, cat, tree, air, beauty                  |\n",
        "| NUM     | numeral                   | 1, 2017, one, seventy-seven, IV, MMXIV        |\n",
        "| PART    | particle                  | ’s, not                                      |\n",
        "| PRON    | pronoun                   | I, you, he, she, myself, themselves, somebody |\n",
        "| PROPN   | proper noun               | Mary, John, London, NATO, HBO                 |\n",
        "| PUNCT   | punctuation               | ., (, ), ?                                    |\n",
        "| SCONJ   | subordinating conjunction | if, while, that                               |\n",
        "| SYM     | symbol                    | $, %, §, ©, +, −, ×, ÷, =, :)               |\n",
        "| VERB    | verb                      | run, runs, running, eat, ate, eating          |\n",
        "| X       | other                     | sfpksdpsxmsa                                  |\n",
        "| SPACE   | space                     |                                                   |\n"
      ],
      "metadata": {
        "id": "CJps3VlyWcoV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inserez la bonne valeur pour l'argument `list_pos_to_keep` afin de pouvoir garder les noms, adjectifs et verbers, ou toute autres combination de POS tag de votre intérêt"
      ],
      "metadata": {
        "id": "b4bLCBgqWlzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Annotation and cleaning : ADD adjective and verbs as POS tag to keep\n",
        "cleaned_tweets = [CleaningText(sent, reduce = 'lemma', list_pos_to_keep = [...], Stopwords_to_add=['http']) for sent in list(df['Tweet Content'])]"
      ],
      "metadata": {
        "id": "xtrWcUOmNxOR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changez les parametres `min_df` afin de ne pas depasser **750 mots** de votre matrice <font color='E3A440'>**Document-Term matrix**</font>, qui est enregistrée dans l'objet `freq_term_DTM`.\n",
        "\n",
        "Notez bien que dans cette fonction le paramtre `ngram_range` est configuré pour avoir les unigrams et les bigrams (sa valeur est : `(1,2)`)."
      ],
      "metadata": {
        "id": "g6JapeP7W2xO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Vectorisation\n",
        "def identity_tokenizer(text):\n",
        "    return text\n",
        "## 2.1 initialise with parameters : \n",
        "vectorized = CountVectorizer(lowercase = False, # Convert all characters to lowercase before tokenizing\n",
        "                             min_df = 10, # Ignore terms that have a document frequency strictly lower than the given threshold \n",
        "                             max_df = 4500, # Ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words)\n",
        "                             stop_words = stopwords.words('english'), # Remove the list of words provided\n",
        "                             ngram_range = (1, 2), # Get the lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted\n",
        "                             tokenizer=identity_tokenizer) # Override the string tokenization step while preserving the preprocessing and n-grams generation steps\n",
        "\n",
        "#\n",
        "freq_term_DTM = vectorized.fit_transform([[w for w, pos in sent] for sent in cleaned_tweets])\n",
        "\n",
        "freq_term_DTM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoTIyCR7daI7",
        "outputId": "b3becf7c-37ce-45a6-f13a-ac470b8d7321"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'b', 'c', 'e', 'f', 'g', 'h', 'j', 'l', 'n', 'p', 'r', 'u', 'v', 'w'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<5000x727 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 20057 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En utilisant le travail fait déjà au point **b.** de la section **2.1**, choisissez la categorie de sentiment pour laquelle vous voulez étudier la spécioficité lexicale ex.  `negative` or `positive`."
      ],
      "metadata": {
        "id": "PC98Vv87YCL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logical_vector = df['compound_category'] == ..."
      ],
      "metadata": {
        "id": "B-qMBlPhPGC_"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En utilisant la fonction définie au point **d.** de la section **2.1**, ajoutez les arguments fondamentales de la focntion, soit la matrice  <font color='E3A440'>**Document-Term matrix**</font> et le **vecteur logique** créé dans le bloc de code précedent."
      ],
      "metadata": {
        "id": "kUkik8fxYXe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GetLexicalSpecificities(freq_term_DTM, logical_vector)"
      ],
      "metadata": {
        "id": "CGFUrGPVPJWb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1113254-4c5f-4934-f3a9-02380d180857"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "beautiful         28.750313\n",
              "congratulation    28.498775\n",
              "super             28.106457\n",
              "honored           27.691420\n",
              "haha              27.565889\n",
              "amazing            6.002708\n",
              "free               5.710942\n",
              "congrats           5.376523\n",
              "happy              5.299217\n",
              "award              4.852961\n",
              "happy birthday     4.553400\n",
              "awesome            4.553400\n",
              "great              4.528526\n",
              "thank love         4.437923\n",
              "fun                4.397281\n",
              "perfect            4.312392\n",
              "strong             4.312392\n",
              "love               4.123814\n",
              "gift               3.759851\n",
              "best               3.743731\n",
              "loved              3.589926\n",
              "birthday           3.569168\n",
              "win                3.553400\n",
              "progress           3.553400\n",
              "honor              3.437923\n",
              "wow                3.355461\n",
              "wish               3.338387\n",
              "special            3.312392\n",
              "hilarious          3.174889\n",
              "peace              3.174889\n",
              "thank much         3.100888\n",
              "kind               3.075353\n",
              "hope               3.062414\n",
              "proud              3.045606\n",
              "friend             2.625550\n",
              "energy             2.553400\n",
              "opportunity        2.437923\n",
              "surprise           2.437923\n",
              "ji                 2.437923\n",
              "thanks             2.290366\n",
              "thank              2.227085\n",
              "share              2.133068\n",
              "dream              2.115995\n",
              "support            2.064465\n",
              "much               2.037385\n",
              "please             1.968438\n",
              "incredible         1.930963\n",
              "feel               1.917091\n",
              "effort             1.852961\n",
              "wife               1.852961\n",
              "Name: Log-likelihood Ratio, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'> b. Étudier de nouvelles categories basée sur le nombre de Retweet </font>\n",
        "\n",
        "Sur tweeter il est possible de Retweetter un Tweet existant. Le nombre de retweet peut être consideré un indicateur su suivi qu'un tweet a obtenu. En accomplissant les mêmes étapes appris au long de cet atelier, quels sont les spécificité lexicales de tweet qui ont eu un très grand suivi? "
      ],
      "metadata": {
        "id": "AcDMzwLhaFyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici la distribution de la colonne `Retweets received`."
      ],
      "metadata": {
        "id": "cNHccN-mc6Gj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Retweets received'].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tnUU_iVQZBh",
        "outputId": "3216cf27-1fe9-4dfe-f9c1-2123bf75d0cc"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count      5000.00000\n",
              "mean       6181.56700\n",
              "std       21201.34775\n",
              "min           0.00000\n",
              "25%         161.00000\n",
              "50%         741.00000\n",
              "75%        3241.00000\n",
              "max      449711.00000\n",
              "Name: Retweets received, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En suivant les percentile qui sont affiché dans la distribution de la colonne `Retweets received` (résultat du bloc de code précedent), ajouté le valeur de decoupage manquante dans la liste des \"bins\". \n",
        "Nous diviserons le nombre de Retweet en quatres categorie: \n",
        "1. `low`, qui regroupe les tweets ayant récu un faible suivi\n",
        "2. `medium`, qui regroupe les tweets ayant récu un suivi moyen\n",
        "3. `high`, qui regroupe les tweets ayant récu un grand suivi\n",
        "4. `very_high`, qui regroupe les tweets ayant récu un très grand suivi"
      ],
      "metadata": {
        "id": "RPuQijA6dAR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bins = [-np.inf, 161, ..., ..., 449711]\n",
        "names = ['low', 'medium', 'high', 'very_high']\n",
        "df['Retweets_received_category']  = pd.cut(df['Retweets received'], bins, labels=names, include_lowest =True)"
      ],
      "metadata": {
        "id": "Z25CYh43QRBK"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choisir la categorie target pour laquelle analyser les spécificités lexicale. La vlauer doit être une des quatres valeurs contenus dans la colonne `Retweets_received_category`générée dans le bloc de code précedent."
      ],
      "metadata": {
        "id": "NcugGIyneFIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logical_vector = df['Retweets_received_category'] == ..."
      ],
      "metadata": {
        "id": "WT1B3Iq-RIPZ"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Éxecutez l'analyse de spécificités."
      ],
      "metadata": {
        "id": "NQ9FdJfnf9qW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GetLexicalSpecificities(freq_term_DTM, logical_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6kVV9xjRM9h",
        "outputId": "5a9a4fd6-9fdb-48cc-acc1-04b9baceeeda"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "realdonaldtrump    7.083156\n",
              "republican         6.083156\n",
              "democrat           6.083156\n",
              "military           4.538835\n",
              "witness            4.498193\n",
              "impeachment        4.407591\n",
              "nothing            4.320655\n",
              "worst              3.953873\n",
              "fact               3.623724\n",
              "taylor             3.591303\n",
              "done               3.439300\n",
              "speaker            3.346190\n",
              "president          3.135043\n",
              "kevin              3.024262\n",
              "call               3.024262\n",
              "american           2.953873\n",
              "congress           2.913231\n",
              "government         2.886759\n",
              "court              2.886759\n",
              "iran               2.835228\n",
              "job                2.761228\n",
              "local              2.761228\n",
              "money              2.761228\n",
              "case               2.673765\n",
              "trying             2.673765\n",
              "trump              2.673765\n",
              "lost               2.645750\n",
              "public             2.645750\n",
              "hearing            2.591303\n",
              "release            2.591303\n",
              "possible           2.591303\n",
              "continues          2.591303\n",
              "wrong              2.591303\n",
              "able               2.591303\n",
              "speech             2.591303\n",
              "event              2.538835\n",
              "number             2.538835\n",
              "president trump    2.498193\n",
              "opportunity        2.439300\n",
              "god                2.439300\n",
              "went               2.439300\n",
              "high               2.346190\n",
              "taken              2.301796\n",
              "run                2.301796\n",
              "news               2.301796\n",
              "anything           2.301796\n",
              "trade              2.301796\n",
              "death              2.258727\n",
              "idea               2.176265\n",
              "happen             2.176265\n",
              "Name: Log-likelihood Ratio, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'> c. Étudier les language spécifique des profiles Tweeter </font>\n",
        "\n",
        "Dans le prochain éxercice, sélectionnez deux ou trois profiles Tweeter de votre choix et comparez les specificité lexicales en étudiant plusieurs combination dePOS tag. Quels sont les grandes différenes lexiclaers entres les profies que vous avez choisie? \n",
        "\n",
        "Voici la liste complète des profiles présent dans le corpus et enregistrés sous la colonne `Profile Account` et les nombre dce tweets par profile."
      ],
      "metadata": {
        "id": "uIiCPShTgE8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Counter(df['Profile Account'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jfl6H0JV0v53",
        "outputId": "704c1ab4-9c4d-43d8-90e8-13babd753e41"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'justinbieber': 109,\n",
              "         'realmadrid': 106,\n",
              "         'cnn': 94,\n",
              "         'nytimes': 109,\n",
              "         'theellenshow': 95,\n",
              "         'jlo': 89,\n",
              "         'sportscenter': 106,\n",
              "         'louis_tomlinson': 104,\n",
              "         'shakira': 107,\n",
              "         'rihanna': 98,\n",
              "         'bbcbreaking': 106,\n",
              "         'jtimberlake': 115,\n",
              "         'realdonaldtrump': 105,\n",
              "         'ladygaga': 87,\n",
              "         'selenagomez': 112,\n",
              "         'britneyspears': 118,\n",
              "         'liampayne': 93,\n",
              "         'neymarjr': 115,\n",
              "         'cnnbrk': 97,\n",
              "         'kingjames': 103,\n",
              "         'twitter': 97,\n",
              "         'cristiano': 111,\n",
              "         'niallofficial': 109,\n",
              "         'narendramodi': 100,\n",
              "         'drake': 56,\n",
              "         'espn': 99,\n",
              "         'liltunechi': 70,\n",
              "         'oprah': 100,\n",
              "         'barackobama': 95,\n",
              "         'iamsrk': 102,\n",
              "         'arianagrande': 105,\n",
              "         'imvkohli': 62,\n",
              "         'jimmyfallon': 116,\n",
              "         'harry_styles': 98,\n",
              "         'instagram': 97,\n",
              "         'beingsalmankhan': 100,\n",
              "         'youtube': 100,\n",
              "         'wizkhalifa': 103,\n",
              "         'kevinhart4real': 132,\n",
              "         'mileycyrus': 96,\n",
              "         'katyperry': 102,\n",
              "         'nasa': 104,\n",
              "         'fcbarcelona': 100,\n",
              "         'pink': 96,\n",
              "         'billgates': 118,\n",
              "         'srbachchan': 115,\n",
              "         'kimkardashian': 116,\n",
              "         'brunomars': 122,\n",
              "         'akshaykumar': 101,\n",
              "         'taylorswift13': 10})"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Annotation and cleaning : ADD adjective and verbs as POS tag to keep\n",
        "cleaned_tweets = [CleaningText(sent, reduce = 'lemma', list_pos_to_keep = [...], Stopwords_to_add=['http']) for sent in list(df['Tweet Content'])]\n",
        "\n",
        "# 2. Vectorisation\n",
        "def identity_tokenizer(text):\n",
        "    return text\n",
        "## 2.1 initialise with parameters : \n",
        "vectorized = CountVectorizer(lowercase = False, # Convert all characters to lowercase before tokenizing\n",
        "                             min_df = 10, # Ignore terms that have a document frequency strictly lower than the given threshold \n",
        "                             max_df = 4500, # Ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words)\n",
        "                             stop_words = stopwords.words('english'), # Remove the list of words provided\n",
        "                             ngram_range = (1, 1), # Get the lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted\n",
        "                             tokenizer=identity_tokenizer) # Override the string tokenization step while preserving the preprocessing and n-grams generation steps\n",
        "\n",
        "#\n",
        "freq_term_DTM = vectorized.fit_transform([[w for w, pos in sent] for sent in cleaned_tweets])\n",
        "\n",
        "freq_term_DTM"
      ],
      "metadata": {
        "id": "yYTkjK6jgWa8",
        "outputId": "9bcf53cd-f318-4d18-a355-da19080cc16c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<5000x437 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 10635 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logical_vector = df['Profile Account'] == 'nasa'\n",
        "GetLexicalSpecificities(freq_term_DTM, logical_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-o-jsBI0pof",
        "outputId": "0aec7ace-5158-475a-8bb8-ee9f8b275e7f"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "artemis       31.795959\n",
              "spacecraft    31.248471\n",
              "nasa           6.994975\n",
              "mar            6.753967\n",
              "moon           6.248731\n",
              "earth          6.220535\n",
              "program        6.169004\n",
              "space          6.031501\n",
              "mission        5.432039\n",
              "launch         5.294535\n",
              "test           5.072143\n",
              "science        4.972607\n",
              "system         4.809108\n",
              "et             4.500986\n",
              "detail         4.124610\n",
              "planet         3.809108\n",
              "data           3.487180\n",
              "flight         3.294535\n",
              "tune           3.046608\n",
              "camera         2.879498\n",
              "meet           2.835103\n",
              "question       2.631570\n",
              "future         2.594095\n",
              "medium         2.207072\n",
              "summer         2.072143\n",
              "share          2.046608\n",
              "help           1.972607\n",
              "honor          1.972607\n",
              "name           1.972607\n",
              "decade         1.879498\n",
              "journey        1.835103\n",
              "hour           1.835103\n",
              "step           1.835103\n",
              "company        1.835103\n",
              "plan           1.835103\n",
              "star           1.709573\n",
              "minute         1.631570\n",
              "photo          1.616463\n",
              "nation         1.594095\n",
              "florida        1.594095\n",
              "june           1.487180\n",
              "challenge      1.487180\n",
              "watch          1.461645\n",
              "update         1.387645\n",
              "power          1.355936\n",
              "look           1.340339\n",
              "eye            1.294535\n",
              "stage          1.207072\n",
              "live           1.165252\n",
              "collection     1.124610\n",
              "Name: Log-likelihood Ratio, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'E3A440'> 2.3 NOTES PERSONELLES: </font>\n",
        "\n",
        "-----\n",
        "\n",
        "-----"
      ],
      "metadata": {
        "id": "IhZSp6JKjH8k"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}