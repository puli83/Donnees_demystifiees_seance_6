{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWNgJtQ3FrN8"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "<font size='10' color = 'E3A440'>**Mégadonnées et techniques avancées démystifiées**</font>\n",
        "=======\n",
        "<font color = 'E3A440'>*Nouvelles méthodes d’analyse et leur implication quant à la gestion des mégadonnées en SSH (partie 1)*</font>\n",
        "=============\n",
        "\n",
        "\n",
        "Cet atelier s’inscrit dans le cadre de la formation [Mégadonnées et techniques avancées démystifiées](https://www.4point0.ca/2022/08/22/formation-megadonnees-demystifiees/) (séance 6).\n",
        "\n",
        "Les sciences humaines et sociales sont souvent confrontées à l’analyse de données non structurées, comme le texte. Après avoir préparé les données, plusieurs techniques d’analyse venant de l’apprentissage automatique peuvent être utilisées. Pendant cet atelier, les participants seront initiés aux méthodes supervisées et non supervisées à des buts d’analyse avec Python.\n",
        "\n",
        "Note : Cet atelier se poursuit lors d’une 2e séance le 10 novembre.\n",
        "\n",
        "Structure de l'atelier :\n",
        "1. Presentation of sections 1 and 2 in a plenary mode (20 minutes)\n",
        "2. Individual work on section 3 (20 minutes)\n",
        "3. Group work on section 4 (60 minutes)\n",
        "4. Plenary session with groups presentations (20 minutes)\n",
        "\n",
        "Ce tutoriel ne peut pas être consideré exaustif .... \n",
        "\n",
        "### Authors: \n",
        "- Bruno Agard <bruno.agard@polymtl.ca>\n",
        "- Davide Pulizzotto <davide.pulizzotto@polymtl.ca>\n",
        "\n",
        "### Table of Contents\n",
        "Bruno Agard\n",
        "\n",
        "Département de Mathématiques et de génie Industriel\n",
        "\n",
        "École Polytechnique de Montréal"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Préparation environnement"
      ],
      "metadata": {
        "id": "HpAX32-4Kj8U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jddhTL7EFrN_",
        "outputId": "f8e7716c-75e3-44c5-ac7c-1ac1093f6fd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Donnees_demystifiees_seance_6'...\n",
            "remote: Enumerating objects: 55, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (52/52), done.\u001b[K\n",
            "remote: Total 55 (delta 16), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (55/55), done.\n"
          ]
        }
      ],
      "source": [
        "# Downloading of data from the GitHub project\n",
        "!rm -rf Donnees_demystifiees_seance_6/\n",
        "!git clone https://github.com/puli83/Donnees_demystifiees_seance_6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJLRFWa6FrOA",
        "outputId": "cabd2e88-ab6a-462f-c21a-ab268b3478f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Import modules\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwFKNpPQFrN-"
      },
      "source": [
        "# <font color = 'E3A440'>*Préparation des données textulles*</font>\n",
        "\n",
        "L'analyse de données textuelles implique la transformation d'un texte en un objet mathematique qui peut être utilisé par des algorithmes et des modèles statistique. Cette étape est importante car permet de **structurer** des données non structurées, comme le texte.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDS10IXPFrOB"
      },
      "source": [
        "###  <font color = 'E3A440'>**Étapes fondamentales du prétraitement**</font>\n",
        "\n",
        "Prenons une phrase pour decortiquer les passages que nous pemrettent de la transformer en information structurée."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "h9AVaykcFrOB"
      },
      "outputs": [],
      "source": [
        "sentence = \"\"\"At eight o'clock, on Thursday morning, the great Arthur didn't feel VERY good.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpFqEU4iFrOC",
        "outputId": "47ef0ec3-cfb0-416e-bc9a-a2b319f54a01"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "78"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "len(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWQNjvKIFrOD"
      },
      "source": [
        "#### <font color = 'E3A440'>*1. Tokenisation*</font>\n",
        "\n",
        "Cette étape consiste à couper la phrase en unités linguistiques élémantaire et dotées de sens, ce qui est gnralement appelé le \"mot\".\n",
        "\n",
        "Dans le module `nltk`, il existe une fonction qui permet cette opération, soit `word_tokenize()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzF3fiUWFrOE",
        "outputId": "938dfc0d-91a1-4b21-af76-241e4cd7e52b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['At', 'eight', \"o'clock\", ',', 'on', 'Thursday', 'morning', ',', 'the', 'great', 'Arthur', 'did', \"n't\", 'feel', 'VERY', 'good', '.']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# La function word_tokenize() prend la phrase comme argument.\n",
        "words = nltk.word_tokenize(sentence)\n",
        "print(words)\n",
        "len(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color = 'E3A440'>*2. Analyse morphosyntaxique*</font>\n",
        "\n",
        "Après avoir identifé tous les mots, il est possible de analyser leur rôle morphosyntaxique, à des fins d'analyse et/ou filtrage. "
      ],
      "metadata": {
        "id": "CvM6JKj1hLX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # La function word_tokenize() prend la liste de mots comme argument.\n",
        "words_pos = nltk.pos_tag(words, tagset='universal')\n",
        "print(words_pos)\n",
        "len(words_pos)"
      ],
      "metadata": {
        "id": "qy06di1ygtmF",
        "outputId": "ceb9bd8b-18f0-43ec-8903-0b81c213b2dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('At', 'ADP'), ('eight', 'NUM'), (\"o'clock\", 'NOUN'), (',', '.'), ('on', 'ADP'), ('Thursday', 'NOUN'), ('morning', 'NOUN'), (',', '.'), ('the', 'DET'), ('great', 'ADJ'), ('Arthur', 'NOUN'), ('did', 'VERB'), (\"n't\", 'ADV'), ('feel', 'VERB'), ('VERY', 'ADV'), ('good', 'ADJ'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici la liste de possible POS tags:"
      ],
      "metadata": {
        "id": "6cECT-Gp2obD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **POS** | **DESCRIPTION**           | **EXAMPLES**                                      |\n",
        "| ------- | ------------------------- | ------------------------------------------------- |\n",
        "| ADJ     | adjective                 | big, old, green, incomprehensible, first      |\n",
        "| ADP     | adposition                | in, to, during                                |\n",
        "| ADV     | adverb                    | very, tomorrow, down, where, there            |\n",
        "| AUX     | auxiliary                 | is, has (done), will (do), should (do)        |\n",
        "| CONJ    | conjunction               | and, or, but                                  |\n",
        "| CCONJ   | coordinating conjunction  | and, or, but                                  |\n",
        "| DET     | determiner                | a, an, the                                    |\n",
        "| INTJ    | interjection              | psst, ouch, bravo, hello                      |\n",
        "| NOUN    | noun                      | girl, cat, tree, air, beauty                  |\n",
        "| NUM     | numeral                   | 1, 2017, one, seventy-seven, IV, MMXIV        |\n",
        "| PART    | particle                  | ’s, not                                      |\n",
        "| PRON    | pronoun                   | I, you, he, she, myself, themselves, somebody |\n",
        "| PROPN   | proper noun               | Mary, John, London, NATO, HBO                 |\n",
        "| PUNCT   | punctuation               | ., (, ), ?                                    |\n",
        "| SCONJ   | subordinating conjunction | if, while, that                               |\n",
        "| SYM     | symbol                    | $, %, §, ©, +, −, ×, ÷, =, :)               |\n",
        "| VERB    | verb                      | run, runs, running, eat, ate, eating          |\n",
        "| X       | other                     | sfpksdpsxmsa                                  |\n",
        "| SPACE   | space                     |                                                   |\n"
      ],
      "metadata": {
        "id": "kUDRUVYe2kLi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4f_8mvLFrOE"
      },
      "source": [
        "#### <font color = 'E3A440'>*3. Retirer la ponctuation*</font>\n",
        "\n",
        "Une autre opération consiste à retirer la ponctuation. Ce type de filtrage reduit le nombre de sugne graphique qui participent le moin à la constructuion de la sémantique de la prhase. \n",
        "Dans certain contexte, comme en stylometrie, ce processus est appliquée avec de terchniques plus sofistiquées. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0P7SkM5RFrOF",
        "outputId": "f78970a7-3c61-4058-a96c-f8c33fb2f526"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('At', 'ADP'), ('eight', 'NUM'), ('on', 'ADP'), ('Thursday', 'NOUN'), ('morning', 'NOUN'), ('the', 'DET'), ('great', 'ADJ'), ('Arthur', 'NOUN'), ('did', 'VERB'), ('feel', 'VERB'), ('VERY', 'ADV'), ('good', 'ADJ')]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# La ligne de code suivant itére sur chaque signe graphique et retient ceux qui contiennet de caracteres alphanumérique.\n",
        "words_pos = [(w, pos) for w, pos in words_pos if w.isalnum()]\n",
        "print(words_pos)\n",
        "len(words_pos)\n",
        "# Ikl est possible aussi d'utiilser le résultat de l'analyse morphosyntaxique pour éliminer la ponctuaction\n",
        "# words_pos = [(w, pos) for w, pos in words_pos if pos != '.']\n",
        "# print(words_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwcORV3vFrOG"
      },
      "source": [
        "#### <font color = 'E3A440'>*4. Convertir chaque caractère en minuscule*</font>\n",
        "\n",
        "Cette étape constitue une première opéraiton de normalisation des mots et leur réduction à une forme graphique unique. Ce genre d'étape permet de regrouper chaque occurence d'un mot sous une seule forme."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8tdwmniFrOG",
        "outputId": "350309c6-67f9-4ac5-8fdd-acee6323feae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('at', 'ADP'), ('eight', 'NUM'), ('on', 'ADP'), ('thursday', 'NOUN'), ('morning', 'NOUN'), ('the', 'DET'), ('great', 'ADJ'), ('arthur', 'NOUN'), ('did', 'VERB'), ('feel', 'VERB'), ('very', 'ADV'), ('good', 'ADJ')]\n"
          ]
        }
      ],
      "source": [
        "# La ligne de code suivant itére sur chaque signe graphique et le transforme en minuscule.\n",
        "words_pos = [(w.lower(), pos) for w, pos in words_pos]\n",
        "print(words_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd1RxnCnFrOH"
      },
      "source": [
        "#### <font color = 'E3A440'>*5. retirer les stopwrods (mots vides)*</font>\n",
        "\n",
        "Une autre opération de filtrage constitue dan l'élimination de mots fonctionnels. Cette liste de mots contient tout les connecteurs de phrase, comme \"et\", \"mais\", \"toutefois\" et de mots avec faible valeur sémantique, comem les verbes modaux. \n",
        "Comme d'autres opéraiton de filtrage, l'enjeuy est celui de nettoyer le plus possible le vocabulaire et de reduyire toutes les occurrences d'un mot sous une forme graphique unique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icU72m20FrOH",
        "outputId": "c3162af7-e04b-42d6-a353-be85c7df5ee7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ],
      "source": [
        "# Nous importons la liste de stopword en anglais\n",
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words(\"english\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ws2STtGKFrOH",
        "outputId": "c8ea2f11-c8ff-40f0-e026-39c71051620d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('eight', 'NUM'), ('thursday', 'NOUN'), ('morning', 'NOUN'), ('great', 'ADJ'), ('arthur', 'NOUN'), ('feel', 'VERB'), ('good', 'ADJ')]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# La ligne de code suivant itére sur chaque signe graphique et garde ce qui n sont pas dans la liste de stopword.\n",
        "words_pos = [(w, pos) for w, pos in words_pos if w not in stopwords.words(\"english\")]\n",
        "print(words_pos)\n",
        "len(words_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8acb4VWFrOI"
      },
      "source": [
        "#### <font color = 'E3A440'>*6. Rammener les mots à leur racine*</font> \n",
        "\n",
        "En suivant le même objectif, nous retirons le suffixe morphologique des mots, ce qui augmente le niveau de réduction de chaque occurrence d'un mot à une unique forme graphique.\n",
        "\n",
        "Ils existent deux méthode fondamentales: la racinisaiton et la lemmatisation.\n",
        "La première reduit les occurence à une racine qui est inférée au moyen de plusieur techniques, l'autre est la réduciton de l'occurrence à son lemme. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_mhuA7zFrOI",
        "outputId": "b6712ef1-a875-4f41-8dab-24b64a094cae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('eight', 'NUM'), ('thursday', 'NOUN'), ('morn', 'NOUN'), ('great', 'ADJ'), ('arthur', 'NOUN'), ('feel', 'VERB'), ('good', 'ADJ')]\n"
          ]
        }
      ],
      "source": [
        "# Racinisation: technique Porter\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmed_pos = [(PorterStemmer().stem(w), pos) for w, pos in words_pos]\n",
        "print(stemmed_pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnsV3pSZFrOI",
        "outputId": "1a559c9a-07d5-4d00-a811-3c5c91cb8179"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('eight', 'NUM'), ('thursday', 'NOUN'), ('morn', 'NOUN'), ('gre', 'ADJ'), ('arth', 'NOUN'), ('feel', 'VERB'), ('good', 'ADJ')]\n"
          ]
        }
      ],
      "source": [
        "# Racinisation: technique Lancaster\n",
        "from nltk.stem import LancasterStemmer\n",
        "stemmed_pos = [(LancasterStemmer().stem(w), pos) for w, pos in words_pos]\n",
        "print(stemmed_pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN7uAtUgFrOJ",
        "outputId": "c228556d-e2ee-4461-b5a3-f56152f406a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('eight', 'NUM'), ('thursday', 'NOUN'), ('morning', 'NOUN'), ('great', 'ADJ'), ('arthur', 'NOUN'), ('feel', 'VERB'), ('good', 'ADJ')]\n"
          ]
        }
      ],
      "source": [
        "# Lemmatisaiton: utilisant le thesaurus wordnet\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lemmed_pos = [(WordNetLemmatizer().lemmatize(w), pos) for w, pos in words_pos]\n",
        "print(lemmed_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color = 'E3A440'>*7. Filtrage selon le rôle morphosyntaxique*</font>\n",
        "\n",
        "Le filtrage des unités lexicales peut s'étendre jusqu'à l'élimination d'unités qui ne font pas partie d'une liste de rôles morphosyntaxique prédéfinie. "
      ],
      "metadata": {
        "id": "n-48nGApkl_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retenir seulement les noms et les adjectifs\n",
        "lemmed_pos = [(w, pos) for w, pos in words_pos if pos in ['NOUN','ADJ']]\n",
        "print(lemmed_pos)"
      ],
      "metadata": {
        "id": "FrOSEBCzk08r",
        "outputId": "c78437a4-b52c-4d74-ee25-ce811f5c54b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('thursday', 'NOUN'), ('morning', 'NOUN'), ('great', 'ADJ'), ('arthur', 'NOUN'), ('good', 'ADJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ges85oatFrOJ"
      },
      "source": [
        "## <font color = 'E3A440'>Traitement d'un texte</font>\n",
        "\n",
        "Le prétraitement d'un corpus de recherche peut mettre en place plusieurs autres étapes. La plus importante est la **segmentation**. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'>*1. Segmentation*</font>\n",
        "\n",
        "Tout dépendant de l'objectif de l'analyse, un segment peut être un document, un paragraphe, une concordance, un groupe de phrases, une phrase, etc.\n",
        "\n"
      ],
      "metadata": {
        "id": "GZqenN7Rcc2D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOzWtcugFrOK",
        "outputId": "ee2204c4-6ee5-445f-a1e8-aedacf2f7f3b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "175"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "text = \"\"\"At eight o'clock, on Thursday morning, the great Arthur didn't feel VERY good.\n",
        "          The following morning, at nine, Arthur felt better.\n",
        "          A dog run in the street.\"\"\"\n",
        "len(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans le bloc de code suivant, nous faisons une segmentation par pharse."
      ],
      "metadata": {
        "id": "sy8Qow43dHKv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Lihe1FNjFrOK",
        "outputId": "2d7515da-68f1-4e98-bf4b-f9168963ac68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"At eight o'clock, on Thursday morning, the great Arthur didn't feel VERY good.\", 'The following morning, at nine, Arthur felt better.', 'A dog run in the street.']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "sentences = nltk.sent_tokenize(text)\n",
        "print(sentences)\n",
        "len(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'>*2. Annotation et nettoyage*</font>\n",
        "\n",
        "Tout dépendant de l'objectif de l'analyse, un segment peut être un document, un paragraphe, une concordance, un groupe de phrases, une phrase, etc.\n"
      ],
      "metadata": {
        "id": "gHi0hAxZdKx4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color = 'E3A440'>*2.1 Création d'une fonction*</font>\n",
        "\n",
        "La cération de funciton est utile pour plusieurs raison. Dans notre casm, nous voulons englober Souvent, il est utile de créer de fonctions pour executer accomplir pluseurs étapes"
      ],
      "metadata": {
        "id": "waMXvFxhgH0u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6rtngk2rFrOL"
      },
      "outputs": [],
      "source": [
        "# To run this function proprlely, you need to import modules needed\n",
        "def CleaningText(text_as_string, language = 'english', reduce = '', list_pos_to_keep = [], Stopwords_to_add = []):\n",
        "    from nltk.corpus import stopwords\n",
        "\n",
        "    words = nltk.word_tokenize(text_as_string)\n",
        "    words_pos = nltk.pos_tag(words, tagset='universal')\n",
        "    words_pos = [(w, pos) for w, pos in words_pos if w.isalnum()]\n",
        "    words_pos = [(w.lower(), pos) for w, pos in words_pos]\n",
        "    \n",
        "    if reduce == 'stem': \n",
        "        from nltk.stem.porter import PorterStemmer\n",
        "        reduced_words_pos = [(PorterStemmer().stem(w), pos) for w, pos in words_pos]\n",
        "        \n",
        "    elif reduce == 'lemma':\n",
        "        from nltk.stem.wordnet import WordNetLemmatizer\n",
        "        reduced_words_pos = [(WordNetLemmatizer().lemmatize(w), pos) for w, pos in words_pos]\n",
        "    else:\n",
        "        import warnings\n",
        "        reduced_words_pos = words_pos\n",
        "        warnings.warn(\"Warning : any reduction was made on words! Please, use \\\"reduce\\\" argument to chosse between 'stem' or  'lemma'\")\n",
        "    if list_pos_to_keep:\n",
        "        reduced_words_pos = [(w, pos) for w, pos in reduced_words_pos if pos in list_pos_to_keep]\n",
        "    else:\n",
        "        import warnings\n",
        "        warnings.warn(\"Warning : any POS filtering was made. Pleae, use \\\"list_pos_to_keep\\\" to create a list of POS tag to keep.\")\n",
        "    \n",
        "    list_stopwords = stopwords.words(language) + Stopwords_to_add\n",
        "    reduced_words_pos = [(w, pos) for w, pos in reduced_words_pos if w not in list_stopwords and len(w) > 1 ]\n",
        "    return reduced_words_pos\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = nltk.word_tokenize(text_as_string)\n",
        "words_pos = nltk.pos_tag(words, tagset='universal')\n",
        "words_pos = [(w, pos) for w, pos in words_pos if w.isalnum()]\n",
        "words_pos = [(w.lower(), pos) for w, pos in words_pos]\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "reduced_words_pos = [(WordNetLemmatizer().lemmatize(w), pos) for w, pos in words_pos]\n",
        "reduced_words_pos = [(w, pos) for w, pos in reduced_words_pos if pos in ['NOUN','ADJ','VERB']]\n",
        "list_stopwords = stopwords.words(language) + ['http']\n",
        "words_pos = [(w, pos) for w, pos in reduced_words_pos if w not in list_stopwords ]\n",
        "reduced_words_pos\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-7eG8dITqdz",
        "outputId": "b3b40339-3230-491d-fe46-11871e038425"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('entire', 'ADJ'),\n",
              " ('swiss', 'ADJ'),\n",
              " ('football', 'NOUN'),\n",
              " ('league', 'NOUN'),\n",
              " ('is', 'VERB'),\n",
              " ('hold', 'NOUN'),\n",
              " ('postponing', 'VERB'),\n",
              " ('game', 'NOUN'),\n",
              " ('professional', 'ADJ'),\n",
              " ('amateur', 'ADJ'),\n",
              " ('level', 'NOUN'),\n",
              " ('coronavirus', 'ADJ'),\n",
              " ('http', 'NOUN')]"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color = 'E3A440'>*2.2 Application function nettoyage*</font>\n",
        "\n",
        "Maintenant, nous pouvons apliquer cette function à chacun de not segment. dans notre cas il s'agit de phrases."
      ],
      "metadata": {
        "id": "kWkIfcMwpqe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_sentences = [CleaningText(sent) for sent in sentences]\n",
        "print(cleaned_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVcF-aGGppjd",
        "outputId": "2e910320-d5f8-4c30-a0a0-09a6e296674d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('eight', 'NUM'), ('thursday', 'NOUN'), ('morning', 'NOUN'), ('great', 'ADJ'), ('arthur', 'NOUN'), ('feel', 'VERB'), ('good', 'ADJ')], [('following', 'ADJ'), ('morning', 'NOUN'), ('nine', 'NUM'), ('arthur', 'NOUN'), ('felt', 'VERB'), ('better', 'ADV')], [('dog', 'NOUN'), ('run', 'NOUN'), ('street', 'NOUN')]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: UserWarning: Warning : any reduction was made on words! Please, use \"reduce\" argument to chosse between 'stem' or  'lemma'\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: UserWarning: Warning : any POS filtering was made. Pleae, use \"list_pos_to_keep\" to create a list of POS tag to keep.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_sentences = [CleaningText(sent, reduce = 'lemma', list_pos_to_keep = ['']) for sent in sentences]\n",
        "print(cleaned_sentences)"
      ],
      "metadata": {
        "id": "vkpxyQfl2Wna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color = 'E3A440'>*2.3 Fréquence mots*</font>\n",
        "\n",
        "Quelle est la fréquence des mots de notre texte? Commencon par retirer le POS tag, pour obtenir une liste de listes de mots ( et non une liste de tuple mot-pos)."
      ],
      "metadata": {
        "id": "TBNxooILr8-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freqs_in_text = nltk.FreqDist([w for sent in cleaned_sentences for w, pos in sent ])\n",
        "freqs_in_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFPhg9Sm6Lbq",
        "outputId": "5012cd6c-2351-4552-ffb0-5d0c9f5f245a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'morning': 2, 'arthur': 2, 'eight': 1, 'thursday': 1, 'great': 1, 'feel': 1, 'good': 1, 'following': 1, 'nine': 1, 'felt': 1, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'>*3. Vectorisation*</font>\n",
        "\n",
        "Généralement, pour utiliser le texte dans un contexte d'analyse de données ou d'apprentissage automatique, ce texte doit être transformé dans un objet mathématique approprié. \n",
        "Le modèle le plus simple et connu est le \"bags-of-words\", dans lequel chaque document (ou chaque mot) est défini par un certain nombre d'unités lexicales qui le caractérise. \n",
        "\n",
        "\n",
        "The main step in text mining is to convert unstructured textual data into a mathematical model to be used in statistical learning. Thus, we need to create a <font color='E3A440'>**Document-Term matrix**</font>, a matrix $n \\times w$, where $n$ is the number of text segments and $w$ is the number of textual features selected.The textual feature can have different nature. In the simplest model, these features correspond to the set of types that resume each token of the corpus. In other terms, $w$ is the number of features that characterize a segment of text. The matrix is generally represented as follows:  \n",
        " \n",
        "$$X = \\begin{bmatrix} \n",
        "x_{11} & x_{12} & \\ldots & x_{1w} \\\\\n",
        "\\vdots & \\vdots       &  \\ddots      & \\vdots \\\\ \n",
        "x_{n1} & x_{12} & \\ldots & x_{nw} \\\\\n",
        "\\end{bmatrix}\n",
        "$$ \n",
        " \n",
        "\\\\"
      ],
      "metadata": {
        "id": "m0YdoOA1stB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisation de l'objet\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def identity_tokenizer(text):\n",
        "    return text\n",
        "\n",
        "# Transforming the word in frequencies\n",
        "vectorized = CountVectorizer(lowercase = False, # Convert all characters to lowercase before tokenizing\n",
        "                             min_df = 1, # Ignore terms that have a document frequency strictly lower than the given threshold \n",
        "                             max_df = 10, # Ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words)\n",
        "                             stop_words = stopwords.words('english'), # Remove the list of words provided\n",
        "                             ngram_range = (1, 1), # Get the lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted\n",
        "                             tokenizer=identity_tokenizer) # Override the string tokenization step while preserving the preprocessing and n-grams generation steps"
      ],
      "metadata": {
        "id": "LpFPJrGI2Nsi"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilisation du \"vectorizer\" avec une liste de listes de mot (et non une liste de tuple de mots-pos)."
      ],
      "metadata": {
        "id": "G9uC3usB5dtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Liste de liste de mots:\n",
        "[[w for w, pos in sent] for sent in cleaned_sentences]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gy8_VTM67Lax",
        "outputId": "466c8c22-e795-44c3-ca45-1b0f375dd43e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['eight', 'thursday', 'morning', 'great', 'arthur', 'feel', 'good'],\n",
              " ['following', 'morning', 'nine', 'arthur', 'felt', 'better'],\n",
              " ['dog', 'run', 'street']]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pplication du vectorizer\n",
        "freq_term_DTM = vectorized.fit_transform([[w for w, pos in sent] for sent in cleaned_sentences])\n",
        "print(pd.DataFrame(freq_term_DTM.todense(), columns =  [k for k, v in sorted(vectorized.vocabulary_.items(), key=lambda item: item[1])] ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8ydVdmC5c76",
        "outputId": "2d941164-981a-40f9-be97-3958bc15b3e9"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   arthur  better  dog  eight  feel  felt  following  good  great  morning  \\\n",
            "0       1       0    0      1     1     0          0     1      1        1   \n",
            "1       1       1    0      0     0     1          1     0      0        1   \n",
            "2       0       0    1      0     0     0          0     0      0        0   \n",
            "\n",
            "   nine  run  street  thursday  \n",
            "0     0    0       0         1  \n",
            "1     1    0       0         0  \n",
            "2     0    1       1         0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QymfV4S_FrOZ"
      },
      "source": [
        "## Exercise : Sentiment analysis on a COVID-19 twetter dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_DIR='Donnees_demystifiees_seance_6/'\n",
        "DATA_DIR=os.path.join(ROOT_DIR, 'Data')\n",
        "import zipfile\n",
        "from datetime import datetime\n",
        "\n",
        "#Unzips the dataset and gets the TSV dataset\n",
        "#with zipfile.ZipFile(os.path.join(DATA_DIR,'4POINT0_Top_50_tweet_profiles.zip'), 'r') as zip_ref:\n",
        "#    zip_ref.extractall(DATA_DIR)\n",
        "\n",
        "df = pd.read_pickle(os.path.join(DATA_DIR,'Top_50_tweet_profiles.pkl')).sample(5000).reset_index()"
      ],
      "metadata": {
        "id": "Q6snBPy8EfBX"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DbKk1dVz5nl",
        "outputId": "0558d4a2-dd74-4895-9c10-7e09b140b17e"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000, 14)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejgQAfv8FIrC",
        "outputId": "599fdc92-b43c-4cc9-fc8c-13de3f05cd26"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "index                                  int64\n",
              "Tweet Id                              object\n",
              "Tweet URL                             object\n",
              "Tweet Posted Time             datetime64[ns]\n",
              "Tweet Content                         object\n",
              "Tweet Type                            object\n",
              "Client                                object\n",
              "Retweets received                      int64\n",
              "Likes received                         int64\n",
              "User Id                               object\n",
              "Name                                  object\n",
              "Username                              object\n",
              "Verified or Non-Verified              object\n",
              "Profile URL                           object\n",
              "Protected or Not Protected            object\n",
              "neg                                  float64\n",
              "neu                                  float64\n",
              "pos                                  float64\n",
              "compound                             float64\n",
              "compound_category                   category\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[0]"
      ],
      "metadata": {
        "id": "54zPNygEJXhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Tweet Content']"
      ],
      "metadata": {
        "id": "5yBg-jU9QLl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cette opération prendra 2 minutes, donc, nous vous suggérons de continuer la lecture dce l'exercices pour vous faire une idée de ce qui s'en vient."
      ],
      "metadata": {
        "id": "_zmlEGCKRaWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_tweets = [CleaningText(sent, reduce = 'lemma', list_pos_to_keep = ['ADJ'], Stopwords_to_add=['http']) for sent in list(df['Tweet Content'])]"
      ],
      "metadata": {
        "id": "NMmjI8IaU_JF"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisation de l'objet\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def identity_tokenizer(text):\n",
        "    return text\n",
        "\n",
        "# Transforming the word in frequencies\n",
        "vectorized = CountVectorizer(lowercase = False, # Convert all characters to lowercase before tokenizing\n",
        "                             min_df = 10, # Ignore terms that have a document frequency strictly lower than the given threshold \n",
        "                             max_df = 4500, # Ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words)\n",
        "                             stop_words = stopwords.words('english'), # Remove the list of words provided\n",
        "                             ngram_range = (1, 1), # Get the lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted\n",
        "                             tokenizer=identity_tokenizer) # Override the string tokenization step while preserving the preprocessing and n-grams generation steps"
      ],
      "metadata": {
        "id": "6Aj4JUdeYBbE"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq_term_DTM = vectorized.fit_transform([[w for w, pos in sent] for sent in cleaned_tweets])\n",
        "freq_term_DTM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkSkuQvUXsvs",
        "outputId": "dc066ca3-dac5-4129-8bcc-146a87b2c276"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'b', 'c', 'e', 'f', 'g', 'h', 'j', 'l', 'n', 'p', 'r', 'u', 'v', 'w'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<5000x71 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 2067 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "pInoj9yrFrOZ",
        "outputId": "27f2080b-fc8e-4a48-dcd3-1eec05a76788",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "sia = SentimentIntensityAnalyzer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "6jAgx5YgFrOa",
        "outputId": "efdbc090-7972-4263-b93a-9dbbbc4a7f04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'neg': 0.0, 'neu': 0.295, 'pos': 0.705, 'compound': 0.8012}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'neg': 0.0, 'neu': 0.488, 'pos': 0.512, 'compound': 0.484}"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "print(sia.polarity_scores(\"Wow, NLTK is really powerful!\"))\n",
        "sia.polarity_scores(\"NLTK is not bad!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "kpGJWGRWFrOa",
        "outputId": "a4860911-d560-4ac6-f0f0-9bb274be6eba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'neg': 0.655, 'neu': 0.345, 'pos': 0.0, 'compound': -0.5848}"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "sia.polarity_scores(\"NLTK is bad!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasent = df.apply(lambda x: sia.polarity_scores(x['Tweet Content']), 1)\n",
        "df = df.join(pd.DataFrame(list(datasent)))"
      ],
      "metadata": {
        "id": "3etQFxEACpty"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['compound']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TggWZ1AlOsdh",
        "outputId": "c0ed0adb-1f95-4624-c31f-ddc19af8107e"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       0.0000\n",
              "1       0.0000\n",
              "2       0.4019\n",
              "3      -0.0772\n",
              "4       0.9632\n",
              "         ...  \n",
              "4995    0.6124\n",
              "4996    0.8360\n",
              "4997    0.0000\n",
              "4998    0.4466\n",
              "4999    0.0000\n",
              "Name: compound, Length: 5000, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['compound'].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xf_vn9Veebrg",
        "outputId": "fbda3d6d-38ee-4b33-fb9d-fac6a918e43e"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    5000.000000\n",
              "mean        0.217127\n",
              "std         0.419101\n",
              "min        -0.982500\n",
              "25%         0.000000\n",
              "50%         0.000000\n",
              "75%         0.585900\n",
              "max         0.993700\n",
              "Name: compound, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bins = [-np.inf, -0.5, 0.5, 1]\n",
        "names = ['negative', 'neu', 'positive']\n",
        "df['compound_category']  = pd.cut(df['compound'], bins, labels=names, include_lowest =True)"
      ],
      "metadata": {
        "id": "jJrj4DKePhGd"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['compound_category'] == 'positive']"
      ],
      "metadata": {
        "id": "9mwSxS3kiAaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "Counter(df['compound_category'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UL-ibgrb4u0M",
        "outputId": "ac5ee938-4a4f-45ff-a527-fd99a6a2af8d"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'neu': 3318, 'positive': 1374, 'negative': 308})"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logical_vector = df['compound_category'] == 'positive'"
      ],
      "metadata": {
        "id": "TVhrPm7PZO5E"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mDSXUeRq51Kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(logical_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogaDNEUNZQU0",
        "outputId": "164be858-59c6-4776-9923-37260d6533e9"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "286"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def GetLexicalSpecificities(freq_term_DTM, logical_vector):\n",
        "    # This code ref takes inspiration from this python module : https://pypi.org/project/corpus-toolkit/\n",
        "    # and its main script:  https://github.com/kristopherkyle/corpus_toolkit/blob/master/corpus_toolkit/corpus_tools.py\n",
        "    # which is based on this paper: https://aclanthology.org/J93-1003/\n",
        "    import math\n",
        "    df_freq_target = pd.DataFrame(np.asarray(freq_term_DTM[logical_vector].sum(0).T).reshape(-1))\n",
        "    df_freq_target.index = [word for (word,idx) in sorted(vectorized.vocabulary_.items(), key= lambda x:x[1])]\n",
        "    df_freq_target.columns = ['freq1']\n",
        "    df_freq_target['freq2'] = np.asarray(freq_term_DTM[~(logical_vector)].sum(0).T).reshape(-1)\n",
        "    df_freq_target['tot'] = df_freq_target['freq1'] + df_freq_target['freq2']\n",
        "\n",
        "    df_freq_target['freq1'] = df_freq_target['freq1'].apply(lambda x: 0.0000001 if x == 0 else x).astype(float)\n",
        "    df_freq_target['freq2'] = df_freq_target['freq2'].apply(lambda x: 0.0000001 if x == 0 else x).astype(float)\n",
        "    #\n",
        "    df_freq_target['freq1_norm'] = df_freq_target['freq1']/df_freq_target['freq1'].sum() * 1000000\n",
        "    df_freq_target['freq2_norm'] = df_freq_target['freq2']/df_freq_target['freq2'].sum() * 1000000\n",
        "    #\n",
        "    df_freq_target['fraction'] = df_freq_target['freq1_norm'] / df_freq_target['freq2_norm']\n",
        "    df_freq_target['Log-likelihood Ratio'] = df_freq_target['fraction'].apply(math.log2)\n",
        "    frequency_threshold = 10 # Insert your frequency threshold as integer\n",
        "    return df_freq_target[df_freq_target['tot'] > frequency_threshold]['Log-likelihood Ratio'].sort_values(ascending=False).iloc[range(50)]"
      ],
      "metadata": {
        "id": "39eDHLjFZcLU"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GetLexicalSpecificities(freq_term_DTM, logical_vector)"
      ],
      "metadata": {
        "id": "8iEQNaJvJ2fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Annotation and cleaning : ADD adjective and verbs as POS tag to keep"
      ],
      "metadata": {
        "id": "8zRK37miNzH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Annotation and cleaning : ADD adjective and verbs as POS tag to keep\n",
        "cleaned_tweets = [CleaningText(sent, reduce = 'lemma', list_pos_to_keep = ['NOUN','ADJ','VERB'], Stopwords_to_add=['http']) for sent in list(df['Tweet Content'])]"
      ],
      "metadata": {
        "id": "xtrWcUOmNxOR"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Vectorisation\n",
        "## 2.1 initialise with parameters : \n",
        "vectorized = CountVectorizer(lowercase = False, # Convert all characters to lowercase before tokenizing\n",
        "                             min_df = 10, # Ignore terms that have a document frequency strictly lower than the given threshold \n",
        "                             max_df = 4500, # Ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words)\n",
        "                             stop_words = stopwords.words('english'), # Remove the list of words provided\n",
        "                             ngram_range = (1, 1), # Get the lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted\n",
        "                             tokenizer=identity_tokenizer) # Override the string tokenization step while preserving the preprocessing and n-grams generation steps\n",
        "\n",
        "#\n",
        "freq_term_DTM = vectorized.fit_transform([[w for w, pos in sent] for sent in cleaned_tweets])\n",
        "\n",
        "freq_term_DTM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoTIyCR7daI7",
        "outputId": "cd4a6bfc-cc78-4002-f791-c4f69d753bb9"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'b', 'c', 'e', 'f', 'g', 'h', 'j', 'l', 'n', 'p', 'r', 'u', 'v', 'w'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<5000x1576 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 24770 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logical_vector = df['compound_category'] == 'positive'"
      ],
      "metadata": {
        "id": "B-qMBlPhPGC_"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GetLexicalSpecificities(freq_term_DTM, logical_vector)"
      ],
      "metadata": {
        "id": "CGFUrGPVPJWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Retweets received'].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tnUU_iVQZBh",
        "outputId": "332993c7-3dc9-41eb-f3fd-265ce7013aa1"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count      5000.000000\n",
              "mean       6138.390800\n",
              "std       20779.908625\n",
              "min           0.000000\n",
              "25%         148.000000\n",
              "50%         733.000000\n",
              "75%        3209.750000\n",
              "max      459154.000000\n",
              "Name: Retweets received, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bins = [-np.inf, 148, 733, 3209, 459154]\n",
        "names = ['low', 'medium', 'high', 'very_high']\n",
        "df['Retweets_received_category']  = pd.cut(df['Retweets received'], bins, labels=names, include_lowest =True)"
      ],
      "metadata": {
        "id": "Z25CYh43QRBK"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logical_vector = df['Retweets_received_category'] == 'low'"
      ],
      "metadata": {
        "id": "WT1B3Iq-RIPZ"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GetLexicalSpecificities(freq_term_DTM, logical_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6kVV9xjRM9h",
        "outputId": "7a80a608-55be-402b-ed2a-fe8f7ac603fe"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "uniform              30.618825\n",
              "padmantrailer        30.104252\n",
              "falta                30.069486\n",
              "só                   30.033862\n",
              "craquedaclaro        30.033862\n",
              "carro                30.033862\n",
              "ganhar               30.033862\n",
              "concorra             30.033862\n",
              "virar                30.033862\n",
              "irresponsibletour    28.618825\n",
              "agora                 6.743840\n",
              "republic              5.365328\n",
              "serve                 5.365328\n",
              "wear                  5.223972\n",
              "srbachchan            5.043400\n",
              "jai                   5.043400\n",
              "nightschool           4.917869\n",
              "superhero             4.780366\n",
              "remember              4.605279\n",
              "wizkhalifa            4.488185\n",
              "kevinhart4real        4.180904\n",
              "gameofgames           4.043400\n",
              "ho                    3.905897\n",
              "writes                3.780366\n",
              "site                  3.780366\n",
              "democratic            3.195403\n",
              "country               3.065095\n",
              "jlo                   3.043400\n",
              "kkwbeauty             2.917869\n",
              "shop                  2.721472\n",
              "creator               2.721472\n",
              "lo                    2.458438\n",
              "presidential          2.458438\n",
              "paper                 2.458438\n",
              "ask                   2.265793\n",
              "worldofdance          2.136510\n",
              "trial                 1.988952\n",
              "college               1.821008\n",
              "need                  1.806361\n",
              "movie                 1.780366\n",
              "hey                   1.780366\n",
              "series                1.780366\n",
              "west                  1.721472\n",
              "found                 1.721472\n",
              "director              1.721472\n",
              "supersoulsunday       1.721472\n",
              "pst                   1.610441\n",
              "washington            1.458438\n",
              "answer                1.458438\n",
              "pic                   1.458438\n",
              "Name: Log-likelihood Ratio, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}